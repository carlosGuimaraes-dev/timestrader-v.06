{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bc944697",
      "metadata": {
        "id": "bc944697"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1a50c71d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a50c71d",
        "outputId": "11c14a05-c2a4-485b-ecf3-0b484c6d090c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config -> seq_len=512, batch_size=2048, k=3, embed_dim=256, HxW=8x8, dropout=0.2\n",
            "Optim -> AdamW max_lr=0.001, wd=0.0001, patience=30, grad_accum=1\n",
            "Loader -> workers=8, pin_memory=True, persistent=True, prefetch=2\n",
            "Device -> cuda | bf16=True | channels_last=True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "CSV_PATH = \"mnq_complete_dataset.csv\"  #@param {type:\"string\"}\n",
        "\n",
        "# Features to use (OHLCV + 7 TA indicators; all lowercase)\n",
        "INDICATORS_TO_USE = ['open', 'high', 'low', 'close', 'volume','atr_14', 'adx_14', 'ema_9', 'ema_21', 'vwap', 'rsi_21', 'stochk_14_3_3']  #@param {type:\"raw\"}\n",
        "\n",
        "# Normalization: 'StandardScaler' or 'MinMaxScaler'\n",
        "NORMALIZATION_TYPE = \"StandardScaler\"  #@param [\"StandardScaler\", \"MinMaxScaler\"]\n",
        "\n",
        "# Chronological split ratios (must sum to 1.0)\n",
        "TRAIN_VALID_TEST_SPLIT = [0.7, 0.15, 0.15]  #@param {type:\"raw\"}\n",
        "\n",
        "# Sequence/window length for model input\n",
        "SEQ_LEN = 512  #@param {type:\"integer\"}\n",
        "\n",
        "# Step size between windows (1 = full overlap)\n",
        "WINDOW_STRIDE = 64  #@param {type:\"integer\"}\n",
        "\n",
        "# TimesBlock\n",
        "TOP_K_PERIODS = 3  #@param {type:\"integer\"}  # k in paper\n",
        "EMBED_DIM = 256     # 2D backbone output channels\n",
        "EMBED_H = 8\n",
        "EMBED_W = 8\n",
        "DROPOUT_RATE = 0.2  #@param {type:\"number\"}\n",
        "\n",
        "# Training\n",
        "NUM_EPOCHS = 100  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 2048  #@param {type:\"integer\"}\n",
        "LEARNING_RATE = 1e-3  # scheduler max_lr\n",
        "WEIGHT_DECAY = 1e-4  #@param {type:\"number\"}\n",
        "PATIENCE = 30  #@param {type:\"integer\"}\n",
        "GRAD_ACCUM_STEPS = 1  #@param {type:\"integer\"}\n",
        "\n",
        "# LR Scheduler (OneCycle)\n",
        "USE_SCHEDULER = True  #@param {type:\"boolean\"}\n",
        "ONECYCLE_PCT_START = 0.15  #@param {type:\"number\"}\n",
        "ONECYCLE_DIV_FACTOR = 25.0  #@param {type:\"number\"}\n",
        "ONECYCLE_FINAL_DIV = 100.0  #@param {type:\"number\"}\n",
        "\n",
        "# Gradient clipping\n",
        "CLIP_GRAD_NORM = 1.0  #@param {type:\"number\"}\n",
        "\n",
        "# Train-only augmentation\n",
        "AUG_NOISE_STD = 0.01  #@param {type:\"number\"}\n",
        "\n",
        "# DataLoader performance\n",
        "DATALOADER_WORKERS = 8  #@param {type:\"integer\"}\n",
        "PIN_MEMORY = True  #@param {type:\"boolean\"}\n",
        "PERSISTENT_WORKERS = True  #@param {type:\"boolean\"}\n",
        "PREFETCH_FACTOR = 2  #@param {type:\"integer\"}\n",
        "\n",
        "# Precision & memory format (A100)\n",
        "USE_BF16 = True  #@param {type:\"boolean\"}\n",
        "CHANNELS_LAST = True  #@param {type:\"boolean\"}\n",
        "PRINT_GPU_MEM = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# Checkpointing\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/timesnet_mnq/checkpoints/best.pt\"  #@param {type:\"string\"}\n",
        "\n",
        "# Logs e histórico\n",
        "LOG_DIR = \"/content/drive/MyDrive/timesnet_mnq/logs\"  #@param {type:\"string\"}\n",
        "HISTORY_PATH = LOG_DIR + \"/training_history.jsonl\"\n",
        "\n",
        "# Resume training if checkpoint exists\n",
        "RESUME_TRAINING = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# Device\n",
        "import os\n",
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Config -> seq_len={SEQ_LEN}, batch_size={BATCH_SIZE}, k={TOP_K_PERIODS}, embed_dim={EMBED_DIM}, HxW={EMBED_H}x{EMBED_W}, dropout={DROPOUT_RATE}\")\n",
        "print(f\"Optim -> AdamW max_lr={LEARNING_RATE}, wd={WEIGHT_DECAY}, patience={PATIENCE}, grad_accum={GRAD_ACCUM_STEPS}\")\n",
        "print(f\"Loader -> workers={DATALOADER_WORKERS}, pin_memory={PIN_MEMORY}, persistent={PERSISTENT_WORKERS}, prefetch={PREFETCH_FACTOR}\")\n",
        "print(f\"Device -> {DEVICE} | bf16={USE_BF16} | channels_last={CHANNELS_LAST}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8044ea06",
      "metadata": {
        "id": "8044ea06"
      },
      "source": [
        "### Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0080a8f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0080a8f2",
        "outputId": "351f4999-a6c0-4562-9b83-b9f58b67a4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU -> NVIDIA A100-SXM4-40GB | VRAM total: 42.47 GB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import sys, subprocess\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# Ensure pandas-ta is available early\n",
        "try:\n",
        "    import pandas_ta as ta\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pandas-ta'])\n",
        "    import pandas_ta as ta\n",
        "\n",
        "import contextlib\n",
        "\n",
        "def _fmt_gb(bytes_val):\n",
        "    try:\n",
        "        return f\"{bytes_val/1e9:.2f} GB\"\n",
        "    except Exception:\n",
        "        return str(bytes_val)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f\"GPU -> {torch.cuda.get_device_name(0)} | VRAM total: {_fmt_gb(props.total_memory)}\")\n",
        "\n",
        "try:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7205188e",
      "metadata": {
        "id": "7205188e"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4a3cd4ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a3cd4ba",
        "outputId": "9caa4e4f-9c91-4a38-a73d-8acfddc8da04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Checkpoints dir -> /content/drive/MyDrive/timesnet_mnq/checkpoints\n",
            "Checkpoint file -> /content/drive/MyDrive/timesnet_mnq/checkpoints/best.pt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "print(f\"Checkpoints dir -> {os.path.dirname(CHECKPOINT_PATH)}\")\n",
        "print(f\"Checkpoint file -> {CHECKPOINT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a44f48",
      "metadata": {
        "id": "e2a44f48"
      },
      "source": [
        "### Data Loading & Chronological Splits (no leakage)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "119ba241",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "119ba241",
        "outputId": "7d11c91d-e66f-4f96-ae61-a6e7c31fe133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CSV: mnq_complete_dataset.csv\n",
            "Features selected: 12 | ['open', 'high', 'low', 'close', 'volume', 'atr_14', 'adx_14', 'ema_9', 'ema_21', 'vwap', 'rsi_21', 'stochk_14_3_3']\n",
            "Total rows after TA + cleanup: 513211 | Feature dim: 12\n",
            "Split -> train: (359247, 12), val: (76981, 12), test: (76983, 12)\n",
            "Scaler(Standard) -> mean range [0.1939, 1653.4210] | std range [0.1121, 2725.6734]\n",
            "Windows -> train: 5606, val: 1195, test: 1195 | stride=64, seq_len=512\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((359247, 12), (76981, 12), (76983, 12))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "def _find_datetime_column(df: pd.DataFrame) -> Optional[str]:\n",
        "    candidates = ['datetime', 'date', 'time', 'timestamp', 'ts']\n",
        "    cols = [c for c in df.columns]\n",
        "    for c in cols:\n",
        "        if c.lower() in candidates:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _find_col_ci(df: pd.DataFrame, name: str) -> Optional[str]:\n",
        "    for c in df.columns:\n",
        "        if c.lower() == name.lower():\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _has_col_ci(df: pd.DataFrame, name: str) -> bool:\n",
        "    return any(c.lower() == name.lower() for c in df.columns)\n",
        "\n",
        "def add_ta_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Normalize base column names to lowercase for consistency\n",
        "    rename_map = {}\n",
        "    for nm in ['Open','High','Low','Close','Volume']:\n",
        "        c = _find_col_ci(df, nm)\n",
        "        if c is not None:\n",
        "            rename_map[c] = nm.lower()\n",
        "    if rename_map:\n",
        "        df = df.rename(columns=rename_map)\n",
        "\n",
        "    required = ['high','low','close','volume']\n",
        "    if any(not _has_col_ci(df, r) for r in required):\n",
        "        raise ValueError('Missing OHLCV columns (high, low, close, volume) to compute pandas-ta indicators.')\n",
        "\n",
        "    # Ensure datetime index for VWAP\n",
        "    time_col = _find_datetime_column(df)\n",
        "    if time_col is not None:\n",
        "        try:\n",
        "            df[time_col] = pd.to_datetime(df[time_col])\n",
        "            df = df.set_index(time_col)\n",
        "        except Exception:\n",
        "            pass # Keep going if datetime conversion fails, but VWAP might fail\n",
        "\n",
        "    # Compute indicators only if a lowercase target is not already present\n",
        "    if not _has_col_ci(df, 'atr_14'):\n",
        "        df.ta.atr(high='high', low='low', close='close', length=14, append=True)\n",
        "    if not _has_col_ci(df, 'adx_14'):\n",
        "        df.ta.adx(high='high', low='low', close='close', length=14, append=True)\n",
        "    if not _has_col_ci(df, 'ema_9'):\n",
        "        df.ta.ema(close='close', length=9, append=True)\n",
        "    if not _has_col_ci(df, 'ema_21'):\n",
        "        df.ta.ema(close='close', length=21, append=True)\n",
        "    if not _has_col_ci(df, 'vwap'):\n",
        "        df.ta.vwap(high='high', low='low', close='close', volume='volume', append=True)\n",
        "    if not _has_col_ci(df, 'rsi_21'):\n",
        "        df.ta.rsi(close='close', length=21, append=True)\n",
        "    if not _has_col_ci(df, 'stochk_14_3_3'):\n",
        "        df.ta.stoch(high='high', low='low', close='close', k=14, d=3, smooth_k=3, append=True)\n",
        "\n",
        "    # Rename known pandas-ta outputs to lowercase canonical names if needed\n",
        "    lower_map = {}\n",
        "    # ATR - pandas-ta outputs ATRr_14\n",
        "    if 'ATRr_14' in df.columns and 'atr_14' not in df.columns:\n",
        "        lower_map['ATRr_14'] = 'atr_14'\n",
        "    # ADX\n",
        "    if 'ADX_14' in df.columns and 'adx_14' not in df.columns:\n",
        "        lower_map['ADX_14'] = 'adx_14'\n",
        "    # EMAs\n",
        "    if 'EMA_9' in df.columns and 'ema_9' not in df.columns:\n",
        "        lower_map['EMA_9'] = 'ema_9'\n",
        "    if 'EMA_21' in df.columns and 'ema_21' not in df.columns:\n",
        "        lower_map['EMA_21'] = 'ema_21'\n",
        "    # VWAP\n",
        "    if 'VWAP' in df.columns and 'vwap' not in df.columns:\n",
        "        lower_map['VWAP'] = 'vwap'\n",
        "    if 'VWAP_D' in df.columns and 'vwap' not in df.columns: # Added to handle VWAP_D\n",
        "        lower_map['VWAP_D'] = 'vwap'\n",
        "    # RSI\n",
        "    if 'RSI_21' in df.columns and 'rsi_21' not in df.columns:\n",
        "        lower_map['RSI_21'] = 'rsi_21'\n",
        "    # STOCH K/D (we'll expose K by default)\n",
        "    if 'STOCHk_14_3_3' in df.columns and 'stochk_14_3_3' not in df.columns:\n",
        "        lower_map['STOCHk_14_3_3'] = 'stochk_14_3_3'\n",
        "    if lower_map:\n",
        "        df = df.rename(columns=lower_map)\n",
        "    return df\n",
        "\n",
        "def load_mnq_csv(csv_path: str, indicators: List[str]) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    time_col = _find_datetime_column(df)\n",
        "    if time_col is not None:\n",
        "        try:\n",
        "            df[time_col] = pd.to_datetime(df[time_col])\n",
        "            df = df.sort_values(by=time_col, ascending=True).reset_index(drop=True)\n",
        "        except Exception:\n",
        "            df = df.reset_index(drop=True)\n",
        "    else:\n",
        "        df = df.reset_index(drop=True)\n",
        "\n",
        "    # Create TA indicators and ensure lowercase canon names\n",
        "    df = add_ta_indicators(df)\n",
        "    # Select requested features (lowercase)\n",
        "    missing = [c for c in indicators if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns after TA creation: {missing}. Available: {list(df.columns)}\")\n",
        "    xdf = df[indicators].copy()\n",
        "    # Clean: numeric, remove inf, drop NaNs to avoid leakage via backward fill\n",
        "    for c in xdf.columns:\n",
        "        xdf[c] = pd.to_numeric(xdf[c], errors='coerce')\n",
        "    xdf = xdf.replace([np.inf, -np.inf], np.nan)\n",
        "    xdf = xdf.dropna()\n",
        "    return xdf\n",
        "\n",
        "def chronological_split(arr: np.ndarray, ratios: List[float]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    assert abs(sum(ratios) - 1.0) < 1e-6, \"TRAIN_VALID_TEST_SPLIT must sum to 1.0\"\n",
        "    T = len(arr)\n",
        "    n_train = int(T * ratios[0])\n",
        "    n_val = int(T * ratios[1])\n",
        "    n_test = T - n_train - n_val\n",
        "    train = arr[:n_train]\n",
        "    val = arr[n_train:n_train+n_val]\n",
        "    test = arr[n_train+n_val:]\n",
        "    return train, val, test\n",
        "\n",
        "def make_scaler(norm_type: str):\n",
        "    if norm_type == \"StandardScaler\":\n",
        "        return StandardScaler()\n",
        "    elif norm_type == \"MinMaxScaler\":\n",
        "        return MinMaxScaler()\n",
        "    else:\n",
        "        raise ValueError(\"NORMALIZATION_TYPE must be 'StandardScaler' or 'MinMaxScaler'.\")\n",
        "\n",
        "def fit_transform_splits(train_arr: np.ndarray, val_arr: np.ndarray, test_arr: np.ndarray, scaler):\n",
        "    # Fit only on training data\n",
        "    scaler.fit(train_arr)\n",
        "    train_scaled = scaler.transform(train_arr)\n",
        "    val_scaled = scaler.transform(val_arr)\n",
        "    test_scaled = scaler.transform(test_arr)\n",
        "    return train_scaled, val_scaled, test_scaled\n",
        "\n",
        "# Load CSV\n",
        "print(f'Loading CSV: {CSV_PATH}')\n",
        "df = load_mnq_csv(CSV_PATH, INDICATORS_TO_USE)\n",
        "print(f'Features selected: {len(INDICATORS_TO_USE)} | {INDICATORS_TO_USE}')\n",
        "data = df.values.astype(np.float32)\n",
        "print(f'Total rows after TA + cleanup: {len(df)} | Feature dim: {data.shape[1]}')\n",
        "\n",
        "# Chronological split (no shuffling)\n",
        "train_raw, val_raw, test_raw = chronological_split(data, TRAIN_VALID_TEST_SPLIT)\n",
        "print(f'Split -> train: {train_raw.shape}, val: {val_raw.shape}, test: {test_raw.shape}')\n",
        "\n",
        "# Train-only normalization\n",
        "scaler = make_scaler(NORMALIZATION_TYPE)\n",
        "train_scaled, val_scaled, test_scaled = fit_transform_splits(train_raw, val_raw, test_raw, scaler)\n",
        "if NORMALIZATION_TYPE == 'StandardScaler':\n",
        "    means = scaler.mean_\n",
        "    stds = scaler.scale_ if hasattr(scaler, 'scale_') else np.sqrt(scaler.var_)\n",
        "    print(f'Scaler(Standard) -> mean range [{means.min():.4f}, {means.max():.4f}] | std range [{stds.min():.4f}, {stds.max():.4f}]')\n",
        "else:\n",
        "    mins = scaler.data_min_\n",
        "    maxs = scaler.data_max_\n",
        "    print(f'Scaler(MinMax) -> min range [{mins.min():.4f}, {mins.max():.4f}] | max range [{maxs.min():.4f}, {maxs.max():.4f}]')\n",
        "\n",
        "def _count_windows(n, L, s):\n",
        "    return max(0, (n - L) // s + 1)\n",
        "nw_train = _count_windows(len(train_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
        "nw_val = _count_windows(len(val_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
        "nw_test = _count_windows(len(test_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
        "print(f'Windows -> train: {nw_train}, val: {nw_val}, test: {nw_test} | stride={WINDOW_STRIDE}, seq_len={SEQ_LEN}')\n",
        "\n",
        "train_scaled.shape, val_scaled.shape, test_scaled.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1bfd75",
      "metadata": {
        "id": "fc1bfd75"
      },
      "source": [
        "### Dataset & DataLoaders (windowed, no shuffle)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "432e2afc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "432e2afc",
        "outputId": "766e38dd-a0f3-474d-cd1e-6ba0e387b9e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARN: batch_size 2048 > windows 1195; ajustando para evitar época vazia.\n",
            "WARN: batch_size 2048 > windows 1195; ajustando para evitar época vazia.\n",
            "DataLoaders -> windows (train/val/test): 5606/1195/1195 | batches: 3/1/1 | batch_size=2048\n",
            "Window shape -> L=512, C=12\n"
          ]
        }
      ],
      "source": [
        "class MNQ_Dataset(Dataset):\n",
        "    def __init__(self, arr_2d: np.ndarray, seq_len: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "        self.x = arr_2d\n",
        "        self.seq_len = int(seq_len)\n",
        "        self.stride = int(stride)\n",
        "        self.T = len(arr_2d)\n",
        "        self.C = arr_2d.shape[1]\n",
        "        if self.T < self.seq_len:\n",
        "            raise ValueError(f\"Not enough timesteps ({self.T}) for seq_len={self.seq_len}\")\n",
        "        # Number of windows using stride\n",
        "        self.idxs = list(range(0, self.T - self.seq_len + 1, self.stride))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = self.idxs[idx]\n",
        "        end = start + self.seq_len\n",
        "        window = self.x[start:end]  # shape [seq_len, C]\n",
        "        # model expects [L, C], training target == input window (self-supervised)\n",
        "        return torch.from_numpy(window).float()\n",
        "\n",
        "def make_loader(arr: np.ndarray, seq_len: int, stride: int, batch_size: int) -> DataLoader:\n",
        "    ds = MNQ_Dataset(arr, seq_len=seq_len, stride=stride)\n",
        "    # No shuffling to avoid any perceived leakage; data is chronologically windowed already\n",
        "\n",
        "    # Ajuste automático de batch para evitar época vazia\n",
        "    bs = int(batch_size)\n",
        "    nwin = len(ds)\n",
        "    if nwin < bs:\n",
        "        print(f'WARN: batch_size {bs} > windows {nwin}; ajustando para evitar época vazia.')\n",
        "        bs = max(1, nwin)\n",
        "\n",
        "    return DataLoader(ds, batch_size=bs, shuffle=False, drop_last=False, num_workers=DATALOADER_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=(PERSISTENT_WORKERS and DATALOADER_WORKERS>0), prefetch_factor=(PREFETCH_FACTOR if DATALOADER_WORKERS>0 else 2))\n",
        "\n",
        "\n",
        "train_loader = make_loader(train_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
        "val_loader = make_loader(val_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
        "test_loader = make_loader(test_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
        "\n",
        "tw, vw, tew = len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)\n",
        "tb, vb, teb = len(train_loader), len(val_loader), len(test_loader)\n",
        "print(f'DataLoaders -> windows (train/val/test): {tw}/{vw}/{tew} | batches: {tb}/{vb}/{teb} | batch_size={BATCH_SIZE}')\n",
        "print(f'Window shape -> L={SEQ_LEN}, C={train_scaled.shape[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29d00a1",
      "metadata": {
        "id": "c29d00a1"
      },
      "source": [
        "### Model: TimesBlock + Inception + Feature Extractor + Decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "q_0Oj0KHtrEF",
      "metadata": {
        "id": "q_0Oj0KHtrEF"
      },
      "outputs": [],
      "source": [
        "class InceptionBlock2D(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # Split out_channels across branches\n",
        "        base, rem = divmod(out_channels, 4)\n",
        "        branch_sizes = [base + (1 if i < rem else 0) for i in range(4)]\n",
        "        b1, b2, b3, b4 = branch_sizes\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, b1, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(b1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        red3 = max(in_channels // 2, 8)\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, red3, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(red3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(red3, b2, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(b2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        red5 = max(in_channels // 2, 8)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, red5, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(red5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(red5, b3, kernel_size=5, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(b3),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, b4, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(b4),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        o1 = self.branch1x1(x)\n",
        "        o2 = self.branch3x3(x)\n",
        "        o3 = self.branch5x5(x)\n",
        "        o4 = self.branch_pool(x)\n",
        "        out = torch.cat([o1, o2, o3, o4], dim=1)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class TimesBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    FFT-based period discovery + 1D->2D folding + 2D Inception backbone + weighted fusion.\n",
        "    Input:  x [B, L, C]\n",
        "    Output: embedding [B, E, H, W]\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.k = k\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_h = embed_h\n",
        "        self.embed_w = embed_w\n",
        "        self.backbone = InceptionBlock2D(in_channels, embed_dim, dropout=dropout)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _find_topk_periods(self, x_bc_l: torch.Tensor, k: int) -> Tuple[List[int], torch.Tensor]:\n",
        "        # x_bc_l: [B, C, L]\n",
        "        B, C, L = x_bc_l.shape\n",
        "        xf = torch.fft.rfft(x_bc_l, dim=-1)  # [B, C, L//2 + 1]\n",
        "        amp = xf.abs().mean(dim=(0, 1))      # [L//2 + 1], averaged over batch & channels\n",
        "        if amp.shape[0] <= 1:\n",
        "            return [L], torch.tensor([1.0], device=x_bc_l.device)\n",
        "\n",
        "        amp[0] = 0.0  # ignore DC\n",
        "        k_eff = min(k, amp.shape[0]-1)\n",
        "        vals, idxs = torch.topk(amp, k=k_eff, largest=True, sorted=True)\n",
        "        periods = []\n",
        "        for idx in idxs.tolist():\n",
        "            p = int(round(L / max(idx, 1)))\n",
        "            p = max(p, 2)\n",
        "            periods.append(p)\n",
        "        # Softmax weights from amplitudes\n",
        "        w = torch.softmax(vals, dim=0)\n",
        "        return periods, w\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [B, L, C]\n",
        "        B, L, C = x.shape\n",
        "        x_bc_l = x.permute(0, 2, 1).contiguous()  # [B, C, L]\n",
        "\n",
        "        periods, weights = self._find_topk_periods(x_bc_l, self.k)\n",
        "        feats = None\n",
        "        for i, p in enumerate(periods):\n",
        "            pad_len = (p - (L % p)) % p\n",
        "            x_pad = F.pad(x_bc_l, (0, pad_len), mode='constant', value=0.0)  # [B,C,Lp]\n",
        "            Lp = x_pad.shape[-1]\n",
        "            w_ = Lp // p\n",
        "            # Fold: [B, C, p, w_]\n",
        "            x_2d = x_pad.view(B, C, w_, p).transpose(2, 3).contiguous()\n",
        "\n",
        "            z = self.backbone(x_2d)  # [B, E, h, w]\n",
        "            z = F.adaptive_avg_pool2d(z, (self.embed_h, self.embed_w))  # [B,E,H,W]\n",
        "            z = z * weights[i].view(1, 1, 1, 1)  # weight this period\n",
        "\n",
        "            feats = z if feats is None else (feats + z)\n",
        "\n",
        "        return feats  # [B, E, H, W]\n",
        "\n",
        "class TimesNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, in_channels: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float=0.1):\n",
        "        super().__init__()\n",
        "        self.block = TimesBlock(in_channels, k, embed_dim, embed_h, embed_w, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [B, L, C]\n",
        "        emb = self.block(x)\n",
        "        return self.dropout(emb)  # [B, E, H, W]\n",
        "\n",
        "class ModelWithDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper for self-supervised training (reconstruction).\n",
        "    During inference, use extractor only.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, seq_len: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float=0.1):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.extractor = TimesNetFeatureExtractor(\n",
        "            in_channels=in_channels,\n",
        "            k=k,\n",
        "            embed_dim=embed_dim,\n",
        "            embed_h=embed_h,\n",
        "            embed_w=embed_w,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        flat_size = embed_dim * embed_h * embed_w\n",
        "        hidden = max(512, flat_size // 2)\n",
        "\n",
        "        # Using Sequential with explicit layers instead of Flatten\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(flat_size, hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden, seq_len * in_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x: [B, L, C]\n",
        "        emb = self.extractor(x)  # [B, E, H, W]\n",
        "        # Explicitly permute to channels_first before flattening with view\n",
        "        if emb.dim() == 4:\n",
        "             # Assuming channels_last if not channels_first or original channels\n",
        "             emb = emb.permute(0, 1, 2, 3).contiguous() # Ensure channels_first (no-op if already)\n",
        "\n",
        "        # Debug: print(f\"Shape before flattening: {emb.shape}\")\n",
        "        emb_flat = emb.view(emb.size(0), -1) # Explicit flatten\n",
        "        rec = self.decoder(emb_flat).view(x.shape[0], self.seq_len, self.in_channels)\n",
        "        return rec, emb\n",
        "\n",
        "    def strip_decoder(self):\n",
        "        self.decoder = None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "595ae284",
      "metadata": {
        "id": "595ae284"
      },
      "source": [
        "### Training Utilities: EarlyStopping, Checkpointing, Train/Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d40cbc82",
      "metadata": {
        "id": "d40cbc82"
      },
      "outputs": [],
      "source": [
        "from contextlib import nullcontext\n",
        "USE_AMP = bool(torch.cuda.is_available() and (DEVICE == 'cuda') and USE_BF16 and getattr(torch.cuda, 'is_bf16_supported', lambda: False)())\n",
        "AMP_DTYPE = torch.bfloat16 if USE_AMP else None\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 7, min_delta: float = 0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best = None\n",
        "        self.num_bad = 0\n",
        "\n",
        "    def step(self, value: float) -> bool:\n",
        "        if self.best is None or value < self.best - self.min_delta:\n",
        "            self.best = value\n",
        "            self.num_bad = 0\n",
        "            return False  # no stop\n",
        "        else:\n",
        "            self.num_bad += 1\n",
        "            return self.num_bad >= self.patience\n",
        "\n",
        "def save_checkpoint(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, epoch: int, best_val: float, extra: dict=None):\n",
        "    state = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"best_val_loss\": best_val,\n",
        "    }\n",
        "    if extra:\n",
        "        state[\"extra\"] = extra\n",
        "    torch.save(state, path)\n",
        "\n",
        "def load_checkpoint_if_any(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, resume: bool):\n",
        "    start_epoch = 1\n",
        "    best_val = float(\"inf\")\n",
        "    if resume and os.path.exists(path):\n",
        "        ckpt = torch.load(path, map_location=\"cpu\")\n",
        "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "        start_epoch = ckpt.get(\"epoch\", 1) + 1\n",
        "        best_val = ckpt.get(\"best_val_loss\", float(\"inf\"))\n",
        "        print(f\"Resuming from epoch {start_epoch-1} with best_val={best_val:.6f}\")\n",
        "    return start_epoch, best_val\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device, scheduler=None, clip_grad_norm: float=None, aug_noise_std: float=0.0):\n",
        "    model.train()\n",
        "    mse = nn.MSELoss()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    for i, xb in enumerate(loader):\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        if aug_noise_std and aug_noise_std > 0:\n",
        "            noise = torch.randn_like(xb) * float(aug_noise_std)\n",
        "            xb = xb + noise\n",
        "        ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if USE_AMP else nullcontext()\n",
        "        with ctx:\n",
        "            rec, emb = model(xb)\n",
        "            loss = mse(rec, xb) / max(int(GRAD_ACCUM_STEPS), 1)\n",
        "        loss.backward()\n",
        "        if (i + 1) % max(int(GRAD_ACCUM_STEPS), 1) == 0:\n",
        "            if clip_grad_norm is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(clip_grad_norm))\n",
        "            optimizer.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "        # Accumulate reporting with true scale\n",
        "        total_loss += (loss.item() * max(int(GRAD_ACCUM_STEPS), 1)) * xb.size(0)\n",
        "        n += xb.size(0)\n",
        "    return total_loss / max(n, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device, aug_noise_std: float=0.0):\n",
        "    model.eval()\n",
        "    mse = nn.MSELoss()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "    for xb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        # Augmentation should only be applied during training, remove this line\n",
        "        # if aug_noise_std and aug_noise_std > 0:\n",
        "        #     noise = torch.randn_like(xb) * float(aug_noise_std)\n",
        "        #     xb = xb + noise\n",
        "        ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if USE_AMP else nullcontext()\n",
        "        with ctx:\n",
        "            rec, emb = model(xb)\n",
        "            loss = mse(rec, xb)\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        n += xb.size(0)\n",
        "    return total_loss / max(n, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51358a74",
      "metadata": {
        "id": "51358a74"
      },
      "source": [
        "### Initialize Model, Optimizer, and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4375a4f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4375a4f0",
        "outputId": "1579e67b-afa8-4488-cf71-46f140674ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In-channels (features): 12\n",
            "Model params: 184.58M\n",
            "Resuming from epoch 19 with best_val=0.000000\n",
            "Starting training at epoch 20 on cuda | train windows=5606, batches/epoch=3, grad_accum=1, est_opt_steps/epoch=3\n",
            "Epoch 020 | train 1.007655 | val 6.017887 | 797 samp/s | lr=5.10e-05 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 021 | train 1.009290 | val 5.907022 | 10548 samp/s | lr=8.34e-05 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 022 | train 1.007252 | val 5.746515 | 11012 samp/s | lr=1.36e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 023 | train 1.001725 | val 5.484633 | 11449 samp/s | lr=2.06e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 024 | train 0.988819 | val 5.020320 | 11039 samp/s | lr=2.90e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 025 | train 0.964965 | val 4.136224 | 10845 samp/s | lr=3.85e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 026 | train 0.930967 | val 2.563833 | 11413 samp/s | lr=4.86e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 027 | train 0.892881 | val 1.068745 | 10879 samp/s | lr=5.88e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 028 | train 0.920454 | val 0.887363 | 10913 samp/s | lr=6.88e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 029 | train 0.940201 | val 0.697195 | 11345 samp/s | lr=7.80e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 030 | train 0.866126 | val 0.894496 | 11471 samp/s | lr=8.59e-04 | GPU peak mem: 15.57 GB | patience 1/30\n",
            "Epoch 031 | train 0.836053 | val 0.921925 | 11051 samp/s | lr=9.24e-04 | GPU peak mem: 15.57 GB | patience 2/30\n",
            "Epoch 032 | train 0.813787 | val 0.788744 | 10970 samp/s | lr=9.70e-04 | GPU peak mem: 15.57 GB | patience 3/30\n",
            "Epoch 033 | train 0.797699 | val 0.796002 | 11427 samp/s | lr=9.95e-04 | GPU peak mem: 15.57 GB | patience 4/30\n",
            "Epoch 034 | train 0.789239 | val 0.804734 | 11430 samp/s | lr=1.00e-03 | GPU peak mem: 15.57 GB | patience 5/30\n",
            "Epoch 035 | train 0.782351 | val 0.783657 | 10997 samp/s | lr=9.99e-04 | GPU peak mem: 15.57 GB | patience 6/30\n",
            "Epoch 036 | train 0.773809 | val 1.082551 | 10642 samp/s | lr=9.98e-04 | GPU peak mem: 15.57 GB | patience 7/30\n",
            "Epoch 037 | train 0.767889 | val 0.954861 | 10964 samp/s | lr=9.96e-04 | GPU peak mem: 15.57 GB | patience 8/30\n",
            "Epoch 038 | train 0.754722 | val 0.885866 | 11453 samp/s | lr=9.94e-04 | GPU peak mem: 15.57 GB | patience 9/30\n",
            "Epoch 039 | train 0.749968 | val 1.021962 | 11446 samp/s | lr=9.90e-04 | GPU peak mem: 15.57 GB | patience 10/30\n",
            "Epoch 040 | train 0.742546 | val 1.027768 | 10724 samp/s | lr=9.86e-04 | GPU peak mem: 15.57 GB | patience 11/30\n",
            "Epoch 041 | train 0.735862 | val 1.234038 | 10927 samp/s | lr=9.82e-04 | GPU peak mem: 15.57 GB | patience 12/30\n",
            "Epoch 042 | train 0.724722 | val 1.131990 | 11370 samp/s | lr=9.76e-04 | GPU peak mem: 15.57 GB | patience 13/30\n",
            "Epoch 043 | train 0.720301 | val 1.275293 | 11024 samp/s | lr=9.71e-04 | GPU peak mem: 15.57 GB | patience 14/30\n",
            "Epoch 044 | train 0.709232 | val 1.346649 | 10963 samp/s | lr=9.64e-04 | GPU peak mem: 15.57 GB | patience 15/30\n",
            "Epoch 045 | train 0.699407 | val 1.397532 | 10999 samp/s | lr=9.57e-04 | GPU peak mem: 15.57 GB | patience 16/30\n",
            "Epoch 046 | train 0.690882 | val 1.458232 | 11003 samp/s | lr=9.49e-04 | GPU peak mem: 15.57 GB | patience 17/30\n",
            "Epoch 047 | train 0.678696 | val 1.439729 | 11006 samp/s | lr=9.41e-04 | GPU peak mem: 15.57 GB | patience 18/30\n",
            "Epoch 048 | train 0.664747 | val 1.224131 | 10975 samp/s | lr=9.31e-04 | GPU peak mem: 15.57 GB | patience 19/30\n",
            "Epoch 049 | train 0.663100 | val 1.391582 | 10957 samp/s | lr=9.22e-04 | GPU peak mem: 15.57 GB | patience 20/30\n",
            "Epoch 050 | train 0.655477 | val 1.122764 | 10975 samp/s | lr=9.12e-04 | GPU peak mem: 15.57 GB | patience 21/30\n",
            "Epoch 051 | train 0.642469 | val 0.946226 | 11454 samp/s | lr=9.01e-04 | GPU peak mem: 15.57 GB | patience 22/30\n",
            "Epoch 052 | train 0.629207 | val 0.841946 | 11451 samp/s | lr=8.90e-04 | GPU peak mem: 15.57 GB | patience 23/30\n",
            "Epoch 053 | train 0.627814 | val 0.848961 | 10930 samp/s | lr=8.78e-04 | GPU peak mem: 15.57 GB | patience 24/30\n",
            "Epoch 054 | train 0.616845 | val 0.708266 | 10919 samp/s | lr=8.65e-04 | GPU peak mem: 15.57 GB | patience 25/30\n",
            "Epoch 055 | train 0.616097 | val 0.649031 | 11009 samp/s | lr=8.53e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 056 | train 0.604491 | val 0.593603 | 10716 samp/s | lr=8.39e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 057 | train 0.602715 | val 0.579762 | 11006 samp/s | lr=8.25e-04 | GPU peak mem: 15.57 GB | patience 0/30\n",
            "Epoch 058 | train 0.590923 | val 0.582352 | 10724 samp/s | lr=8.11e-04 | GPU peak mem: 15.57 GB | patience 1/30\n",
            "Epoch 059 | train 0.595051 | val 0.581877 | 11026 samp/s | lr=7.96e-04 | GPU peak mem: 15.57 GB | patience 2/30\n",
            "Epoch 060 | train 0.582484 | val 0.602725 | 11421 samp/s | lr=7.81e-04 | GPU peak mem: 15.57 GB | patience 3/30\n",
            "Epoch 061 | train 0.576251 | val 0.605274 | 11279 samp/s | lr=7.66e-04 | GPU peak mem: 15.57 GB | patience 4/30\n",
            "Epoch 062 | train 0.581437 | val 0.603182 | 11476 samp/s | lr=7.50e-04 | GPU peak mem: 15.57 GB | patience 5/30\n",
            "Epoch 063 | train 0.569647 | val 0.625235 | 11436 samp/s | lr=7.34e-04 | GPU peak mem: 15.57 GB | patience 6/30\n",
            "Epoch 064 | train 0.566500 | val 0.641563 | 11036 samp/s | lr=7.17e-04 | GPU peak mem: 15.57 GB | patience 7/30\n",
            "Epoch 065 | train 0.567552 | val 0.648114 | 10901 samp/s | lr=7.01e-04 | GPU peak mem: 15.57 GB | patience 8/30\n",
            "Epoch 066 | train 0.565885 | val 0.656510 | 11356 samp/s | lr=6.84e-04 | GPU peak mem: 15.57 GB | patience 9/30\n",
            "Epoch 067 | train 0.552112 | val 0.690977 | 11058 samp/s | lr=6.66e-04 | GPU peak mem: 15.57 GB | patience 10/30\n",
            "Epoch 068 | train 0.555748 | val 0.714003 | 10824 samp/s | lr=6.49e-04 | GPU peak mem: 15.57 GB | patience 11/30\n",
            "Epoch 069 | train 0.544161 | val 0.715091 | 11033 samp/s | lr=6.31e-04 | GPU peak mem: 15.57 GB | patience 12/30\n",
            "Epoch 070 | train 0.548416 | val 0.742613 | 11487 samp/s | lr=6.13e-04 | GPU peak mem: 15.57 GB | patience 13/30\n",
            "Epoch 071 | train 0.547492 | val 0.746370 | 11489 samp/s | lr=5.95e-04 | GPU peak mem: 15.57 GB | patience 14/30\n",
            "Epoch 072 | train 0.547866 | val 0.741278 | 10814 samp/s | lr=5.77e-04 | GPU peak mem: 15.57 GB | patience 15/30\n",
            "Epoch 073 | train 0.540815 | val 0.810085 | 10991 samp/s | lr=5.59e-04 | GPU peak mem: 15.57 GB | patience 16/30\n",
            "Epoch 074 | train 0.548055 | val 0.708269 | 10769 samp/s | lr=5.40e-04 | GPU peak mem: 15.57 GB | patience 17/30\n",
            "Epoch 075 | train 0.540896 | val 0.840454 | 11005 samp/s | lr=5.22e-04 | GPU peak mem: 15.57 GB | patience 18/30\n",
            "Epoch 076 | train 0.539276 | val 0.687616 | 11042 samp/s | lr=5.03e-04 | GPU peak mem: 15.57 GB | patience 19/30\n",
            "Epoch 077 | train 0.538189 | val 0.904923 | 11018 samp/s | lr=4.85e-04 | GPU peak mem: 15.57 GB | patience 20/30\n",
            "Epoch 078 | train 0.539673 | val 0.682324 | 11447 samp/s | lr=4.66e-04 | GPU peak mem: 15.57 GB | patience 21/30\n",
            "Epoch 079 | train 0.539709 | val 0.841790 | 11172 samp/s | lr=4.48e-04 | GPU peak mem: 15.57 GB | patience 22/30\n",
            "Epoch 080 | train 0.541616 | val 0.785110 | 11479 samp/s | lr=4.30e-04 | GPU peak mem: 15.57 GB | patience 23/30\n",
            "Epoch 081 | train 0.534868 | val 0.759763 | 11155 samp/s | lr=4.11e-04 | GPU peak mem: 15.57 GB | patience 24/30\n",
            "Epoch 082 | train 0.529945 | val 0.842924 | 11061 samp/s | lr=3.93e-04 | GPU peak mem: 15.57 GB | patience 25/30\n",
            "Epoch 083 | train 0.533576 | val 0.771279 | 11484 samp/s | lr=3.75e-04 | GPU peak mem: 15.57 GB | patience 26/30\n",
            "Epoch 084 | train 0.535692 | val 0.820646 | 11038 samp/s | lr=3.58e-04 | GPU peak mem: 15.57 GB | patience 27/30\n",
            "Epoch 085 | train 0.533795 | val 0.837241 | 10915 samp/s | lr=3.40e-04 | GPU peak mem: 15.57 GB | patience 28/30\n",
            "Epoch 086 | train 0.535786 | val 0.808756 | 11481 samp/s | lr=3.23e-04 | GPU peak mem: 15.57 GB | patience 29/30\n",
            "Epoch 087 | train 0.526281 | val 0.835062 | 11032 samp/s | lr=3.05e-04 | GPU peak mem: 15.57 GB | patience 30/30\n",
            "Early stopping at epoch 87. Best val: 0.000000\n"
          ]
        }
      ],
      "source": [
        "in_channels = train_scaled.shape[1]\n",
        "print(f'In-channels (features): {in_channels}')\n",
        "model = ModelWithDecoder(\n",
        "    in_channels=in_channels,\n",
        "    seq_len=SEQ_LEN,\n",
        "    k=TOP_K_PERIODS,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    embed_h=EMBED_H,\n",
        "    embed_w=EMBED_W,\n",
        "    dropout=DROPOUT_RATE\n",
        ").to(DEVICE)\n",
        "if CHANNELS_LAST and (DEVICE == 'cuda'):\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Model params: {params/1e6:.2f}M')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "start_epoch, best_val = load_checkpoint_if_any(CHECKPOINT_PATH, model, optimizer, resume=RESUME_TRAINING)\n",
        "\n",
        "# Ensure the optimizer's initial LR is set before creating the scheduler when resuming\n",
        "if RESUME_TRAINING and os.path.exists(CHECKPOINT_PATH):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = LEARNING_RATE\n",
        "\n",
        "scheduler = None\n",
        "if USE_SCHEDULER:\n",
        "    steps_per_epoch = max(1, (len(train_loader) + max(int(GRAD_ACCUM_STEPS),1) - 1) // max(int(GRAD_ACCUM_STEPS),1))\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=LEARNING_RATE, steps_per_epoch=steps_per_epoch, epochs=NUM_EPOCHS,\n",
        "        pct_start=ONECYCLE_PCT_START, div_factor=ONECYCLE_DIV_FACTOR, final_div_factor=ONECYCLE_FINAL_DIV,\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "training_history = []\n",
        "early = EarlyStopping(patience=PATIENCE, min_delta=0.0)\n",
        "\n",
        "tb, vb = len(train_loader), len(val_loader)\n",
        "tw, vw = len(train_loader.dataset), len(val_loader.dataset)\n",
        "est_opt_steps = (tb + max(int(GRAD_ACCUM_STEPS),1) - 1) // max(int(GRAD_ACCUM_STEPS),1)\n",
        "print(f\"Starting training at epoch {start_epoch} on {DEVICE} | train windows={tw}, batches/epoch={tb}, grad_accum={GRAD_ACCUM_STEPS}, est_opt_steps/epoch={est_opt_steps}\")\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    if DEVICE == 'cuda':\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    ep_start = time.perf_counter()\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, DEVICE, scheduler=scheduler, clip_grad_norm=CLIP_GRAD_NORM, aug_noise_std=AUG_NOISE_STD)\n",
        "    val_loss = evaluate(model, val_loader, DEVICE, aug_noise_std=0.0) # Pass 0.0 for augmentation noise during evaluation\n",
        "    ep_time = time.perf_counter() - ep_start\n",
        "    throughput = len(train_loader.dataset) / max(ep_time, 1e-9)\n",
        "\n",
        "    improved = val_loss < best_val - 1e-12\n",
        "    if improved:\n",
        "        best_val = val_loss\n",
        "        save_checkpoint(\n",
        "            CHECKPOINT_PATH, model, optimizer, epoch, best_val,\n",
        "            extra={\n",
        "                \"INDICATORS_TO_USE\": INDICATORS_TO_USE,\n",
        "                \"NORMALIZATION_TYPE\": NORMALIZATION_TYPE,\n",
        "                \"SEQ_LEN\": SEQ_LEN,\n",
        "                \"TOP_K_PERIODS\": TOP_K_PERIODS,\n",
        "                \"EMBED_DIM\": EMBED_DIM,\n",
        "                \"EMBED_H\": EMBED_H,\n",
        "                \"EMBED_W\": EMBED_W,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    stop = early.step(val_loss)\n",
        "    lr = optimizer.param_groups[0].get('lr', None)\n",
        "    msg = f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f} | {throughput:.0f} samp/s\"\n",
        "    if lr is not None:\n",
        "        msg += f\" | lr={lr:.2e}\"\n",
        "    if improved:\n",
        "        msg += \" | [saved]\"\n",
        "    if PRINT_GPU_MEM and (DEVICE == 'cuda'):\n",
        "        peak = torch.cuda.max_memory_allocated() / 1e9\n",
        "        msg += f\" | GPU peak mem: {peak:.2f} GB\"\n",
        "    print(msg + f\" | patience {early.num_bad}/{PATIENCE}\")\n",
        "    # Log epoch metrics\n",
        "    try:\n",
        "        import json as _json\n",
        "        rec = {\n",
        "            'epoch': int(epoch),\n",
        "            'train_loss': float(train_loss),\n",
        "            'val_loss': float(val_loss),\n",
        "            'lr': float(lr) if lr is not None else None,\n",
        "            'throughput': float(throughput),\n",
        "            'epoch_time': float(ep_time),\n",
        "            'gpu_peak_gb': float(peak) if (PRINT_GPU_MEM and (DEVICE=='cuda')) else None,\n",
        "            'improved': bool(improved),\n",
        "            'best_val': float(best_val)\n",
        "        }\n",
        "        training_history.append(rec)\n",
        "        with open(HISTORY_PATH, 'a') as f:\n",
        "            f.write(_json.dumps(rec) + '\\n')\n",
        "    except Exception as e:\n",
        "        print('WARN: falha ao gravar histórico:', e)\n",
        "    if stop:\n",
        "        print(f\"Early stopping at epoch {epoch}. Best val: {best_val:.6f}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b9b7db",
      "metadata": {
        "id": "94b9b7db"
      },
      "source": [
        "### Verification: Embedding-only forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167d28ad",
      "metadata": {
        "id": "167d28ad"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load best checkpoint\n",
        "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "model = ModelWithDecoder(\n",
        "    in_channels=in_channels,\n",
        "    seq_len=SEQ_LEN,\n",
        "    k=TOP_K_PERIODS,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    embed_h=EMBED_H,\n",
        "    embed_w=EMBED_W,\n",
        "    dropout=DROPOUT_RATE\n",
        ")\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Remove temporary reconstruction decoder\n",
        "model.strip_decoder()\n",
        "assert model.decoder is None\n",
        "\n",
        "# Take a sample batch from test set and compute embeddings\n",
        "xb = next(iter(test_loader))\n",
        "xb = xb.to(DEVICE, non_blocking=True)\n",
        "with torch.no_grad():\n",
        "    ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n",
        "    with ctx:\n",
        "        # Use the extractor directly for inference-only embeddings\n",
        "        emb = model.extractor(xb)  # [B, E, H, W]\n",
        "\n",
        "print(\"Embedding tensor shape:\", tuple(emb.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4209340c",
      "metadata": {
        "id": "4209340c"
      },
      "source": [
        "### Probing: Configuração"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8937a8db",
      "metadata": {
        "id": "8937a8db"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Probing configuration ===\n",
        "PROBE_HORIZON = 1  # steps ahead for label (adjust as needed)\n",
        "PROBE_RETURN_COL = 'close'  # which column to compute returns from\n",
        "PROBE_POOLING = 'avg'  # 'avg' over HxW or 'flatten'\n",
        "PROBE_REG_C = 1.0\n",
        "PROBE_MAX_ITER = 1000\n",
        "PROBE_RANDOM_STATE = 42\n",
        "\n",
        "# Regime detection windows (based on returns of PROBE_RETURN_COL)\n",
        "REG_TREND_WIN = 128  # steps for trend proxy\n",
        "REG_VOL_WIN = 128    # steps for realized volatility\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f50a8fb7",
      "metadata": {
        "id": "f50a8fb7"
      },
      "source": [
        "### Probing: Funções Utilitárias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1183568a",
      "metadata": {
        "id": "1183568a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "\n",
        "def compute_forward_returns(raw_split: np.ndarray, close_idx: int, horizon: int) -> np.ndarray:\n",
        "    close = raw_split[:, close_idx].astype(np.float64)\n",
        "    # log returns for stability\n",
        "    lr = np.zeros_like(close)\n",
        "    if horizon >= 1:\n",
        "        lr[:-horizon] = np.log(close[horizon:] / close[:-horizon])\n",
        "        lr[-horizon:] = np.nan\n",
        "    return lr\n",
        "\n",
        "\n",
        "def ds_valid_range(ds, horizon: int, total_len: int):\n",
        "    # windows whose end + horizon is within array\n",
        "    val_mask = []\n",
        "    for s in ds.idxs:\n",
        "        e = s + ds.seq_len\n",
        "        val_mask.append(e + horizon <= total_len)\n",
        "    return np.array(val_mask, dtype=bool)\n",
        "\n",
        "\n",
        "def window_labels_for_ds(ds, raw_split: np.ndarray, close_idx: int, horizon: int):\n",
        "    fwd_ret = compute_forward_returns(raw_split, close_idx, horizon)\n",
        "    mask = ds_valid_range(ds, horizon, len(raw_split))\n",
        "    labels = []\n",
        "    rets = []\n",
        "    for ok, s in zip(mask, ds.idxs):\n",
        "        if not ok:\n",
        "            break\n",
        "        e = s + ds.seq_len\n",
        "        r = fwd_ret[e-1]  # return immediately after the window end\n",
        "        labels.append(1 if r > 0 else 0)\n",
        "        rets.append(r)\n",
        "    return np.array(labels, dtype=np.int64), np.array(rets, dtype=np.float64), mask.sum()\n",
        "\n",
        "\n",
        "def extract_embeddings(loader, model, device, pooling: str = 'avg'):\n",
        "    model.eval()\n",
        "    vecs = []\n",
        "    with torch.no_grad():\n",
        "        ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n",
        "        for xb in loader:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            with ctx:\n",
        "                emb = model.extractor(xb)  # [B,E,H,W]\n",
        "            if pooling == 'avg':\n",
        "                v = emb.mean(dim=(2,3))  # [B,E]\n",
        "            else:\n",
        "                v = emb.view(emb.size(0), -1)\n",
        "            vecs.append(v.detach().to('cpu').numpy())\n",
        "    return np.concatenate(vecs, axis=0)\n",
        "\n",
        "\n",
        "def make_regimes(raw_split: np.ndarray, close_idx: int, trend_win: int, vol_win: int):\n",
        "    close = raw_split[:, close_idx].astype(np.float64)\n",
        "    lr = np.zeros_like(close)\n",
        "    lr[1:] = np.log(close[1:] / close[:-1])\n",
        "    # trend proxy: rolling mean return\n",
        "    trend = pd.Series(lr).rolling(trend_win, min_periods=trend_win//2).mean().to_numpy()\n",
        "    # vol proxy: rolling std of returns\n",
        "    vol = pd.Series(lr).rolling(vol_win, min_periods=vol_win//2).std(ddof=0).to_numpy()\n",
        "    # quantile thresholds\n",
        "    t_lo, t_hi = np.nanquantile(trend, [0.33, 0.67])\n",
        "    v_lo, v_hi = np.nanquantile(vol, [0.33, 0.67])\n",
        "    trend_reg = np.where(trend <= t_lo, -1, np.where(trend >= t_hi, 1, 0))  # -1 bear, 0 neutral, 1 bull\n",
        "    vol_reg = np.where(vol <= v_lo, 0, np.where(vol >= v_hi, 2, 1))  # 0 low,1 mid,2 high\n",
        "    return trend_reg, vol_reg\n",
        "\n",
        "\n",
        "def eval_probe(y_true, y_score, fwd_returns):\n",
        "    out = {}\n",
        "    out['roc_auc'] = roc_auc_score(y_true, y_score)\n",
        "    out['pr_auc'] = average_precision_score(y_true, y_score)\n",
        "    # IC: Spearman between predicted prob and realized return\n",
        "    vmask = ~np.isnan(fwd_returns)\n",
        "    ic, _ = spearmanr(y_score[vmask], fwd_returns[vmask])\n",
        "    out['ic'] = float(ic)\n",
        "    # ICIR: mean/std of rolling-50 ICs for stability\n",
        "    if vmask.sum() > 100:\n",
        "        import numpy as _np\n",
        "        wins = 50\n",
        "        ics = []\n",
        "        for i in range(0, vmask.sum()-wins+1):\n",
        "            seg = slice(i, i+wins)\n",
        "            c, _ = spearmanr(y_score[vmask][seg], fwd_returns[vmask][seg])\n",
        "            if _np.isfinite(c):\n",
        "                ics.append(c)\n",
        "        if len(ics) >= 2:\n",
        "            out['icir'] = float(_np.mean(ics) / ( _np.std(ics) + 1e-12))\n",
        "        else:\n",
        "            out['icir'] = float('nan')\n",
        "    else:\n",
        "        out['icir'] = float('nan')\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d8ecb95",
      "metadata": {
        "id": "7d8ecb95"
      },
      "source": [
        "### Probing: Pipeline e Métricas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bd1edfd",
      "metadata": {
        "id": "7bd1edfd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Probing pipeline: fit on val, eval on test ===\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Find column index for PROBE_RETURN_COL\n",
        "try:\n",
        "    close_idx = INDICATORS_TO_USE.index(PROBE_RETURN_COL)\n",
        "except ValueError:\n",
        "    raise ValueError(f\"PROBE_RETURN_COL={PROBE_RETURN_COL} não está em INDICATORS_TO_USE: {INDICATORS_TO_USE}\")\n",
        "\n",
        "# Labels aligned to windows\n",
        "y_val, r_val, n_val = window_labels_for_ds(val_loader.dataset, val_raw, close_idx, PROBE_HORIZON)\n",
        "y_test, r_test, n_test = window_labels_for_ds(test_loader.dataset, test_raw, close_idx, PROBE_HORIZON)\n",
        "\n",
        "# Embeddings extraídos e alinhados (mantendo apenas janelas válidas para o horizonte)\n",
        "X_val_full = extract_embeddings(val_loader, model, DEVICE, pooling=PROBE_POOLING)\n",
        "X_test_full = extract_embeddings(test_loader, model, DEVICE, pooling=PROBE_POOLING)\n",
        "X_val = X_val_full[:n_val]\n",
        "X_test = X_test_full[:n_test]\n",
        "\n",
        "print(f\"Embeddings -> val: {X_val.shape}, test: {X_test.shape}\")\n",
        "print(f\"Labels -> val: {y_val.shape}, test: {y_test.shape}\")\n",
        "\n",
        "# Probe: logistic regression balanceada\n",
        "probe = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('clf', LogisticRegression(\n",
        "        C=PROBE_REG_C,\n",
        "        max_iter=PROBE_MAX_ITER,\n",
        "        class_weight='balanced',\n",
        "        solver='lbfgs',\n",
        "        random_state=PROBE_RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "probe.fit(X_val, y_val)\n",
        "\n",
        "# Scores em probabilidade (classe positiva = retorno>0)\n",
        "proba_test = probe.predict_proba(X_test)[:,1]\n",
        "metrics_all = eval_probe(y_test, proba_test, r_test)\n",
        "print('--- Probe (TEST) - métricas gerais ---')\n",
        "for k, v in metrics_all.items():\n",
        "    print(f\"{k}: {v:.6f}\")\n",
        "\n",
        "# Métricas por regime\n",
        "trend_reg_test, vol_reg_test = make_regimes(test_raw, close_idx, REG_TREND_WIN, REG_VOL_WIN)\n",
        "# Alinhar regimes ao final da janela\n",
        "reg_aligned = trend_reg_test[SEQ_LEN-1:SEQ_LEN-1+n_test]\n",
        "vol_aligned = vol_reg_test[SEQ_LEN-1:SEQ_LEN-1+n_test]\n",
        "\n",
        "print('\n",
        "--- Por regime de tendência (bear=-1, neutral=0, bull=1) ---')\n",
        "for lbl in [-1,0,1]:\n",
        "    idx = np.where(reg_aligned == lbl)[0]\n",
        "    if len(idx) < 200:\n",
        "        continue\n",
        "    m = eval_probe(y_test[idx], proba_test[idx], r_test[idx])\n",
        "    print(f\"trend={lbl} | n={len(idx)} | auc={m['roc_auc']:.4f} | pr={m['pr_auc']:.4f} | ic={m['ic']:.4f} | icir={m['icir']:.4f}\")\n",
        "\n",
        "print('\n",
        "--- Por regime de volatilidade (0=low,1=mid,2=high) ---')\n",
        "for lbl in [0,1,2]:\n",
        "    idx = np.where(vol_aligned == lbl)[0]\n",
        "    if len(idx) < 200:\n",
        "        continue\n",
        "    m = eval_probe(y_test[idx], proba_test[idx], r_test[idx])\n",
        "    print(f\"vol={lbl} | n={len(idx)} | auc={m['roc_auc']:.4f} | pr={m['pr_auc']:.4f} | ic={m['ic']:.4f} | icir={m['icir']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "171dad07",
      "metadata": {
        "id": "171dad07"
      },
      "source": [
        "### Embeddings: Export para PPO Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec9b158",
      "metadata": {
        "id": "8ec9b158"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Exportar embeddings para consumo por PPO Agent ===\n",
        "import os, json as _json\n",
        "from pathlib import Path as _Path\n",
        "\n",
        "# Diretório de saída (ajuste se necessário)\n",
        "EMBED_SAVE_DIR = \"/content/drive/MyDrive/timesnet_mnq/embeddings\"  #@param {type:\"string\"}\n",
        "EMBED_POOLING_EXPORT = 'avg'  #@param [\"avg\", \"flatten\"]\n",
        "\n",
        "os.makedirs(EMBED_SAVE_DIR, exist_ok=True)\n",
        "print(f\"Salvando em: {EMBED_SAVE_DIR}\")\n",
        "\n",
        "# Função local de extração para evitar dependências de ordem de execução\n",
        "@torch.no_grad()\n",
        "def _extract_embeddings(loader, model, device, pooling: str = 'avg'):\n",
        "    model.eval()\n",
        "    vecs = []\n",
        "    ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n",
        "    for xb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        with ctx:\n",
        "            emb = model.extractor(xb)  # [B,E,H,W]\n",
        "        if pooling == 'avg':\n",
        "            v = emb.mean(dim=(2,3))  # [B,E]\n",
        "        else:\n",
        "            v = emb.view(emb.size(0), -1)\n",
        "        vecs.append(v.detach().to('cpu').numpy())\n",
        "    import numpy as _np\n",
        "    return _np.concatenate(vecs, axis=0)\n",
        "\n",
        "# Helper para alinhar janelas válidas (DataLoader usa drop_last=True)\n",
        "def _align_by_batches(loader, ds):\n",
        "    total = len(loader) * loader.batch_size\n",
        "    idxs = ds.idxs[:total]\n",
        "    import numpy as _np\n",
        "    start_idx = _np.array(idxs, dtype=_np.int64)\n",
        "    end_idx = start_idx + int(SEQ_LEN) - 1\n",
        "    return start_idx, end_idx\n",
        "\n",
        "splits = {\n",
        "    'train': (train_loader, train_raw),\n",
        "    'val':   (val_loader,   val_raw),\n",
        "    'test':  (test_loader,  test_raw),\n",
        "}\n",
        "\n",
        "saved = {}\n",
        "for name, (loader, raw) in splits.items():\n",
        "    X = _extract_embeddings(loader, model, DEVICE, pooling=EMBED_POOLING_EXPORT)\n",
        "    # Cortar para múltiplo do batch (drop_last)\n",
        "    N = len(loader) * loader.batch_size\n",
        "    X = X[:N]\n",
        "    start_idx, end_idx = _align_by_batches(loader, loader.dataset)\n",
        "    out_path = os.path.join(EMBED_SAVE_DIR, f\"{name}_emb_{EMBED_POOLING_EXPORT}.npz\")\n",
        "    import numpy as _np\n",
        "    _np.savez_compressed(out_path, X=X, start_idx=start_idx, end_idx=end_idx)\n",
        "    saved[name] = out_path\n",
        "    print(f\"{name}: salvo {X.shape} -> {out_path}\")\n",
        "\n",
        "# Metadados úteis para consumidores downstream\n",
        "meta = {\n",
        "    'seq_len': int(SEQ_LEN),\n",
        "    'window_stride': int(WINDOW_STRIDE),\n",
        "    'features': INDICATORS_TO_USE,\n",
        "    'normalization': NORMALIZATION_TYPE,\n",
        "    'embed_dim': int(EMBED_DIM),\n",
        "    'embed_hw': [int(EMBED_H), int(EMBED_W)],\n",
        "    'pooling': EMBED_POOLING_EXPORT,\n",
        "    'checkpoint_path': CHECKPOINT_PATH,\n",
        "    'files': saved,\n",
        "}\n",
        "meta_path = os.path.join(EMBED_SAVE_DIR, 'embeddings_meta.json')\n",
        "with open(meta_path, 'w') as f:\n",
        "    _json.dump(meta, f, indent=2)\n",
        "print(f\"Meta salvo em: {meta_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce3f893",
      "metadata": {
        "id": "5ce3f893"
      },
      "source": [
        "### Visualização Dinâmica\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fed431",
      "metadata": {
        "id": "71fed431"
      },
      "outputs": [],
      "source": [
        "\n",
        "# === Visualização dinâmica do histórico de treino ===\n",
        "import os, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as w\n",
        "\n",
        "HIST_PATH = HISTORY_PATH\n",
        "assert os.path.exists(HIST_PATH), f\"Histórico não encontrado em {HIST_PATH}. Treine primeiro.\"\n",
        "\n",
        "# Carrega JSON Lines\n",
        "rows = []\n",
        "with open(HIST_PATH, 'r') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            rows.append(json.loads(line))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "if df.empty:\n",
        "    raise SystemExit('Histórico vazio.')\n",
        "\n",
        "# Widgets\n",
        "smoothing = w.IntSlider(description='Smoothing', min=1, max=10, value=1)\n",
        "show_lr = w.Checkbox(value=True, description='Mostrar LR')\n",
        "show_throughput = w.Checkbox(value=False, description='Mostrar Throughput')\n",
        "show_gpu = w.Checkbox(value=False, description='Mostrar GPU Peak')\n",
        "epoch_range = w.IntRangeSlider(description='Épocas', min=int(df.epoch.min()), max=int(df.epoch.max()), value=[int(df.epoch.min()), int(df.epoch.max())], step=1)\n",
        "refresh = w.Button(description='Recarregar', button_style='')\n",
        "\n",
        "out = w.Output()\n",
        "\n",
        "# Plot function\n",
        "\n",
        "def _plot(*args):\n",
        "    with out:\n",
        "        clear_output(wait=True)\n",
        "        lo, hi = epoch_range.value\n",
        "        d = df[(df.epoch>=lo)&(df.epoch<=hi)].copy()\n",
        "        if smoothing.value>1:\n",
        "            d['train_s'] = d['train_loss'].rolling(smoothing.value, min_periods=1).mean()\n",
        "            d['val_s'] = d['val_loss'].rolling(smoothing.value, min_periods=1).mean()\n",
        "        else:\n",
        "            d['train_s'] = d['train_loss']\n",
        "            d['val_s'] = d['val_loss']\n",
        "        best_ep = int(df.loc[df.val_loss.idxmin(),'epoch'])\n",
        "        fig, ax1 = plt.subplots(1,1, figsize=(10,5))\n",
        "        ax1.plot(d['epoch'], d['train_s'], label='train (smoothed)')\n",
        "        ax1.plot(d['epoch'], d['val_s'], label='val (smoothed)')\n",
        "        ax1.axvline(best_ep, color='g', linestyle='--', alpha=0.5, label=f'best@{best_ep}')\n",
        "        ax1.set_xlabel('epoch')\n",
        "        ax1.set_ylabel('loss')\n",
        "        ax1.grid(True, alpha=0.2)\n",
        "        lines, labels = ax1.get_legend_handles_labels()\n",
        "        # Optional axes\n",
        "        if show_lr.value:\n",
        "            ax2 = ax1.twinx()\n",
        "            ax2.plot(d['epoch'], d['lr'], color='tab:purple', alpha=0.4, label='lr')\n",
        "            ax2.set_ylabel('lr')\n",
        "            l2, lab2 = ax2.get_legend_handles_labels()\n",
        "            lines += l2; labels += lab2\n",
        "        if show_throughput.value:\n",
        "            ax3 = ax1.twinx()\n",
        "            ax3.spines.right.set_position((\"axes\", 1.1))\n",
        "            ax3.plot(d['epoch'], d['throughput'], color='tab:orange', alpha=0.4, label='samp/s')\n",
        "            l3, lab3 = ax3.get_legend_handles_labels()\n",
        "            lines += l3; labels += lab3\n",
        "        if show_gpu.value and 'gpu_peak_gb' in d.columns:\n",
        "            ax4 = ax1.twinx()\n",
        "            ax4.spines.right.set_position((\"axes\", 1.2))\n",
        "            ax4.plot(d['epoch'], d['gpu_peak_gb'], color='tab:red', alpha=0.4, label='gpu peak GB')\n",
        "            l4, lab4 = ax4.get_legend_handles_labels()\n",
        "            lines += l4; labels += lab4\n",
        "        ax1.legend(lines, labels, loc='best')\n",
        "        plt.show()\n",
        "\n",
        "# Refresh handler\n",
        "\n",
        "def _reload(_btn):\n",
        "    global df\n",
        "    rows = []\n",
        "    with open(HIST_PATH, 'r') as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line: continue\n",
        "            try: rows.append(json.loads(line))\n",
        "            except Exception: pass\n",
        "    df = pd.DataFrame(rows)\n",
        "    epoch_range.max = int(max(epoch_range.max, df.epoch.max()))\n",
        "    epoch_range.value = [int(df.epoch.min()), int(df.epoch.max())]\n",
        "    _plot()\n",
        "\n",
        "refresh.on_click(_reload)\n",
        "for wdg in [smoothing, show_lr, show_throughput, show_gpu, epoch_range]:\n",
        "    wdg.observe(_plot, names='value')\n",
        "\n",
        "controls = w.HBox([smoothing, show_lr, show_throughput, show_gpu])\n",
        "display(controls, epoch_range, refresh, out)\n",
        "_plot()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}