{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "XWqhE-eMcrSn",
      "metadata": {
        "id": "XWqhE-eMcrSn"
      },
      "source": [
        "# 1) Setup & Install\n",
        "Install dependencies for Gymnasium, Stable-Baselines3 (SAC), and pandas-ta for technical indicators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91149d46",
      "metadata": {
        "id": "91149d46"
      },
      "outputs": [],
      "source": [
        "# 1) Setup & Install — deps for RL, plotting, and technical indicators\n",
        "# First, uninstall conflicting packages that may be pre-installed in the Colab environment.\n",
        "!pip uninstall -y dopamine-rl gym > /dev/null 2>&1\n",
        "\n",
        "import sys, subprocess, shlex\n",
        "def _pip(args: str):\n",
        "    cmd = [sys.executable, '-m', 'pip'] + shlex.split(args)\n",
        "    print('[pip]', ' '.join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Upgrade basics and satisfy IPython\n",
        "_pip('--quiet install --upgrade pip setuptools wheel')\n",
        "_pip('--quiet install jedi>=0.18.0')\n",
        "\n",
        "# Colab-friendly latest pins for RL stack\n",
        "PKGS = [\n",
        "    'numpy==2.1.4',\n",
        "    'pandas==2.2.2',\n",
        "    'pandas-ta==0.4.71b0',\n",
        "    'gymnasium==0.29.1',\n",
        "    'stable-baselines3==2.3.2',\n",
        "    'tensorboard',\n",
        "    'matplotlib',\n",
        "]\n",
        "_pip('--quiet install --upgrade --force-reinstall ' + ' '.join(PKGS))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9QptH4HRcrSp",
      "metadata": {
        "id": "9QptH4HRcrSp"
      },
      "source": [
        "# 2) Google Drive Mount\n",
        "Access TimesNet embeddings stored in Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Etn6wwtWcrSq",
      "metadata": {
        "id": "Etn6wwtWcrSq"
      },
      "outputs": [],
      "source": [
        "# 2) Google Drive Mount — access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y8FFJ5m-crSq",
      "metadata": {
        "id": "y8FFJ5m-crSq"
      },
      "source": [
        "# 3) Configuration\n",
        "Paths, market specifics, costs, rewards, and SAC hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53e0313",
      "metadata": {
        "id": "a53e0313"
      },
      "outputs": [],
      "source": [
        "# 3) Configuration — paths and parameters\n",
        "import os\n",
        "\n",
        "# Data paths (TimesNet embeddings)\n",
        "DATA_DIR = '/content/drive/MyDrive/timesnet_mnq/embeddings/'  # Corrected to be a directory\n",
        "TRAIN_NPZ = os.path.join(DATA_DIR, 'train_emb_avg.npz')\n",
        "VAL_NPZ   = os.path.join(DATA_DIR, 'val_emb_avg.npz')\n",
        "TEST_NPZ  = os.path.join(DATA_DIR, 'test_emb_avg.npz')\n",
        "META_JSON = os.path.join(DATA_DIR, 'embeddings_meta.json')\n",
        "\n",
        "# OHLC paths for ATR and indicators\n",
        "RAW_DATA_DIR = '/content'  # where you upload mnq_complete_dataset.csv in Colab\n",
        "TRAIN_OHLC_CSV = os.path.join(RAW_DATA_DIR, 'mnq_train.csv')\n",
        "VAL_OHLC_CSV   = os.path.join(RAW_DATA_DIR, 'mnq_val.csv')\n",
        "TEST_OHLC_CSV  = os.path.join(RAW_DATA_DIR, 'mnq_test.csv')\n",
        "OHLC_RESAMPLE_RULE = '5min'  # aggregate 1-min to 5-min to match TimesNet (pandas>=2 warning safe)\n",
        "\n",
        "# Bar timing (5-minute bars) and annualization\n",
        "BAR_SECONDS = 300\n",
        "STEPS_PER_DAY = 288  # 24*60/5\n",
        "STEPS_PER_YEAR = int(STEPS_PER_DAY * 252)\n",
        "\n",
        "# Market specifics\n",
        "POINT_VALUE_MNQ = 2.0      # USD per point\n",
        "TICK_SIZE = 0.25\n",
        "TICK_VALUE_USD = POINT_VALUE_MNQ * TICK_SIZE  # $0.50 per tick\n",
        "\n",
        "# Costs\n",
        "FIXED_COMMISSION_USD = 1.80  # round-trip all-in (entry+exit)\n",
        "\n",
        "# Risk parameters (training env termination)\n",
        "MAX_EPISODE_DRAWDOWN_USD = 500.0\n",
        "\n",
        "# Reward policy coefficients (scaled to be impactful)\n",
        "DRAWDOWN_PENALTY_COEFF = 0.03\n",
        "DRAWDOWN_PENALTY_CLIP = 0.3\n",
        "PROFIT_KEEPING_BONUS_COEFF = 0.5\n",
        "PATIENCE_BONUS = 0.0\n",
        "CHOPPY_ATR_TO_PRICE_MAX = 0.01\n",
        "\n",
        "\n",
        "# Environment\n",
        "EPISODE_LENGTH = 1024\n",
        "N_ENVS = 16\n",
        "SEED = 42\n",
        "\n",
        "# Toggles for ablations\n",
        "USE_ROLLING_ZSCORE = True   # If False, use raw features (no rolling Z-score)\n",
        "SOFT_ACTIONS_TRAIN = True   # Use continuous actions in training env (discretize only in backtest)\n",
        "USE_VECNORMALIZE  = True   # If False, skip VecNormalize wrapping\n",
        "\n",
        "# Extra reward shaping coefficients (small weights)\n",
        "FLIP_PENALTY_COEFF   = 0.005  # light penalty to reduce churn\n",
        "SOFT_ACTIONS_EPS     = 0.05   # threshold to treat small actions as flat when soft-actions is enabled\n",
        "SHARPE_BONUS_COEFF   = 0.5   # disable Sharpe incremental bonus for ablation\n",
        "\n",
        "# SAC (SB3) hyperparameters\n",
        "SAC_KW = dict(\n",
        "    learning_rate=1e-4,\n",
        "    buffer_size=1_000_000,\n",
        "    learning_starts=10000,\n",
        "    batch_size=512,\n",
        "    tau=0.005,\n",
        "    gamma=0.99,\n",
        "    train_freq=1,\n",
        "    gradient_steps=4,\n",
        "    ent_coef='auto_0.1',\n",
        "    target_entropy=-0.5,\n",
        "    target_update_interval=1,\n",
        ")\n",
        "\n",
        "# Training run cadence\n",
        "TOTAL_TIMESTEPS = 1_000_000\n",
        "EVAL_FREQ_STEPS = 10000\n",
        "LOG_DIR = '/content/drive/MyDrive/timesnet_mnq/sac_logs'\n",
        "BEST_MODEL_PATH = os.path.join(LOG_DIR, 'best_model.zip')\n",
        "# Reward scaling factor to stabilize SAC entropy dynamics\n",
        "REWARD_SCALE = 5.0\n",
        "os.makedirs(LOG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54aa538d",
      "metadata": {
        "id": "54aa538d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Assuming DATA_DIR is correctly set in a previous cell\n",
        "# If not, you may need to define it here or run the configuration cell first.\n",
        "try:\n",
        "    with np.load(TRAIN_NPZ, allow_pickle=False) as Z:\n",
        "        print(f\"Keys in {TRAIN_NPZ}: {list(Z.keys())}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {TRAIN_NPZ} not found. Please ensure DATA_DIR is set correctly and the file exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2191fec",
      "metadata": {
        "id": "a2191fec"
      },
      "source": [
        "# 4) Imports & Utilities\n",
        "Helpers for data loading, indicator calculation, and metrics (Calmar, Sharpe, MDD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f904ed4d",
      "metadata": {
        "id": "f904ed4d"
      },
      "outputs": [],
      "source": [
        "# 4) Imports & Utilities — loaders and metrics\n",
        "from typing import Optional\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas_ta as ta\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _read_ohlc_csv(csv_path: str, resample_rule: Optional[str] = None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    # Robust datetime detection: try common names, else scan all columns for parseable datetimes\n",
        "    time_col = None\n",
        "    for cand in ['timestamp', 'datetime', 'date', 'time']:\n",
        "        if cand in df.columns:\n",
        "            time_col = cand\n",
        "            break\n",
        "    if time_col is None and len(df.columns) > 0:\n",
        "        best_col = None\n",
        "        best_hits = -1\n",
        "        best_series = None\n",
        "        for c in df.columns:\n",
        "            s = df[c]\n",
        "            parsed = pd.to_datetime(s, errors='coerce', utc=True)\n",
        "            hits = int(parsed.notna().sum())\n",
        "            # Heuristic for epoch timestamps if parse failed heavily and dtype is numeric\n",
        "            if hits < int(0.8 * len(s)) and pd.api.types.is_numeric_dtype(s):\n",
        "                try:\n",
        "                    parsed_s = pd.to_datetime(s.astype('int64'), unit='ms', errors='coerce', utc=True)\n",
        "                    hits_ms = int(parsed_s.notna().sum())\n",
        "                    if hits_ms > hits:\n",
        "                        parsed, hits = parsed_s, hits_ms\n",
        "                except Exception:\n",
        "                    pass\n",
        "                try:\n",
        "                    parsed_s = pd.to_datetime(s.astype('int64'), unit='s', errors='coerce', utc=True)\n",
        "                    hits_s = int(parsed_s.notna().sum())\n",
        "                    if hits_s > hits:\n",
        "                        parsed, hits = parsed_s, hits_s\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if hits > best_hits:\n",
        "                best_col, best_hits, best_series = c, hits, parsed\n",
        "        if best_col is not None and best_hits >= int(0.8 * len(df)):\n",
        "            df[best_col] = best_series\n",
        "            time_col = best_col\n",
        "    if time_col is None:\n",
        "        raise TypeError(\n",
        "            'OHLC CSV must contain a datetime column (e.g., timestamp/datetime/date/time). Add/rename a time column to enable resampling.'\n",
        "        )\n",
        "    # Build DatetimeIndex and sort\n",
        "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce', utc=True)\n",
        "    df = df.sort_values(time_col).set_index(time_col)\n",
        "    if resample_rule:\n",
        "        # Match TimesNet resample semantics: right-closed, right-labeled bars\n",
        "        agg_base = {'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}\n",
        "        agg = {col: agg_base.get(col, 'last') for col in df.columns}\n",
        "        df = df.resample(resample_rule, label='right', closed='right').agg(agg)\n",
        "        df = df.dropna(how='any')\n",
        "    return df\n",
        "\n",
        "def _atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14) -> np.ndarray:\n",
        "    high = high.astype(np.float64)\n",
        "    low = low.astype(np.float64)\n",
        "    close = close.astype(np.float64)\n",
        "    n = len(close)\n",
        "    atr = np.empty(n, dtype=np.float64)\n",
        "    if n == 0: return atr\n",
        "    prev_close = close[0]\n",
        "    tr0 = max(high[0]-low[0], abs(high[0]-prev_close), abs(low[0]-prev_close))\n",
        "    atr[0] = tr0\n",
        "    for t in range(1, n):\n",
        "        tr = max(high[t]-low[t], abs(high[t]-close[t-1]), abs(low[t]-close[t-1]))\n",
        "        if t < period:\n",
        "            atr[t] = (atr[t-1]*t + tr) / (t+1)\n",
        "        else:\n",
        "            atr[t] = (atr[t-1]*(period-1) + tr) / period\n",
        "    return atr\n",
        "\n",
        "def load_split(npz_path: str, ohlc_csv_path: Optional[str] = None, resample_rule: Optional[str] = None, atr_period: int = 14):\n",
        "    with np.load(npz_path, allow_pickle=False) as Z:\n",
        "        obs = Z['obs'] if 'obs' in Z.files else Z['X']\n",
        "        close = Z['close'] if 'close' in Z.files else None\n",
        "        ts = None\n",
        "        for _k in ['timestamp','timestamps','time','datetime','date']:\n",
        "            if _k in Z.files:\n",
        "                ts = Z[_k]\n",
        "                break\n",
        "    if close is None:\n",
        "        raise ValueError('NPZ must contain close series')\n",
        "\n",
        "    explicit_features = None\n",
        "\n",
        "    if ohlc_csv_path:\n",
        "        df = _read_ohlc_csv(ohlc_csv_path, resample_rule)\n",
        "        required_ohlc_cols = ['open', 'high', 'low', 'close', 'volume']\n",
        "        if not all(c in df.columns for c in required_ohlc_cols):\n",
        "             raise ValueError(f'OHLC CSV must contain {required_ohlc_cols}')\n",
        "\n",
        "        # ### ALTERAÇÃO: Engenharia de Features - 13 indicadores explícitos ###\n",
        "        # 1. Calcular retornos percentuais em múltiplos horizontes\n",
        "        df['close_pct_return'] = df['close'].pct_change()\n",
        "        df['ret_3'] = df['close'].pct_change(3)\n",
        "        df['ret_6'] = df['close'].pct_change(6)\n",
        "        df['ret_12'] = df['close'].pct_change(12)\n",
        "        df['ret_24'] = df['close'].pct_change(24)\n",
        "\n",
        "        # 2. Calcular ATR (usando a mesma lógica de antes para consistência)\n",
        "        df['atr_14'] = _atr_wilder(df['high'].to_numpy(), df['low'].to_numpy(), df['close'].to_numpy(), period=14)\n",
        "\n",
        "        # 3. Calcular indicadores restantes com pandas-ta\n",
        "        df.ta.adx(length=14, append=True)\n",
        "        df.ta.ema(length=9, append=True)\n",
        "        df.ta.ema(length=21, append=True)\n",
        "        df.ta.vwap(append=True) # Usa H,L,C,V, que estão presentes\n",
        "        # Compatibilidade pandas-ta: algumas versões nomeiam 'VWAP' em vez de 'VWAP_D'\n",
        "        if 'VWAP_D' not in df.columns and 'VWAP' in df.columns:\n",
        "            df['VWAP_D'] = df['VWAP']\n",
        "        df.ta.rsi(length=21, append=True)\n",
        "        df.ta.stoch(k=14, d=3, smooth_k=3, append=True)\n",
        "\n",
        "        # 4. Relações simples preço/EMAs e spread\n",
        "        df['close_over_ema9'] = df['close'] / df['EMA_9']\n",
        "        df['close_over_ema21'] = df['close'] / df['EMA_21']\n",
        "        df['ema_spread_9_21'] = df['EMA_9'] - df['EMA_21']\n",
        "\n",
        "        # 5. Definir lista final de colunas de features e tratar NaNs\n",
        "        feature_cols = [\n",
        "            'open', 'high', 'low', 'close', 'volume', 'atr_14', 'ADX_14',\n",
        "            'EMA_9', 'EMA_21', 'VWAP_D', 'RSI_21', 'STOCHk_14_3_3',\n",
        "            'close_pct_return', 'ret_3', 'ret_6', 'ret_12', 'ret_24',\n",
        "            'close_over_ema9', 'close_over_ema21', 'ema_spread_9_21'\n",
        "        ]\n",
        "        # Preenche NaNs gerados no início das séries de indicadores\n",
        "        df = df.ffill().bfill()\n",
        "\n",
        "        # 6. Normalização (ablação controlada por USE_ROLLING_ZSCORE)\n",
        "        if USE_ROLLING_ZSCORE:\n",
        "            norm_window = STEPS_PER_DAY * 30 # ~30 dias\n",
        "            for col in feature_cols:\n",
        "                mean = df[col].rolling(window=norm_window, min_periods=2).mean()\n",
        "                std = df[col].rolling(window=norm_window, min_periods=2).std()\n",
        "                df[f'{col}_norm'] = (df[col] - mean) / std.replace(0, 1e-6)\n",
        "            df = df.fillna(0.0)\n",
        "            features_df = df[[f'{col}_norm' for col in feature_cols]]\n",
        "        else:\n",
        "            features_df = df[feature_cols].astype('float64').fillna(0.0)\n",
        "        # FIM DA ALTERAÇÃO\n",
        "\n",
        "        # Alinhamento dos dados (essencial para garantir que embeddings e features correspondam)\n",
        "        if ts is not None:\n",
        "            ts_idx = pd.to_datetime(ts, errors='coerce', utc=True)\n",
        "            aligned_features = features_df.reindex(ts_idx).ffill().bfill()\n",
        "            explicit_features = aligned_features.to_numpy()\n",
        "        else:\n",
        "            raw_features = features_df.to_numpy()\n",
        "            if len(raw_features) != len(close):\n",
        "                import warnings\n",
        "                warnings.warn(f'Features length ({len(raw_features)}) != close length ({len(close)}). Aligning by trimming.')\n",
        "                if len(raw_features) > len(close):\n",
        "                    explicit_features = raw_features[-len(close):]\n",
        "                else:\n",
        "                    pad = np.full((len(close) - len(raw_features), raw_features.shape[1]), 0.0, dtype=np.float64)\n",
        "                    explicit_features = np.concatenate([pad, raw_features], axis=0)\n",
        "            else:\n",
        "                explicit_features = raw_features\n",
        "\n",
        "    if explicit_features is None:\n",
        "         raise ValueError(\"Explicit features (OHLCV, indicators) could not be generated. Provide ohlc_csv_path.\")\n",
        "\n",
        "    # A função _atr_wilder é chamada internamente para o cálculo de 'atr_14'. O ATR para shaping de recompensa no env pode ser extraído do array já calculado.\n",
        "    atr_final = explicit_features[:, feature_cols.index('atr_14')] # Extrai a coluna ATR dos features normalizados\n",
        "    # No entanto, a recompensa usa ATR não-normalizado. Recalculamos aqui para manter a lógica original intacta.\n",
        "    df_temp_for_atr = _read_ohlc_csv(ohlc_csv_path, resample_rule)\n",
        "    atr_unnormalized = _atr_wilder(df_temp_for_atr['high'].values[-len(close):], df_temp_for_atr['low'].values[-len(close):], df_temp_for_atr['close'].values[-len(close):])\n",
        "\n",
        "    return obs.astype(np.float32), close.astype(np.float64), atr_unnormalized.astype(np.float64), explicit_features.astype(np.float32)\n",
        "\n",
        "def synth_time_features(n: int, steps_per_day: int = STEPS_PER_DAY) -> np.ndarray:\n",
        "    t = np.arange(n, dtype=np.float32)\n",
        "    hour_phase = 2*np.pi * (t % steps_per_day) / max(steps_per_day, 1)\n",
        "    week_phase = 2*np.pi * (t % (steps_per_day*5)) / max(steps_per_day*5, 1)\n",
        "    return np.stack([np.sin(hour_phase), np.cos(hour_phase), np.sin(week_phase), np.cos(week_phase)], axis=1).astype(np.float32)\n",
        "\n",
        "def map_action_to_position(action: int) -> int:\n",
        "    # 0->short(-1), 1->flat(0), 2->long(+1)\n",
        "    return (-1 if action == 0 else (0 if action == 1 else 1))\n",
        "\n",
        "def pnl_from_price_change(prev_pos: int, price_change: float, point_value: float) -> float:\n",
        "    return float(prev_pos * price_change * point_value)\n",
        "\n",
        "def compute_drawdown(equity: np.ndarray) -> np.ndarray:\n",
        "    # Robust to negative equity: clamp ratio to [0,1] so DD ∈ [0,100%]\n",
        "    peaks = np.maximum.accumulate(np.maximum(equity, 1e-9))\n",
        "    ratio = np.divide(equity, peaks, out=np.zeros_like(peaks, dtype=np.float64), where=peaks>0)\n",
        "    dd = 1.0 - np.clip(ratio, 0.0, 1.0)\n",
        "    return dd\n",
        "\n",
        "def max_drawdown(equity: np.ndarray) -> float:\n",
        "    dd = compute_drawdown(equity)\n",
        "    return float(np.nanmax(dd) if len(dd) else 0.0)\n",
        "\n",
        "def cagr_from_equity(equity: np.ndarray) -> float:\n",
        "    n = len(equity)\n",
        "    if n < 2:\n",
        "        return 0.0\n",
        "    start = float(equity[0])\n",
        "    years = n / max(STEPS_PER_YEAR, 1)\n",
        "    if years <= 0 or start <= 0:\n",
        "        return 0.0\n",
        "    end = float(equity[-1])\n",
        "    if end <= 0:\n",
        "        return -1.0\n",
        "    ratio = end / start\n",
        "    return float(ratio ** (1.0 / years) - 1.0)\n",
        "\n",
        "def calmar_ratio(equity: np.ndarray) -> float:\n",
        "    mdd = max_drawdown(equity)\n",
        "    cagr = cagr_from_equity(equity)\n",
        "    if not np.isfinite(mdd) or not np.isfinite(cagr):\n",
        "        return 0.0\n",
        "    if mdd <= 1e-12:\n",
        "        return float('inf') if cagr > 0 else 0.0\n",
        "    return cagr / mdd\n",
        "\n",
        "def sharpe_ratio(returns: np.ndarray, risk_free=0.0) -> float:\n",
        "    if len(returns) < 2:\n",
        "        return 0.0\n",
        "    mean = float(np.mean(returns))\n",
        "    std = float(np.std(returns, ddof=1))\n",
        "    if std <= 1e-12:\n",
        "        return 0.0\n",
        "    ann_factor = math.sqrt(STEPS_PER_YEAR)\n",
        "    return float((mean - risk_free) / std * ann_factor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a073b2",
      "metadata": {
        "id": "e9a073b2"
      },
      "source": [
        "# 4.1) Imports & Utilities II\n",
        "Materializar OHLC por split para features (alinha com os NPZ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "materialize_ohlc",
      "metadata": {
        "id": "materialize_ohlc"
      },
      "outputs": [],
      "source": [
        "# Use quando você tiver um único CSV base (mnq_complete_dataset.csv) e quiser gerar train/val/test CSVs.\n",
        "# Requer que os NPZ (TRAIN_NPZ/VAL_NPZ/TEST_NPZ) já existam no DATA_DIR.\n",
        "BASE_OHLC_CSV = os.path.join(RAW_DATA_DIR, 'mnq_complete_dataset.csv')\n",
        "if os.path.exists(BASE_OHLC_CSV):\n",
        "    df_all = _read_ohlc_csv(BASE_OHLC_CSV, resample_rule=OHLC_RESAMPLE_RULE)\n",
        "    def _len_close(npz_path):\n",
        "        with np.load(npz_path, allow_pickle=False) as Z:\n",
        "            return len(Z['close'] if 'close' in Z.files else Z['X'])\n",
        "    n_train = _len_close(TRAIN_NPZ)\n",
        "    n_val   = _len_close(VAL_NPZ)\n",
        "    n_test  = _len_close(TEST_NPZ)\n",
        "    total   = n_train + n_val + n_test\n",
        "    if len(df_all) < total:\n",
        "        raise ValueError(f'OHLC length {len(df_all)} < NPZ total {total}. Verifique CSV base e resample.')\n",
        "    # Compensa warm-up/limpezas no TimesNet: corta excesso do início para casar os comprimentos\n",
        "    excess = len(df_all) - total\n",
        "    if excess > 0:\n",
        "        df_all = df_all.iloc[excess:]\n",
        "    assert len(df_all) == total\n",
        "    os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
        "    _train_csv = os.path.join(RAW_DATA_DIR, 'mnq_train.csv')\n",
        "    _val_csv   = os.path.join(RAW_DATA_DIR, 'mnq_val.csv')\n",
        "    _test_csv  = os.path.join(RAW_DATA_DIR, 'mnq_test.csv')\n",
        "    # Preserve the time index as a column named 'timestamp' for downstream resampling/alignment\n",
        "    df_all = df_all.copy()\n",
        "    if df_all.index.name is None:\n",
        "        df_all.index.name = 'timestamp'\n",
        "    df_all.iloc[:n_train].to_csv(_train_csv, index=True)\n",
        "    df_all.iloc[n_train:n_train+n_val].to_csv(_val_csv, index=True)\n",
        "    df_all.iloc[n_train+n_val:].to_csv(_test_csv, index=True)\n",
        "    print('Wrote:', _train_csv, _val_csv, _test_csv)\n",
        "    # Atualiza caminhos usados pelo SAC\n",
        "    TRAIN_OHLC_CSV = _train_csv\n",
        "    VAL_OHLC_CSV   = _val_csv\n",
        "    TEST_OHLC_CSV  = _test_csv\n",
        "else:\n",
        "    print(f'[Aviso] BASE_OHLC_CSV não encontrado em: {BASE_OHLC_CSV}')\n",
        "    print('[Ação] Faça upload do mnq_complete_dataset.csv em RAW_DATA_DIR ou ajuste RAW_DATA_DIR/BASE_OHLC_CSV.')\n",
        "    print('[Ação] Se já possui train/val/test CSVs, defina TRAIN_OHLC_CSV/VAL_OHLC_CSV/TEST_OHLC_CSV e pule esta célula.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bsCOCr0ocrSs",
      "metadata": {
        "id": "bsCOCr0ocrSs"
      },
      "source": [
        "# 5) Environment (MNQEmbEnv)\n",
        "Training-only risk shaping on embeddings + explicit features; no hard stops.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00036e3d",
      "metadata": {
        "id": "00036e3d"
      },
      "outputs": [],
      "source": [
        "# 5) Environment — MNQEmbEnv\n",
        "class MNQEmbEnv(gym.Env):\n",
        "    metadata = {'render.modes': []}\n",
        "    def __init__(\n",
        "        self,\n",
        "        npz_path: str,\n",
        "        meta_path: Optional[str] = None,\n",
        "        ohlc_csv_path: Optional[str] = None,\n",
        "        ohlc_resample_rule: Optional[str] = None,\n",
        "        episode_length: int = EPISODE_LENGTH,\n",
        "        point_value: float = POINT_VALUE_MNQ,\n",
        "        fixed_commission_usd: float = FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff: float = DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff: float = PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus: float = PATIENCE_BONUS,\n",
        "        choppy_atr_to_price_max: float = CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd: float = MAX_EPISODE_DRAWDOWN_USD,\n",
        "        dd_penalty_clip: float = DRAWDOWN_PENALTY_CLIP,\n",
        "        reward_scale: float = REWARD_SCALE,\n",
        "        flip_penalty_coeff: float = FLIP_PENALTY_COEFF,\n",
        "        sharpe_bonus_coeff: float = SHARPE_BONUS_COEFF,\n",
        "        discretize_actions: bool = True,\n",
        "        seed: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        # ### ALTERAÇÃO: Capturar o array de features explícitos retornado por load_split ###\n",
        "        self.obs_raw, self.close, self.atr, self.explicit_features = load_split(npz_path, ohlc_csv_path=ohlc_csv_path, resample_rule=ohlc_resample_rule)\n",
        "        self.meta = {}\n",
        "        if meta_path and os.path.exists(meta_path):\n",
        "            with open(meta_path, 'r') as f:\n",
        "                self.meta = json.load(f)\n",
        "        self.T, self.D_emb = self.obs_raw.shape\n",
        "        self.time_features = synth_time_features(self.T, STEPS_PER_DAY)\n",
        "        self.point_value = float(point_value)\n",
        "        self.fixed_commission = float(fixed_commission_usd)\n",
        "        self.dd_penalty = float(drawdown_penalty_coeff)\n",
        "        self.profit_bonus = float(profit_keeping_bonus_coeff)\n",
        "        self.patience_bonus_v = float(patience_bonus)\n",
        "        self.choppy_thr = float(choppy_atr_to_price_max)\n",
        "        self.max_episode_dd = float(max_episode_drawdown_usd)\n",
        "        self.episode_length = int(episode_length)\n",
        "        self.dd_penalty_clip = float(dd_penalty_clip)\n",
        "        self.reward_scale = float(reward_scale)\n",
        "        self.flip_penalty_coeff = float(flip_penalty_coeff)\n",
        "        self.sharpe_bonus_coeff = float(sharpe_bonus_coeff)\n",
        "        self.discretize_actions = bool(discretize_actions)\n",
        "\n",
        "        # ### ALTERAÇÃO: Atualizar o espaço de observação para incluir os features explícitos ###\n",
        "        self.D_explicit = self.explicit_features.shape[1]\n",
        "        # Observation: [embedding D_emb] + [explicit D_explicit] + [time 4] + [position 1] + [portfolio 3]\n",
        "        self.obs_dim = self.D_emb + self.D_explicit + 4 + 1 + 3\n",
        "        self.action_space = spaces.Box(low=np.array([-1.0], dtype=np.float32), high=np.array([1.0], dtype=np.float32), shape=(1,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_dim,), dtype=np.float32)\n",
        "        self._reset_state()\n",
        "\n",
        "    def _reset_state(self):\n",
        "        min_start = 1\n",
        "        max_start = max(min_start, self.T - self.episode_length - 2)\n",
        "        self.start = int(self.rng.integers(min_start, max_start+1)) if max_start > min_start else min_start\n",
        "        self.step_in_ep = 0\n",
        "        self.current_step = self.start\n",
        "        self.current_position = 0\n",
        "        self.entry_price = 0.0\n",
        "        self.best_price = 0.0\n",
        "        self.entry_step = 0\n",
        "        self.equity = 1000.0\n",
        "        self.peak_equity = self.equity\n",
        "        self.equity_series = [self.equity]\n",
        "        self.returns_series = []\n",
        "\n",
        "    def is_market_choppy(self, t: int) -> bool:\n",
        "        price = float(self.close[t])\n",
        "        if price <= 0:\n",
        "            return False\n",
        "        return (float(self.atr[t]) / price) <= self.choppy_thr\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        emb = self.obs_raw[self.current_step].astype(np.float32)\n",
        "        # ### ALTERAÇÃO: Adicionar features explícitos ao vetor de observação ###\n",
        "        explicit = self.explicit_features[self.current_step].astype(np.float32)\n",
        "        tf = self.time_features[self.current_step].astype(np.float32)\n",
        "        pf = np.array([self.current_position], dtype=np.float32)\n",
        "        # === Portfolio features ===\n",
        "        if self.current_position != 0:\n",
        "            atr_entry = float(max(self.atr[self.entry_step], 1e-9))\n",
        "            pnl_usd = (float(self.close[self.current_step]) - self.entry_price) * self.current_position * self.point_value\n",
        "            pnl_norm = float(pnl_usd) / max(atr_entry * self.point_value, 1e-9)\n",
        "            bars_in_trade = max(0, self.current_step - int(self.entry_step))\n",
        "        else:\n",
        "            pnl_norm = 0.0\n",
        "            bars_in_trade = 0\n",
        "        dur_norm = float(bars_in_trade) / max(self.episode_length, 1)\n",
        "        dd_usd = float(self.peak_equity - self.equity)\n",
        "        dd_norm = dd_usd / max(self.max_episode_dd, 1e-6)\n",
        "        portfolio_state = np.array([pnl_norm, dur_norm, dd_norm], dtype=np.float32)\n",
        "\n",
        "        # ### ALTERAÇÃO: Concatenar o vetor de estado com a nova estrutura ###\n",
        "        return np.concatenate([emb, explicit, tf, pf, portfolio_state], axis=0)\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options=None):\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "        self._reset_state()\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # Map continuous action a∈[-1,1] to {-1,0,+1} via rounding\n",
        "        if isinstance(action, (list, tuple, np.ndarray)):\n",
        "            a = float(action[0])\n",
        "        else:\n",
        "            a = float(action)\n",
        "        a = float(np.clip(a, -1.0, 1.0))\n",
        "        if self.discretize_actions:\n",
        "            new_pos = int(np.round(a))\n",
        "        else:\n",
        "            new_pos = (0.0 if abs(a) < SOFT_ACTIONS_EPS else float(a))\n",
        "        prev_pos = self.current_position\n",
        "        t = self.current_step\n",
        "        prev_t = max(0, t-1)\n",
        "\n",
        "        # Price change and gross PnL for the previous position\n",
        "        price_change = float(self.close[t] - self.close[prev_t])\n",
        "        gross_pnl = pnl_from_price_change(prev_pos, price_change, self.point_value)\n",
        "\n",
        "        # Transaction costs: charge full round-trip cost only on new trade entry (from flat or reversal)\n",
        "        costs = 0.0\n",
        "        is_new_trade_entry = ((prev_pos == 0 and new_pos != 0) or\n",
        "                              (np.sign(new_pos) != np.sign(prev_pos) and new_pos != 0))\n",
        "        entry_intended_price = None\n",
        "        if is_new_trade_entry:\n",
        "            commission = self.fixed_commission\n",
        "            entry_intended_price = float(self.close[t])\n",
        "            costs = commission\n",
        "        base_reward = gross_pnl - costs\n",
        "\n",
        "        # Unrealized PnL of the active trade (based on prev_pos and prior entry)\n",
        "        if prev_pos != 0:\n",
        "            unrealized_pnl_prev = (float(self.close[t]) - self.entry_price) * prev_pos * self.point_value\n",
        "        else:\n",
        "            unrealized_pnl_prev = 0.0\n",
        "\n",
        "        # Risk penalties and behavioral incentives\n",
        "        # Normalized per-step drawdown penalty (only when a position is open)\n",
        "        if prev_pos != 0:\n",
        "            atr_entry = float(max(self.atr[self.entry_step], 1e-9))\n",
        "            dd_usd = max(0.0, -unrealized_pnl_prev)\n",
        "            dd_norm = dd_usd / max(atr_entry * self.point_value, 1e-9)\n",
        "        else:\n",
        "            dd_norm = 0.0\n",
        "        drawdown_penalty = self.dd_penalty * min(dd_norm, self.dd_penalty_clip)\n",
        "        current_trade_profit = max(0.0, unrealized_pnl_prev)\n",
        "        profit_keeping_bonus = self.profit_bonus * current_trade_profit\n",
        "        patience_bonus = (self.patience_bonus_v if self.is_market_choppy(t) and new_pos == 0 else\n",
        "                          (-self.patience_bonus_v if self.is_market_choppy(t) and new_pos != 0 else 0.0))\n",
        "        # Flip penalty (discourage frequent position changes)\n",
        "        flip_flag = int((np.sign(new_pos) != np.sign(prev_pos)) and (new_pos != 0))\n",
        "        flip_penalty = self.flip_penalty_coeff * flip_flag\n",
        "\n",
        "        # Sharpe incremental shaping (small weight)\n",
        "        step_return_temp = (gross_pnl - costs) / max(abs(self.equity_series[-1]), 1e-9)\n",
        "        if len(self.returns_series) >= 2:\n",
        "            prev_sh = sharpe_ratio(np.array(self.returns_series, dtype=np.float64))\n",
        "            new_sh = sharpe_ratio(np.array(self.returns_series + [step_return_temp], dtype=np.float64))\n",
        "            sharpe_bonus = self.sharpe_bonus_coeff * (new_sh - prev_sh)\n",
        "        else:\n",
        "            sharpe_bonus = 0.0\n",
        "\n",
        "        # Scale only the base PnL component, keep penalty terms outside to avoid collapse to flat policy\n",
        "        final_reward = (self.reward_scale * base_reward) - drawdown_penalty - flip_penalty + profit_keeping_bonus + patience_bonus + sharpe_bonus\n",
        "\n",
        "        # Update equity statistics\n",
        "        self.equity += gross_pnl - costs\n",
        "        self.returns_series.append((gross_pnl - costs) / max(abs(self.equity_series[-1]), 1e-9))\n",
        "        self.equity_series.append(self.equity)\n",
        "        self.peak_equity = max(self.peak_equity, self.equity)\n",
        "        episode_drawdown = self.peak_equity - self.equity\n",
        "        terminated = episode_drawdown > self.max_episode_dd\n",
        "\n",
        "        # Trade state management: handle new entries, exits, and reversals\n",
        "        position_flipped = (np.sign(new_pos) != np.sign(prev_pos))\n",
        "        if (prev_pos == 0 and new_pos != 0) or (position_flipped and new_pos != 0):\n",
        "            # New trade started (from flat or by reversal)\n",
        "            self.entry_price = float(self.close[t])\n",
        "            self.best_price = self.entry_price\n",
        "            self.entry_step = int(t)\n",
        "        elif new_pos == 0 and prev_pos != 0:\n",
        "            # Trade closed\n",
        "            self.entry_price = 0.0\n",
        "            self.best_price = 0.0\n",
        "            self.entry_step = int(self.current_step)\n",
        "        elif new_pos == prev_pos and new_pos != 0:\n",
        "            # Position maintained: update best price favorably\n",
        "            if new_pos > 0:\n",
        "                self.best_price = max(self.best_price, float(self.close[t]))\n",
        "            else:\n",
        "                self.best_price = min(self.best_price, float(self.close[t]))\n",
        "\n",
        "        # Advance step\n",
        "        self.current_position = new_pos\n",
        "        self.current_step += 1\n",
        "        self.step_in_ep += 1\n",
        "        truncated = (self.step_in_ep >= self.episode_length) or (self.current_step >= (self.T - 1))\n",
        "        obs = self._get_obs()\n",
        "        info = {\n",
        "            'gross_pnl': gross_pnl,\n",
        "            'costs': costs,\n",
        "            'equity': self.equity,\n",
        "            'drawdown': episode_drawdown,\n",
        "            'dd_norm_step': float(dd_norm),\n",
        "            'position': self.current_position,\n",
        "            'entry_intended_price': entry_intended_price,\n",
        "        }\n",
        "        return obs, float(final_reward), bool(terminated), bool(truncated), info\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc606ad",
      "metadata": {
        "id": "cfc606ad"
      },
      "source": [
        "# 6) Backtester\n",
        "Deterministic backtest with continuous action mapping and enriched state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1199aa85",
      "metadata": {
        "id": "1199aa85"
      },
      "outputs": [],
      "source": [
        "# 6) Backtester — supports VecNormalize for obs.\n",
        "def run_backtest(model, env_like: MNQEmbEnv, deterministic=True, collect_reports=False, vecnormalize=None):\n",
        "    obs_raw = env_like.obs_raw\n",
        "    close = env_like.close\n",
        "    atr = env_like.atr\n",
        "    # ### ALTERAÇÃO: Carregar os features explícitos a partir do objeto do ambiente ###\n",
        "    explicit_features = env_like.explicit_features\n",
        "\n",
        "    # ### ALTERAÇÃO: Garantir que o comprimento T seja consistente entre todos os arrays de dados ###\n",
        "    T = min(len(obs_raw), len(close), len(atr), len(explicit_features))\n",
        "    close = close[:T]\n",
        "    atr = atr[:T]\n",
        "    explicit_features = explicit_features[:T]\n",
        "    obs_raw = obs_raw[:T]\n",
        "\n",
        "    position = 0\n",
        "    entry_price = 0.0\n",
        "    entry_step = None\n",
        "    best_price = 0.0\n",
        "    equity = 1000.0\n",
        "    peak_equity = equity\n",
        "    equity_series = [equity]\n",
        "    step_returns = []\n",
        "    tf_all = synth_time_features(T, STEPS_PER_DAY)\n",
        "    trade_reports = [] if collect_reports else None\n",
        "    for t in range(1, T):\n",
        "        # === CONSTRUÇÃO DE ESTADO IDÊNTICA AO AMBIENTE DE TREINO ===\n",
        "        if position != 0:\n",
        "            atr_entry_idx = entry_step if entry_step is not None else t\n",
        "            atr_entry = float(max(atr[atr_entry_idx], 1e-9))\n",
        "            pnl_usd = (float(close[t]) - entry_price) * position * env_like.point_value\n",
        "            pnl_norm = pnl_usd / max(atr_entry * env_like.point_value, 1e-9)\n",
        "            bars_in_trade = max(0, t - atr_entry_idx)\n",
        "        else:\n",
        "            pnl_norm = 0.0\n",
        "            bars_in_trade = 0\n",
        "\n",
        "        dur_norm = float(bars_in_trade) / max(EPISODE_LENGTH, 1)\n",
        "        dd_usd = float(max(peak_equity - equity, 0.0))\n",
        "        dd_norm = dd_usd / max(MAX_EPISODE_DRAWDOWN_USD, 1e-6)\n",
        "\n",
        "        portfolio_state = np.array([pnl_norm, dur_norm, dd_norm], dtype=np.float32)\n",
        "\n",
        "        # ### ALTERAÇÃO: Adicionar features explícitos ao vetor de estado do backtest ###\n",
        "        state = np.concatenate([\n",
        "            obs_raw[t].astype(np.float32),\n",
        "            explicit_features[t].astype(np.float32), # Adicionado!\n",
        "            tf_all[t].astype(np.float32),\n",
        "            np.array([position], dtype=np.float32),\n",
        "            portfolio_state\n",
        "        ])\n",
        "        # Apply VecNormalize stats if provided\n",
        "        if vecnormalize is not None:\n",
        "            mean = getattr(vecnormalize.obs_rms, 'mean', None)\n",
        "            var = getattr(vecnormalize.obs_rms, 'var', None)\n",
        "            eps = getattr(vecnormalize, 'epsilon', 1e-8)\n",
        "            clip = getattr(vecnormalize, 'clip_obs', 10.0)\n",
        "            if (mean is not None) and (var is not None):\n",
        "                state_n = np.clip((state - mean) / np.sqrt(var + eps), -clip, clip).astype(np.float32)\n",
        "            else:\n",
        "                state_n = state.astype(np.float32)\n",
        "        else:\n",
        "            state_n = state.astype(np.float32)\n",
        "        # ==========================================================\n",
        "\n",
        "        action, _ = model.predict(state_n, deterministic=deterministic)\n",
        "        a = float(action[0]) if isinstance(action, (list, tuple, np.ndarray)) else float(action)\n",
        "        a = float(np.clip(a, -1.0, 1.0))\n",
        "        desired_pos = int(np.round(a))\n",
        "\n",
        "        # 3) Execute decision (apply PnL for previous position)\n",
        "        prev_pos = position\n",
        "        price_change = float(close[t] - close[t-1])\n",
        "        gross_pnl = pnl_from_price_change(prev_pos, price_change, env_like.point_value)\n",
        "        costs = 0.0\n",
        "        is_new_trade_entry = ((prev_pos == 0 and desired_pos != 0) or (np.sign(desired_pos) != np.sign(prev_pos) and desired_pos != 0))\n",
        "        if is_new_trade_entry:\n",
        "            commission = env_like.fixed_commission\n",
        "            costs = commission\n",
        "            # Build trade report (entry)\n",
        "            if collect_reports:\n",
        "                intended_price = float(close[t])\n",
        "                trade_reports.append(dict(\n",
        "                    t=int(t),\n",
        "                    side=int(np.sign(desired_pos)),\n",
        "                    entry_intended_price=intended_price,\n",
        "                    commission_usd=float(commission),\n",
        "                    costs_charged_usd=float(costs),\n",
        "                ))\n",
        "        equity += gross_pnl - costs\n",
        "        peak_equity = max(peak_equity, equity)\n",
        "        equity_series.append(equity)\n",
        "        step_returns.append((gross_pnl - costs) / max(abs(equity_series[-2]), 1e-9))\n",
        "\n",
        "        # 4) Update trade state (entries, exits, reversals)\n",
        "        if (prev_pos == 0 and desired_pos != 0) or (np.sign(desired_pos) != np.sign(prev_pos) and desired_pos != 0):\n",
        "            entry_price = float(close[t])\n",
        "            entry_step = int(t)\n",
        "            best_price = entry_price\n",
        "        elif desired_pos == 0 and prev_pos != 0:\n",
        "            entry_price = 0.0\n",
        "            entry_step = None\n",
        "            best_price = 0.0\n",
        "        elif desired_pos == prev_pos and desired_pos != 0:\n",
        "            if desired_pos > 0:\n",
        "                best_price = max(best_price, float(close[t]))\n",
        "            else:\n",
        "                best_price = min(best_price, float(close[t]))\n",
        "        position = desired_pos\n",
        "\n",
        "    # Always return a 3-tuple for type stability; trade_reports may be None\n",
        "    return np.asarray(equity_series, dtype=np.float64), np.asarray(step_returns, dtype=np.float64), trade_reports\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Izp94ERhcrSt",
      "metadata": {
        "id": "Izp94ERhcrSt"
      },
      "source": [
        "# 7) Calmar Callback\n",
        "Early stopping by Calmar on validation backtests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sYe3YHXOcrSu",
      "metadata": {
        "id": "sYe3YHXOcrSu"
      },
      "outputs": [],
      "source": [
        "# 7) Calmar Callback — validation selection\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.logger import Figure\n",
        "\n",
        "class CalmarCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_env_ctor,\n",
        "        eval_env_kwargs,\n",
        "        eval_freq,\n",
        "        best_model_save_path,\n",
        "        verbose=1,\n",
        "        early_stop_patience=None,\n",
        "        early_stop_min_delta=0.0,\n",
        "        early_stop_target=None,\n",
        "        also_eval_test=False,\n",
        "        test_env_ctor=None,\n",
        "        test_env_kwargs=None,\n",
        "    ):\n",
        "        super().__init__(verbose)\n",
        "        self.eval_env_ctor = eval_env_ctor\n",
        "        self.eval_env_kwargs = eval_env_kwargs or {}\n",
        "        self.eval_freq = int(eval_freq)\n",
        "        self.best_model_save_path = best_model_save_path\n",
        "        self.best_calmar = -np.inf\n",
        "        # Early stopping config\n",
        "        self.early_patience = (int(early_stop_patience) if early_stop_patience is not None else None)\n",
        "        self.early_min_delta = float(early_stop_min_delta)\n",
        "        self.early_target = (float(early_stop_target) if early_stop_target is not None else None)\n",
        "        self._no_improve_evals = 0\n",
        "        # Optional second evaluation on TEST split\n",
        "        self.also_eval_test = bool(also_eval_test)\n",
        "        self.test_env_ctor = test_env_ctor\n",
        "        self.test_env_kwargs = test_env_kwargs or {}\n",
        "        # live logging defaults\n",
        "        self.log_tb = True\n",
        "        self.plot_figures = True\n",
        "        self.save_csv = False\n",
        "        try:\n",
        "            import os as _os\n",
        "            self.log_dir = _os.path.dirname(best_model_save_path)\n",
        "        except Exception:\n",
        "            self.log_dir = None\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.eval_freq != 0:\n",
        "            return True\n",
        "        eval_env = self.eval_env_ctor(**self.eval_env_kwargs)\n",
        "        # Collect trade_reports to log number of trades during validation\n",
        "        equity, step_returns, trade_reports = run_backtest(self.model, eval_env, deterministic=True, collect_reports=True, vecnormalize=self.model.get_vec_normalize_env())\n",
        "        calmar = calmar_ratio(equity)\n",
        "        sharpe = sharpe_ratio(step_returns)\n",
        "        mdd_v = max_drawdown(equity)\n",
        "        cagr_v = cagr_from_equity(equity)\n",
        "        trades_count = len(trade_reports) if trade_reports is not None else 0\n",
        "        if self.verbose:\n",
        "            print(f'[CalmarCallback] step={self.n_calls} calmar={calmar:.4f} best={self.best_calmar:.4f} | sharpe={sharpe:.3f} mdd={mdd_v:.2%} cagr={cagr_v:.2%} | trades={trades_count}')\n",
        "        # TensorBoard scalars + figures\n",
        "        if self.log_tb:\n",
        "            self.logger.record('eval/calmar', float(calmar))\n",
        "            self.logger.record('eval/sharpe', float(sharpe))\n",
        "            self.logger.record('eval/mdd', float(mdd_v))\n",
        "            self.logger.record('eval/cagr', float(cagr_v))\n",
        "            self.logger.record('eval/trades', int(trades_count))\n",
        "            if self.plot_figures:\n",
        "                fig1 = plt.figure(figsize=(8, 2.4))\n",
        "                plt.plot(equity)\n",
        "                plt.title('Equity (val)')\n",
        "                plt.grid(True)\n",
        "                self.logger.record('eval/equity_curve', Figure(fig1, close=True), exclude=('stdout',))\n",
        "                dd = compute_drawdown(equity)\n",
        "                fig2 = plt.figure(figsize=(8, 2.0))\n",
        "                plt.plot(dd)\n",
        "                plt.title('Drawdown (val)')\n",
        "                plt.grid(True)\n",
        "                self.logger.record('eval/drawdown_curve', Figure(fig2, close=True), exclude=('stdout',))\n",
        "        # Optional CSV snapshot\n",
        "        if self.save_csv and self.log_dir is not None:\n",
        "            try:\n",
        "                import os\n",
        "                import numpy as _np\n",
        "                os.makedirs(self.log_dir, exist_ok=True)\n",
        "                _np.savetxt(os.path.join(self.log_dir, f'equity_step_{self.n_calls}.csv'), equity, delimiter=',')\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print('[CalmarCallback] CSV save failed:', e)\n",
        "        # Optionally evaluate TEST split as well\n",
        "        if self.also_eval_test:\n",
        "            test_env = (self.test_env_ctor or self.eval_env_ctor)(**self.test_env_kwargs)\n",
        "            teq, tret, treports = run_backtest(self.model, test_env, deterministic=True, collect_reports=True, vecnormalize=self.model.get_vec_normalize_env())\n",
        "            tcalmar = calmar_ratio(teq)\n",
        "            tsharpe = sharpe_ratio(tret)\n",
        "            tmdd = max_drawdown(teq)\n",
        "            tcagr = cagr_from_equity(teq)\n",
        "            ttrades = len(treports) if treports is not None else 0\n",
        "            if self.verbose:\n",
        "                print(f'[CalmarCallback][TEST] step={self.n_calls} calmar={tcalmar:.4f} | sharpe={tsharpe:.3f} mdd={tmdd:.2%} cagr={tcagr:.2%} | trades={ttrades}')\n",
        "            if self.log_tb:\n",
        "                self.logger.record('test/calmar', float(tcalmar))\n",
        "                self.logger.record('test/sharpe', float(tsharpe))\n",
        "                self.logger.record('test/mdd', float(tmdd))\n",
        "                self.logger.record('test/cagr', float(tcagr))\n",
        "                self.logger.record('test/trades', int(ttrades))\n",
        "                if self.plot_figures:\n",
        "                    figt1 = plt.figure(figsize=(8, 2.4))\n",
        "                    plt.plot(teq)\n",
        "                    plt.title('Equity (test)')\n",
        "                    plt.grid(True)\n",
        "                    self.logger.record('test/equity_curve', Figure(figt1, close=True), exclude=('stdout',))\n",
        "                    tdd = compute_drawdown(teq)\n",
        "                    figt2 = plt.figure(figsize=(8, 2.0))\n",
        "                    plt.plot(tdd)\n",
        "                    plt.title('Drawdown (test)')\n",
        "                    plt.grid(True)\n",
        "                    self.logger.record('test/drawdown_curve', Figure(figt2, close=True), exclude=('stdout',))\n",
        "        improved = (calmar > (self.best_calmar + self.early_min_delta))\n",
        "        if improved:\n",
        "            self.best_calmar = calmar\n",
        "            self._no_improve_evals = 0\n",
        "            self.model.save(self.best_model_save_path)\n",
        "            # Save VecNormalize stats alongside the best model for consistent evaluation\n",
        "            try:\n",
        "                ve = self.model.get_vec_normalize_env()\n",
        "                if ve is not None:\n",
        "                    import os as _os\n",
        "                    _dir = _os.path.dirname(self.best_model_save_path)\n",
        "                    _os.makedirs(_dir, exist_ok=True)\n",
        "                    ve.save(_os.path.join(_dir, 'vecnormalize_best.pkl'))\n",
        "            except Exception as _e:\n",
        "                if self.verbose:\n",
        "                    print('[CalmarCallback] VecNormalize best save failed:', _e)\n",
        "        else:\n",
        "            self._no_improve_evals += 1\n",
        "        # Early target stop\n",
        "        if (self.early_target is not None) and (calmar >= self.early_target):\n",
        "            if self.verbose:\n",
        "                print(f'[CalmarCallback] Early stop: target calmar {self.early_target:.4f} reached at step={self.n_calls}.')\n",
        "            return False\n",
        "        # Early patience stop\n",
        "        if (self.early_patience is not None) and (self._no_improve_evals >= self.early_patience):\n",
        "            if self.verbose:\n",
        "                print(f'[CalmarCallback] Early stop: no improvement in {self.early_patience} evals (best={self.best_calmar:.4f}).')\n",
        "            return False\n",
        "        return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "progress_cb",
      "metadata": {
        "id": "progress_cb"
      },
      "outputs": [],
      "source": [
        "# 7.1) Progress Printer Callback — heartbeat during training\n",
        "import time as _time\n",
        "import numpy as _np\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class ProgressPrinterCallback(BaseCallback):\n",
        "    def __init__(self, print_freq: int = 5000, verbose: int = 1):\n",
        "        super().__init__(verbose)\n",
        "        self.print_freq = int(print_freq)\n",
        "        self._t0 = None\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        self._t0 = _time.time()\n",
        "        try:\n",
        "            ent_cfg = getattr(self.model, 'ent_coef', None)\n",
        "        except Exception:\n",
        "            ent_cfg = None\n",
        "        if self.verbose:\n",
        "            print(f'[Train][Start] timesteps=0 | ent_coef={ent_cfg} | USE_VECNORMALIZE={USE_VECNORMALIZE} | USE_ROLLING_ZSCORE={USE_ROLLING_ZSCORE}')\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if (self.n_calls % self.print_freq) != 0:\n",
        "            return True\n",
        "        # Gather replay buffer info\n",
        "        rb_size = rb_cap = None\n",
        "        try:\n",
        "            rb = getattr(self.model, 'replay_buffer', None)\n",
        "            if rb is not None:\n",
        "                rb_size = (rb.size() if hasattr(rb, 'size') else getattr(rb, 'pos', None))\n",
        "                rb_cap = getattr(rb, 'buffer_size', None)\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Entropy coefficient (auto)\n",
        "        ent_val = None\n",
        "        try:\n",
        "            import torch as _t\n",
        "            lec = getattr(self.model, 'log_ent_coef', None)\n",
        "            if lec is not None:\n",
        "                ent_val = float(_t.exp(lec).item())\n",
        "            else:\n",
        "                ec = getattr(self.model, 'ent_coef', None)\n",
        "                if isinstance(ec, (float, int)):\n",
        "                    ent_val = float(ec)\n",
        "        except Exception:\n",
        "            pass\n",
        "        # Learning rate (actor)\n",
        "        lr_val = None\n",
        "        try:\n",
        "            lr_val = float(self.model.actor.optimizer.param_groups[0]['lr'])\n",
        "        except Exception:\n",
        "            pass\n",
        "        elapsed = (_time.time() - self._t0) if self._t0 else None\n",
        "        msg = '[Train] step={} | timesteps={} | rb={}/{} | ent_coef={} | lr={}'.format(self.n_calls, self.model.num_timesteps, rb_size, rb_cap, ent_val if ent_val is not None else 'n/a', lr_val if lr_val is not None else 'n/a')\n",
        "        if elapsed is not None:\n",
        "            msg += f\" | elapsed={elapsed/60:.1f}m' .format(elapsed/60)\n",
        "        print(msg)\n",
        "        return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lqrrzL21crSu",
      "metadata": {
        "id": "lqrrzL21crSu"
      },
      "source": [
        "# 8) Training — SB3 SAC\n",
        "Vectorized envs + VecNormalize, CalmarCallback, and learn() loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0edd20df",
      "metadata": {
        "id": "0edd20df"
      },
      "outputs": [],
      "source": [
        "# 8) Training — SB3 SAC\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, VecNormalize\n",
        "from stable_baselines3.common.callbacks import CallbackList\n",
        "\n",
        "# Seed global RNGs for reproducibility\n",
        "import random\n",
        "import numpy as _np\n",
        "_np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    try:\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "    except Exception:\n",
        "        pass\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def make_train_env():\n",
        "    return MNQEmbEnv(\n",
        "        npz_path=TRAIN_NPZ, meta_path=META_JSON, ohlc_csv_path=TRAIN_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE,\n",
        "        discretize_actions= (not SOFT_ACTIONS_TRAIN), seed=SEED\n",
        "    )\n",
        "\n",
        "def make_val_env():\n",
        "    return MNQEmbEnv(\n",
        "        npz_path=VAL_NPZ, meta_path=META_JSON, ohlc_csv_path=VAL_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED+1\n",
        "    )\n",
        "\n",
        "# Build vectorized env with optional observation normalization (reward unnormalized)\n",
        "base_env = DummyVecEnv([make_train_env for _ in range(N_ENVS)])\n",
        "if USE_VECNORMALIZE:\n",
        "    vec_env = VecNormalize(base_env, norm_obs=True, norm_reward=False)\n",
        "else:\n",
        "    vec_env = base_env\n",
        "vec_env = VecMonitor(vec_env, filename=None)\n",
        "\n",
        "# Policy with ReLU, moderate net_arch, AdamW (weight decay), and Dropout features extractor\n",
        "import torch\n",
        "from torch import nn\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "\n",
        "class DualPathExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, *, d_emb, d_explicit, d_ctx=8, p=0.2, hidden_emb=128, hidden_tech=64):\n",
        "        super().__init__(observation_space, features_dim=hidden_emb + hidden_tech + int(d_ctx))\n",
        "        self.d_emb = int(d_emb)\n",
        "        self.d_explicit = int(d_explicit)\n",
        "        self.d_ctx = int(d_ctx)\n",
        "        self.lin_emb = nn.Linear(self.d_emb, int(hidden_emb))\n",
        "        self.lin_tech = nn.Linear(self.d_explicit, int(hidden_tech))\n",
        "        self.ln_ctx = nn.LayerNorm(self.d_ctx)\n",
        "        self.act = nn.ReLU()\n",
        "        self.drop = nn.Dropout(p=float(p))\n",
        "        self._features_dim = int(hidden_emb + hidden_tech + self.d_ctx)\n",
        "    def forward(self, observations):\n",
        "        x = observations\n",
        "        emb = x[..., :self.d_emb]\n",
        "        tech = x[..., self.d_emb:self.d_emb + self.d_explicit]\n",
        "        ctx = x[..., self.d_emb + self.d_explicit:self.d_emb + self.d_explicit + self.d_ctx]\n",
        "        emb_enc = self.drop(self.act(self.lin_emb(emb)))\n",
        "        tech_enc = self.drop(self.act(self.lin_tech(tech)))\n",
        "        ctx_enc = self.ln_ctx(ctx)\n",
        "        return torch.cat([emb_enc, tech_enc, ctx_enc], dim=-1)\n",
        "\n",
        "# Infer emb/feature dims to parametrize DualPathExtractor\n",
        "_tmp_env = MNQEmbEnv(\n",
        "    npz_path=TRAIN_NPZ, meta_path=META_JSON, ohlc_csv_path=TRAIN_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "    point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "    drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF, profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "    patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "    max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED\n",
        ")\n",
        "D_EMB = int(_tmp_env.D_emb)\n",
        "D_EXPL = int(_tmp_env.D_explicit)\n",
        "D_CTX = 8\n",
        "del _tmp_env\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    net_arch=[1024, 512],\n",
        "    activation_fn=nn.ReLU,\n",
        "    optimizer_class=torch.optim.AdamW,\n",
        "    optimizer_kwargs=dict(weight_decay=1e-4),\n",
        "    features_extractor_class=DualPathExtractor,\n",
        "    features_extractor_kwargs=dict(d_emb=D_EMB, d_explicit=D_EXPL, d_ctx=D_CTX, p=0.3),\n",
        ")\n",
        "\n",
        "model = SAC('MlpPolicy', vec_env, policy_kwargs=policy_kwargs, tensorboard_log=LOG_DIR, seed=SEED, **SAC_KW)\n",
        "\n",
        "calmar_cb = CalmarCallback(\n",
        "    eval_env_ctor=MNQEmbEnv,\n",
        "    eval_env_kwargs=dict(\n",
        "        npz_path=VAL_NPZ, meta_path=META_JSON, ohlc_csv_path=VAL_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED+123\n",
        "    ),\n",
        "    eval_freq=EVAL_FREQ_STEPS,\n",
        "    best_model_save_path=BEST_MODEL_PATH,\n",
        "    verbose=1,\n",
        "    early_stop_patience=10,         # patience for Calmar early stopping\n",
        "    early_stop_min_delta=0.0,       # require strictly better Calmar\n",
        "    early_stop_target=None,         # or set a target Calmar to stop early\n",
        "    also_eval_test=True,\n",
        "    test_env_ctor=MNQEmbEnv,\n",
        "    test_env_kwargs=dict(\n",
        "        npz_path=TEST_NPZ, meta_path=META_JSON, ohlc_csv_path=TEST_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED+456\n",
        "    )\n",
        ")\n",
        "\n",
        "# Train with Calmar validation only (entropy schedule disabled for stability)\n",
        "# Combine progress heartbeat and Calmar validation callbacks\n",
        "progress_cb = ProgressPrinterCallback(print_freq=5000, verbose=1)\n",
        "cb_list = CallbackList([progress_cb, calmar_cb])\n",
        "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=cb_list, progress_bar=True)\n",
        "# Save VecNormalize stats for deterministic evaluation/backtesting\n",
        "try:\n",
        "    import os as _os\n",
        "    vec_path = _os.path.join(LOG_DIR, 'vecnormalize.pkl')\n",
        "    vec_env.save(vec_path)\n",
        "    print(f'[VecNormalize] saved stats to {vec_path}')\n",
        "except Exception as e:\n",
        "    print('[VecNormalize] save failed:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-BF0Js9wcrSu",
      "metadata": {
        "id": "-BF0Js9wcrSu"
      },
      "source": [
        "# 9) Final Backtest & Plots\n",
        "Evaluate best model (SAC) on the test split and visualize results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f60fdf",
      "metadata": {
        "id": "39f60fdf"
      },
      "outputs": [],
      "source": [
        "# 9) Final Backtest & Plots — test split evaluation\n",
        "from stable_baselines3 import SAC\n",
        "best_model = SAC.load(BEST_MODEL_PATH, device='auto')\n",
        "# Load VecNormalize stats saved after training to normalize observations in backtest\n",
        "from stable_baselines3.common.vec_env import VecNormalize as _VecNorm, DummyVecEnv as _DummyVecEnv\n",
        "vec_stats = None\n",
        "try:\n",
        "    # Attach stats to a dummy env with the same observation space\n",
        "    _dummy = _DummyVecEnv([make_val_env])\n",
        "    # Prefer best-model VecNormalize stats; fallback to final stats if needed\n",
        "    best_path = os.path.join(LOG_DIR, 'vecnormalize_best.pkl')\n",
        "    final_path = os.path.join(LOG_DIR, 'vecnormalize.pkl')\n",
        "    load_path = best_path if os.path.exists(best_path) else final_path\n",
        "    vec_stats = _VecNorm.load(load_path, _dummy)\n",
        "    vec_stats.training = False\n",
        "    vec_stats.norm_reward = False\n",
        "    print('[VecNormalize] loaded stats for backtest')\n",
        "except Exception as e:\n",
        "    print('[VecNormalize] load failed:', e)\n",
        "test_env = MNQEmbEnv(\n",
        "    npz_path=TEST_NPZ, meta_path=META_JSON, ohlc_csv_path=TEST_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=10**9,\n",
        "    point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "    drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "    profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "    patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "    max_episode_drawdown_usd=1e12, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED+999\n",
        ")\n",
        "equity, step_returns, trade_reports = run_backtest(best_model, test_env, deterministic=True, collect_reports=True, vecnormalize=vec_stats)\n",
        "# Log trade count and breakdown\n",
        "num_trades = len(trade_reports) if trade_reports is not None else 0\n",
        "num_longs = sum(1 for r in (trade_reports or []) if r.get('side', 0) > 0)\n",
        "num_shorts = sum(1 for r in (trade_reports or []) if r.get('side', 0) < 0)\n",
        "print(f'Trades: {num_trades} (long: {num_longs}, short: {num_shorts})')\n",
        "\n",
        "calmar = calmar_ratio(equity)\n",
        "sharpe = sharpe_ratio(step_returns)\n",
        "mdd = max_drawdown(equity)\n",
        "cagr = cagr_from_equity(equity)\n",
        "print(f'Test Calmar: {calmar:.3f} | Sharpe: {sharpe:.3f} | MDD: {mdd:.2%} | CAGR: {cagr:.2%}')\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(equity)\n",
        "plt.title('Equity Curve (Test)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "dd = compute_drawdown(equity)\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.plot(dd)\n",
        "plt.title('Drawdown (Test)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tb_viewer_hdr",
      "metadata": {
        "id": "tb_viewer_hdr"
      },
      "source": [
        "# 10) TensorBoard Viewer\n",
        "Visualize training and evaluation logs recorded by SB3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tb_viewer_cell",
      "metadata": {
        "id": "tb_viewer_cell"
      },
      "outputs": [],
      "source": [
        "# 10) TensorBoard Viewer — launch inside notebook\n",
        "%load_ext tensorboard\n",
        "print(f\"Using LOG_DIR={LOG_DIR}\")\n",
        "%tensorboard --logdir \"$LOG_DIR\" --port 6006\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}