{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bc944697",
      "metadata": {},
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a50c71d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "CSV_PATH = \"mnq_complete_dataset.csv\"  #@param {type:\"string\"}\n",
        "\n",
        "# Features to use (OHLCV + 7 TA indicators; all lowercase)\n",
        "INDICATORS_TO_USE = [\n",
        "    'open', 'high', 'low', 'close', 'volume',\n",
        "    'atr_14', 'adx_14', 'ema_9', 'ema_21', 'vwap', 'rsi_21', 'stochk_14_3_3'\n",
        "]  #@param {type:\"raw\"}\n",
        "\n",
        "# Normalization: 'StandardScaler' or 'MinMaxScaler'\n",
        "NORMALIZATION_TYPE = \"StandardScaler\"  #@param [\"StandardScaler\", \"MinMaxScaler\"]\n",
        "\n",
        "# Chronological split ratios (must sum to 1.0)\n",
        "TRAIN_VALID_TEST_SPLIT = [0.7, 0.15, 0.15]  #@param {type:\"raw\"}\n",
        "\n",
        "# Sequence/window length for model input\n",
        "SEQ_LEN = 512  #@param {type:\"integer\"}\n",
        "\n",
        "# Step size between windows (1 = full overlap)\n",
        "WINDOW_STRIDE = 1  #@param {type:\"integer\"}\n",
        "\n",
        "# TimesBlock\n",
        "TOP_K_PERIODS = 3  #@param {type:\"integer\"}  # k in paper\n",
        "EMBED_DIM = 192     # 2D backbone output channels\n",
        "EMBED_H = 8\n",
        "EMBED_W = 8\n",
        "DROPOUT_RATE = 0.1  #@param {type:\"number\"}\n",
        "\n",
        "# Training\n",
        "NUM_EPOCHS = 50  #@param {type:\"integer\"}\n",
        "BATCH_SIZE = 2048  #@param {type:\"integer\"}\n",
        "LEARNING_RATE = 1e-3  #@param {type:\"number\"}\n",
        "WEIGHT_DECAY = 1e-4  #@param {type:\"number\"}\n",
        "PATIENCE = 7  #@param {type:\"integer\"}\n",
        "GRAD_ACCUM_STEPS = 1  #@param {type:\"integer\"}\n",
        "\n",
        "# DataLoader performance\n",
        "DATALOADER_WORKERS = 4  #@param {type:\"integer\"}\n",
        "PIN_MEMORY = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# Precision & memory format (A100)\n",
        "USE_BF16 = True  #@param {type:\"boolean\"}\n",
        "CHANNELS_LAST = True  #@param {type:\"boolean\"}\n",
        "PRINT_GPU_MEM = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# Checkpointing\n",
        "CHECKPOINT_PATH = \"/content/drive/MyDrive/timesnet_mnq/checkpoints/best.pt\"  #@param {type:\"string\"}\n",
        "\n",
        "# Resume training if checkpoint exists\n",
        "RESUME_TRAINING = True  #@param {type:\"boolean\"}\n",
        "\n",
        "# Device\n",
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Config -> seq_len={SEQ_LEN}, batch_size={BATCH_SIZE}, k={TOP_K_PERIODS}, embed_dim={EMBED_DIM}, HxW={EMBED_H}x{EMBED_W}, dropout={DROPOUT_RATE}\")\n",
        "print(f\"Optim -> AdamW lr={LEARNING_RATE}, wd={WEIGHT_DECAY}, patience={PATIENCE}, grad_accum={GRAD_ACCUM_STEPS}\")\n",
        "print(f\"Loader -> workers={DATALOADER_WORKERS}, pin_memory={PIN_MEMORY}\")\n",
        "print(f\"Device -> {DEVICE} | bf16={USE_BF16} | channels_last={CHANNELS_LAST}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8044ea06",
      "metadata": {},
      "source": [
        "### Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0080a8f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import sys, subprocess\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# Ensure pandas-ta is available early\n",
        "try:\n",
        "    import pandas_ta as ta\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pandas-ta'])\n",
        "    import pandas_ta as ta\n",
        "\n",
        "import contextlib\n",
        "\n",
        "def _fmt_gb(bytes_val):\n",
        "    try:\n",
        "        return f\"{bytes_val/1e9:.2f} GB\"\n",
        "    except Exception:\n",
        "        return str(bytes_val)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    print(f\"GPU -> {torch.cuda.get_device_name(0)} | VRAM total: {_fmt_gb(props.total_memory)}\")\n",
        "\n",
        "try:\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "except Exception:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7205188e",
      "metadata": {},
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3cd4ba",
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
        "print(f\"Checkpoints dir -> {os.path.dirname(CHECKPOINT_PATH)}\")\n",
        "print(f\"Checkpoint file -> {CHECKPOINT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2a44f48",
      "metadata": {},
      "source": [
        "### Data Loading & Chronological Splits (no leakage)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "119ba241",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _find_datetime_column(df: pd.DataFrame) -> Optional[str]:\n",
        "    candidates = ['datetime', 'date', 'time', 'timestamp', 'ts']\n",
        "    cols = [c for c in df.columns]\n",
        "    for c in cols:\n",
        "        if c.lower() in candidates:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _find_col_ci(df: pd.DataFrame, name: str) -> Optional[str]:\n",
        "    for c in df.columns:\n",
        "        if c.lower() == name.lower():\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "def _has_col_ci(df: pd.DataFrame, name: str) -> bool:\n",
        "    return any(c.lower() == name.lower() for c in df.columns)\n",
        "\n",
        "def add_ta_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # Normalize base column names to lowercase for consistency\n",
        "    rename_map = {}\n",
        "    for nm in ['Open','High','Low','Close','Volume']:\n",
        "        c = _find_col_ci(df, nm)\n",
        "        if c is not None:\n",
        "            rename_map[c] = nm.lower()\n",
        "    if rename_map:\n",
        "        df = df.rename(columns=rename_map)\n",
        "\n",
        "    required = ['high','low','close','volume']\n",
        "    if any(not _has_col_ci(df, r) for r in required):\n",
        "        raise ValueError('Missing OHLCV columns (high, low, close, volume) to compute pandas-ta indicators.')\n",
        "\n",
        "    # Compute indicators only if a lowercase target is not already present\n",
        "    if not _has_col_ci(df, 'atr_14'):\n",
        "        df.ta.atr(high='high', low='low', close='close', length=14, append=True)\n",
        "    if not _has_col_ci(df, 'adx_14'):\n",
        "        df.ta.adx(high='high', low='low', close='close', length=14, append=True)\n",
        "    if not _has_col_ci(df, 'ema_9'):\n",
        "        df.ta.ema(close='close', length=9, append=True)\n",
        "    if not _has_col_ci(df, 'ema_21'):\n",
        "        df.ta.ema(close='close', length=21, append=True)\n",
        "    if not _has_col_ci(df, 'vwap'):\n",
        "        df.ta.vwap(high='high', low='low', close='close', volume='volume', append=True)\n",
        "    if not _has_col_ci(df, 'rsi_21'):\n",
        "        df.ta.rsi(close='close', length=21, append=True)\n",
        "    if not _has_col_ci(df, 'stochk_14_3_3'):\n",
        "        df.ta.stoch(high='high', low='low', close='close', k=14, d=3, smooth_k=3, append=True)\n",
        "\n",
        "    # Rename known pandas-ta outputs to lowercase canonical names if needed\n",
        "    lower_map = {}\n",
        "    # ATR\n",
        "    if 'ATR_14' in df.columns and 'atr_14' not in df.columns:\n",
        "        lower_map['ATR_14'] = 'atr_14'\n",
        "    # ADX\n",
        "    if 'ADX_14' in df.columns and 'adx_14' not in df.columns:\n",
        "        lower_map['ADX_14'] = 'adx_14'\n",
        "    # EMAs\n",
        "    if 'EMA_9' in df.columns and 'ema_9' not in df.columns:\n",
        "        lower_map['EMA_9'] = 'ema_9'\n",
        "    if 'EMA_21' in df.columns and 'ema_21' not in df.columns:\n",
        "        lower_map['EMA_21'] = 'ema_21'\n",
        "    # VWAP\n",
        "    if 'VWAP' in df.columns and 'vwap' not in df.columns:\n",
        "        lower_map['VWAP'] = 'vwap'\n",
        "    # RSI\n",
        "    if 'RSI_21' in df.columns and 'rsi_21' not in df.columns:\n",
        "        lower_map['RSI_21'] = 'rsi_21'\n",
        "    # STOCH K/D (we'll expose K by default)\n",
        "    if 'STOCHk_14_3_3' in df.columns and 'stochk_14_3_3' not in df.columns:\n",
        "        lower_map['STOCHk_14_3_3'] = 'stochk_14_3_3'\n",
        "    if lower_map:\n",
        "        df = df.rename(columns=lower_map)\n",
        "    return df\n",
        "\n",
        "def load_mnq_csv(csv_path: str, indicators: List[str]) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    time_col = _find_datetime_column(df)\n",
        "    if time_col is not None:\n",
        "        try:\n",
        "            df[time_col] = pd.to_datetime(df[time_col])\n",
        "            df = df.sort_values(by=time_col, ascending=True).reset_index(drop=True)\n",
        "        except Exception:\n",
        "            df = df.reset_index(drop=True)\n",
        "    else:\n",
        "        df = df.reset_index(drop=True)\n",
        "\n",
        "    # Create TA indicators and ensure lowercase canon names\n",
        "    df = add_ta_indicators(df)\n",
        "    # Select requested features (lowercase)\n",
        "    missing = [c for c in indicators if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns after TA creation: {missing}. Available: {list(df.columns)}\")\n",
        "    xdf = df[indicators].copy()\n",
        "    # Clean: numeric, remove inf, drop NaNs to avoid leakage via backward fill\n",
        "    for c in xdf.columns:\n",
        "        xdf[c] = pd.to_numeric(xdf[c], errors='coerce')\n",
        "    xdf = xdf.replace([np.inf, -np.inf], np.nan)\n",
        "    xdf = xdf.dropna()\n",
        "    return xdf\n",
        "\n",
        "def chronological_split(arr: np.ndarray, ratios: List[float]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    assert abs(sum(ratios) - 1.0) < 1e-6, \"TRAIN_VALID_TEST_SPLIT must sum to 1.0\"\n",
        "    T = len(arr)\n",
        "    n_train = int(T * ratios[0])\n",
        "    n_val = int(T * ratios[1])\n",
        "    n_test = T - n_train - n_val\n",
        "    train = arr[:n_train]\n",
        "    val = arr[n_train:n_train+n_val]\n",
        "    test = arr[n_train+n_val:]\n",
        "    return train, val, test\n",
        "\n",
        "def make_scaler(norm_type: str):\n",
        "    if norm_type == \"StandardScaler\":\n",
        "        return StandardScaler()\n",
        "    elif norm_type == \"MinMaxScaler\":\n",
        "        return MinMaxScaler()\n",
        "    else:\n",
        "        raise ValueError(\"NORMALIZATION_TYPE must be 'StandardScaler' or 'MinMaxScaler'.\")\n",
        "\n",
        "def fit_transform_splits(train_arr: np.ndarray, val_arr: np.ndarray, test_arr: np.ndarray, scaler):\n",
        "    # Fit only on training data\n",
        "    scaler.fit(train_arr)\n",
        "    train_scaled = scaler.transform(train_arr)\n",
        "    val_scaled = scaler.transform(val_arr)\n",
        "    test_scaled = scaler.transform(test_arr)\n",
        "    return train_scaled, val_scaled, test_scaled\n",
        "\n",
        "# Load CSV\n",
        "print(f'Loading CSV: {CSV_PATH}')\n",
        "df = load_mnq_csv(CSV_PATH, INDICATORS_TO_USE)\n",
        "print(f'Features selected: {len(INDICATORS_TO_USE)} | {INDICATORS_TO_USE}')\n",
        "data = df.values.astype(np.float32)\n",
        "print(f'Total rows after TA + cleanup: {len(df)} | Feature dim: {data.shape[1]}')\n",
        "\n",
        "# Chronological split (no shuffling)\n",
        "train_raw, val_raw, test_raw = chronological_split(data, TRAIN_VALID_TEST_SPLIT)\n",
        "print(f'Split -> train: {train_raw.shape}, val: {val_raw.shape}, test: {test_raw.shape}')\n",
        "\n",
        "# Train-only normalization\n",
        "scaler = make_scaler(NORMALIZATION_TYPE)\n",
        "train_scaled, val_scaled, test_scaled = fit_transform_splits(train_raw, val_raw, test_raw, scaler)\n",
        "if NORMALIZATION_TYPE == 'StandardScaler':\n",
        "    means = scaler.mean_\n",
        "    stds = scaler.scale_ if hasattr(scaler, 'scale_') else np.sqrt(scaler.var_)\n",
        "    print(f'Scaler(Standard) -> mean range [{means.min():.4f}, {means.max():.4f}] | std range [{stds.min():.4f}, {stds.max():.4f}]')\n",
        "else:\n",
        "    mins = scaler.data_min_\n",
        "    maxs = scaler.data_max_\n",
        "    print(f'Scaler(MinMax) -> min range [{mins.min():.4f}, {mins.max():.4f}] | max range [{maxs.min():.4f}, {maxs.max():.4f}]')\n",
        "\n",
        "def _count_windows(n, L, s):\n",
        "    return max(0, (n - L) // s + 1)\n",
        "nw_train = _count_windows(len(train_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
        "nw_val = _count_windows(len(val_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
        "nw_test = _count_windows(len(test_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
        "print(f'Windows -> train: {nw_train}, val: {nw_val}, test: {nw_test} | stride={WINDOW_STRIDE}, seq_len={SEQ_LEN}')\n",
        "\n",
        "train_scaled.shape, val_scaled.shape, test_scaled.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc1bfd75",
      "metadata": {},
      "source": [
        "### Dataset & DataLoaders (windowed, no shuffle)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432e2afc",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MNQ_Dataset(Dataset):\n",
        "    def __init__(self, arr_2d: np.ndarray, seq_len: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "        self.x = arr_2d\n",
        "        self.seq_len = int(seq_len)\n",
        "        self.stride = int(stride)\n",
        "        self.T = len(arr_2d)\n",
        "        self.C = arr_2d.shape[1]\n",
        "        if self.T < self.seq_len:\n",
        "            raise ValueError(f\"Not enough timesteps ({self.T}) for seq_len={self.seq_len}\")\n",
        "        # Number of windows using stride\n",
        "        self.idxs = list(range(0, self.T - self.seq_len + 1, self.stride))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idxs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = self.idxs[idx]\n",
        "        end = start + self.seq_len\n",
        "        window = self.x[start:end]  # shape [seq_len, C]\n",
        "        # model expects [L, C], training target == input window (self-supervised)\n",
        "        return torch.from_numpy(window).float()\n",
        "\n",
        "def make_loader(arr: np.ndarray, seq_len: int, stride: int, batch_size: int) -> DataLoader:\n",
        "    ds = MNQ_Dataset(arr, seq_len=seq_len, stride=stride)\n",
        "    # No shuffling to avoid any perceived leakage; data is chronologically windowed already\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=DATALOADER_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "train_loader = make_loader(train_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
        "val_loader = make_loader(val_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
        "test_loader = make_loader(test_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
        "\n",
        "tw, vw, tew = len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)\n",
        "tb, vb, teb = len(train_loader), len(val_loader), len(test_loader)\n",
        "print(f'DataLoaders -> windows (train/val/test): {tw}/{vw}/{tew} | batches: {tb}/{vb}/{teb} | batch_size={BATCH_SIZE}')\n",
        "print(f'Window shape -> L={SEQ_LEN}, C={train_scaled.shape[1]}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29d00a1",
      "metadata": {},
      "source": [
        "### Model: TimesBlock + Inception + Feature Extractor + Decoder\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class InceptionBlock2D(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        # Split out_channels across branches\n",
        "        b = out_channels // 4\n",
        "        r = out_channels - 3*b  # distribute remainder to first branch\n",
        "        b1 = b + r\n",
        "        b2 = b\n",
        "        b3 = b\n",
        "        b4 = b\n",
        "\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, b1, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(b1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        red3 = max(in_channels // 2, 8)\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, red3, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(red3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(red3, b2, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(b2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        red5 = max(in_channels // 2, 8)\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, red5, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(red5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(red5, b3, kernel_size=5, padding=2, bias=False),\n",
        "            nn.BatchNorm2d(b3),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, b4, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(b4),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        o1 = self.branch1x1(x)\n",
        "        o2 = self.branch3x3(x)\n",
        "        o3 = self.branch5x5(x)\n",
        "        o4 = self.branch_pool(x)\n",
        "        out = torch.cat([o1, o2, o3, o4], dim=1)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class TimesBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    FFT-based period discovery + 1D->2D folding + 2D Inception backbone + weighted fusion.\n",
        "    Input:  x [B, L, C]\n",
        "    Output: embedding [B, E, H, W]\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.k = k\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_h = embed_h\n",
        "        self.embed_w = embed_w\n",
        "        self.backbone = InceptionBlock2D(in_channels, embed_dim, dropout=dropout)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _find_topk_periods(self, x_bc_l: torch.Tensor, k: int) -> Tuple[List[int], torch.Tensor]:\n",
        "        # x_bc_l: [B, C, L]\n",
        "        B, C, L = x_bc_l.shape\n",
        "        xf = torch.fft.rfft(x_bc_l, dim=-1)  # [B, C, L//2 + 1]\n",
        "        amp = xf.abs().mean(dim=(0, 1))      # [L//2 + 1], averaged over batch & channels\n",
        "        if amp.shape[0] <= 1:\n",
        "            return [L], torch.tensor([1.0], device=x_bc_l.device)\n",
        "\n",
        "        amp[0] = 0.0  # ignore DC\n",
        "        k_eff = min(k, amp.shape[0]-1)\n",
        "        vals, idxs = torch.topk(amp, k=k_eff, largest=True, sorted=True)\n",
        "        periods = []\n",
        "        for idx in idxs.tolist():\n",
        "            p = int(round(L / max(idx, 1)))\n",
        "            p = max(p, 2)\n",
        "            periods.append(p)\n",
        "        # Softmax weights from amplitudes\n",
        "        w = torch.softmax(vals, dim=0)\n",
        "        return periods, w\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [B, L, C]\n",
        "        B, L, C = x.shape\n",
        "        x_bc_l = x.permute(0, 2, 1).contiguous()  # [B, C, L]\n",
        "\n",
        "        periods, weights = self._find_topk_periods(x_bc_l, self.k)\n",
        "        feats = None\n",
        "        for i, p in enumerate(periods):\n",
        "            pad_len = (p - (L % p)) % p\n",
        "            x_pad = F.pad(x_bc_l, (0, pad_len), mode='constant', value=0.0)  # [B,C,Lp]\n",
        "            Lp = x_pad.shape[-1]\n",
        "            w_ = Lp // p\n",
        "            # Fold: [B, C, p, w_]\n",
        "            x_2d = x_pad.view(B, C, w_, p).transpose(2, 3).contiguous()\n",
        "\n",
        "            z = self.backbone(x_2d)  # [B, E, h, w]\n",
        "            z = F.adaptive_avg_pool2d(z, (self.embed_h, self.embed_w))  # [B,E,H,W]\n",
        "            z = z * weights[i].view(1, 1, 1, 1)  # weight this period\n",
        "\n",
        "            feats = z if feats is None else (feats + z)\n",
        "\n",
        "        return feats  # [B, E, H, W]\n",
        "\n",
        "class TimesNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, in_channels: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float=0.1):\n",
        "        super().__init__()\n",
        "        self.block = TimesBlock(in_channels, k, embed_dim, embed_h, embed_w, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: [B, L, C]\n",
        "        emb = self.block(x)\n",
        "        return self.dropout(emb)  # [B, E, H, W]\n",
        "\n",
        "class ModelWithDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper for self-supervised training (reconstruction).\n",
        "    During inference, use extractor only.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, seq_len: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float=0.1):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.extractor = TimesNetFeatureExtractor(\n",
        "            in_channels=in_channels,\n",
        "            k=k,\n",
        "            embed_dim=embed_dim,\n",
        "            embed_h=embed_h,\n",
        "            embed_w=embed_w,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        flat_size = embed_dim * embed_h * embed_w\n",
        "        hidden = max(512, flat_size // 2)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Flatten(),                           # [B, E*H*W]\n",
        "            nn.Linear(flat_size, hidden),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hidden, seq_len * in_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x: [B, L, C]\n",
        "        emb = self.extractor(x)  # [B, E, H, W]\n",
        "        rec = self.decoder(emb).view(x.shape[0], self.seq_len, self.in_channels)\n",
        "        return rec, emb\n",
        "\n",
        "    def strip_decoder(self):\n",
        "        self.decoder = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "595ae284",
      "metadata": {},
      "source": [
        "### Training Utilities: EarlyStopping, Checkpointing, Train/Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d40cbc82",
      "metadata": {},
      "outputs": [],
      "source": [
        "from contextlib import nullcontext\n",
        "USE_AMP = bool(torch.cuda.is_available() and (DEVICE == 'cuda') and USE_BF16 and getattr(torch.cuda, 'is_bf16_supported', lambda: False)())\n",
        "AMP_DTYPE = torch.bfloat16 if USE_AMP else None\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int = 7, min_delta: float = 0.0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best = None\n",
        "        self.num_bad = 0\n",
        "\n",
        "    def step(self, value: float) -> bool:\n",
        "        if self.best is None or value < self.best - self.min_delta:\n",
        "            self.best = value\n",
        "            self.num_bad = 0\n",
        "            return False  # no stop\n",
        "        else:\n",
        "            self.num_bad += 1\n",
        "            return self.num_bad >= self.patience\n",
        "\n",
        "def save_checkpoint(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, epoch: int, best_val: float, extra: dict=None):\n",
        "    state = {\n",
        "        \"epoch\": epoch,\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"best_val_loss\": best_val,\n",
        "    }\n",
        "    if extra:\n",
        "        state[\"extra\"] = extra\n",
        "    torch.save(state, path)\n",
        "\n",
        "def load_checkpoint_if_any(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, resume: bool):\n",
        "    start_epoch = 1\n",
        "    best_val = float(\"inf\")\n",
        "    if resume and os.path.exists(path):\n",
        "        ckpt = torch.load(path, map_location=\"cpu\")\n",
        "        model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n",
        "        start_epoch = ckpt.get(\"epoch\", 1) + 1\n",
        "        best_val = ckpt.get(\"best_val_loss\", float(\"inf\"))\n",
        "        print(f\"Resuming from epoch {start_epoch-1} with best_val={best_val:.6f}\")\n",
        "    return start_epoch, best_val\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, device):\n",
        "    model.train()\n",
        "    mse = nn.MSELoss()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    for i, xb in enumerate(loader):\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if USE_AMP else nullcontext()\n",
        "        with ctx:\n",
        "            rec, emb = model(xb)\n",
        "            loss = mse(rec, xb) / max(int(GRAD_ACCUM_STEPS), 1)\n",
        "        loss.backward()\n",
        "        if (i + 1) % max(int(GRAD_ACCUM_STEPS), 1) == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "        # Accumulate reporting with true scale\n",
        "        total_loss += (loss.item() * max(int(GRAD_ACCUM_STEPS), 1)) * xb.size(0)\n",
        "        n += xb.size(0)\n",
        "    return total_loss / max(n, 1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    mse = nn.MSELoss()\n",
        "    total_loss = 0.0\n",
        "    n = 0\n",
        "    for xb in loader:\n",
        "        xb = xb.to(device, non_blocking=True)\n",
        "        ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if USE_AMP else nullcontext()\n",
        "        with ctx:\n",
        "            rec, emb = model(xb)\n",
        "            loss = mse(rec, xb)\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "        n += xb.size(0)\n",
        "    return total_loss / max(n, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51358a74",
      "metadata": {},
      "source": [
        "### Initialize Model, Optimizer, and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4375a4f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "in_channels = train_scaled.shape[1]\n",
        "print(f'In-channels (features): {in_channels}')\n",
        "model = ModelWithDecoder(\n",
        "    in_channels=in_channels,\n",
        "    seq_len=SEQ_LEN,\n",
        "    k=TOP_K_PERIODS,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    embed_h=EMBED_H,\n",
        "    embed_w=EMBED_W,\n",
        "    dropout=DROPOUT_RATE\n",
        ").to(DEVICE)\n",
        "if CHANNELS_LAST and (DEVICE == 'cuda'):\n",
        "    model = model.to(memory_format=torch.channels_last)\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Model params: {params/1e6:.2f}M')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "start_epoch, best_val = load_checkpoint_if_any(CHECKPOINT_PATH, model, optimizer, resume=RESUME_TRAINING)\n",
        "early = EarlyStopping(patience=PATIENCE, min_delta=0.0)\n",
        "\n",
        "tb, vb = len(train_loader), len(val_loader)\n",
        "tw, vw = len(train_loader.dataset), len(val_loader.dataset)\n",
        "est_opt_steps = (tb + max(int(GRAD_ACCUM_STEPS),1) - 1) // max(int(GRAD_ACCUM_STEPS),1)\n",
        "print(f\"Starting training at epoch {start_epoch} on {DEVICE} | train windows={tw}, batches/epoch={tb}, grad_accum={GRAD_ACCUM_STEPS}, est_opt_steps/epoch={est_opt_steps}\")\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    if DEVICE == 'cuda':\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "    ep_start = time.perf_counter()\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, DEVICE)\n",
        "    val_loss = evaluate(model, val_loader, DEVICE)\n",
        "    ep_time = time.perf_counter() - ep_start\n",
        "    throughput = len(train_loader.dataset) / max(ep_time, 1e-9)\n",
        "\n",
        "    improved = val_loss < best_val - 1e-12\n",
        "    if improved:\n",
        "        best_val = val_loss\n",
        "        save_checkpoint(\n",
        "            CHECKPOINT_PATH, model, optimizer, epoch, best_val,\n",
        "            extra={\n",
        "                \"INDICATORS_TO_USE\": INDICATORS_TO_USE,\n",
        "                \"NORMALIZATION_TYPE\": NORMALIZATION_TYPE,\n",
        "                \"SEQ_LEN\": SEQ_LEN,\n",
        "                \"TOP_K_PERIODS\": TOP_K_PERIODS,\n",
        "                \"EMBED_DIM\": EMBED_DIM,\n",
        "                \"EMBED_H\": EMBED_H,\n",
        "                \"EMBED_W\": EMBED_W,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    stop = early.step(val_loss)\n",
        "    lr = optimizer.param_groups[0].get('lr', None)\n",
        "    msg = f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f} | {throughput:.0f} samp/s\"\n",
        "    if lr is not None:\n",
        "        msg += f\" | lr={lr:.2e}\"\n",
        "    if improved:\n",
        "        msg += \" | [saved]\"\n",
        "    if PRINT_GPU_MEM and (DEVICE == 'cuda'):\n",
        "        peak = torch.cuda.max_memory_allocated() / 1e9\n",
        "        msg += f\" | GPU peak mem: {peak:.2f} GB\"\n",
        "    print(msg + f\" | patience {early.num_bad}/{PATIENCE}\")\n",
        "    if stop:\n",
        "        print(f\"Early stopping at epoch {epoch}. Best val: {best_val:.6f}\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94b9b7db",
      "metadata": {},
      "source": [
        "### Verification: Embedding-only forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167d28ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Load best checkpoint\n",
        "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
        "model = ModelWithDecoder(\n",
        "    in_channels=in_channels,\n",
        "    seq_len=SEQ_LEN,\n",
        "    k=TOP_K_PERIODS,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    embed_h=EMBED_H,\n",
        "    embed_w=EMBED_W,\n",
        "    dropout=DROPOUT_RATE\n",
        ")\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n",
        "model = model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# Remove temporary reconstruction decoder\n",
        "model.strip_decoder()\n",
        "assert model.decoder is None\n",
        "\n",
        "# Take a sample batch from test set and compute embeddings\n",
        "xb = next(iter(test_loader))\n",
        "xb = xb.to(DEVICE, non_blocking=True)\n",
        "with torch.no_grad():\n",
        "    ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n",
        "    with ctx:\n",
        "        # Use the extractor directly for inference-only embeddings\n",
        "        emb = model.extractor(xb)  # [B, E, H, W]\n",
        "\n",
        "print(\"Embedding tensor shape:\", tuple(emb.shape))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
