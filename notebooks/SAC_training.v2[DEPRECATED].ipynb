{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "XWqhE-eMcrSn",
      "metadata": {
        "id": "XWqhE-eMcrSn"
      },
      "source": [
        "# 1) Setup & Install\n",
        "Install dependencies for Gymnasium and Stable-Baselines3 (SAC).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91149d46",
      "metadata": {
        "id": "91149d46"
      },
      "outputs": [],
      "source": [
        "# 1) Setup & Install — deps for RL and plotting\n",
        "# First, uninstall conflicting packages that may be pre-installed in the Colab environment.\n",
        "!pip uninstall -y dopamine-rl gym > /dev/null 2>&1\n",
        "\n",
        "# Install stable, non-yanked versions of SB3 and Gymnasium known to be compatible.\n",
        "!pip -q install gymnasium==0.28.1 stable-baselines3==2.2.1 tensorboard matplotlib pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9QptH4HRcrSp",
      "metadata": {
        "id": "9QptH4HRcrSp"
      },
      "source": [
        "# 2) Google Drive Mount\n",
        "Access TimesNet embeddings stored in Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Etn6wwtWcrSq",
      "metadata": {
        "id": "Etn6wwtWcrSq"
      },
      "outputs": [],
      "source": [
        "# 2) Google Drive Mount — access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y8FFJ5m-crSq",
      "metadata": {
        "id": "y8FFJ5m-crSq"
      },
      "source": [
        "# 3) Configuration\n",
        "Paths, market specifics, costs, rewards, and SAC hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53e0313",
      "metadata": {
        "id": "a53e0313"
      },
      "outputs": [],
      "source": [
        "# 3) Configuration — paths and parameters\n",
        "import os\n",
        "\n",
        "# Data paths (TimesNet embeddings)\n",
        "DATA_DIR = '/content/drive/MyDrive/timesnet_mnq/embeddings/'  # Corrected to be a directory\n",
        "TRAIN_NPZ = os.path.join(DATA_DIR, 'train_emb_avg.npz')\n",
        "VAL_NPZ   = os.path.join(DATA_DIR, 'val_emb_avg.npz')\n",
        "TEST_NPZ  = os.path.join(DATA_DIR, 'test_emb_avg.npz')\n",
        "META_JSON = os.path.join(DATA_DIR, 'embeddings_meta.json')\n",
        "\n",
        "# OHLC paths for ATR only (no direct use in observations)\n",
        "RAW_DATA_DIR = '/content'  # where you upload mnq_complete_dataset.csv in Colab\n",
        "TRAIN_OHLC_CSV = os.path.join(RAW_DATA_DIR, 'mnq_train.csv')\n",
        "VAL_OHLC_CSV   = os.path.join(RAW_DATA_DIR, 'mnq_val.csv')\n",
        "TEST_OHLC_CSV  = os.path.join(RAW_DATA_DIR, 'mnq_test.csv')\n",
        "OHLC_RESAMPLE_RULE = '5min'  # aggregate 1-min to 5-min to match TimesNet (pandas>=2 warning safe)\n",
        "\n",
        "# Bar timing (5-minute bars) and annualization\n",
        "BAR_SECONDS = 300\n",
        "STEPS_PER_DAY = 288  # 24*60/5\n",
        "STEPS_PER_YEAR = int(STEPS_PER_DAY * 252)\n",
        "\n",
        "# Market specifics\n",
        "POINT_VALUE_MNQ = 2.0      # USD per point\n",
        "TICK_SIZE = 0.25\n",
        "TICK_VALUE_USD = POINT_VALUE_MNQ * TICK_SIZE  # $0.50 per tick\n",
        "\n",
        "# Costs\n",
        "FIXED_COMMISSION_USD = 1.80  # round-trip all-in (entry+exit)\n",
        "\n",
        "# Risk parameters (training env termination)\n",
        "MAX_EPISODE_DRAWDOWN_USD = 500.0\n",
        "\n",
        "# Reward policy coefficients (scaled to be impactful)\n",
        "DRAWDOWN_PENALTY_COEFF = 0.025\n",
        "DRAWDOWN_PENALTY_CLIP = 0.5\n",
        "PROFIT_KEEPING_BONUS_COEFF = 0.075\n",
        "PATIENCE_BONUS = 0.01\n",
        "CHOPPY_ATR_TO_PRICE_MAX = 0.005\n",
        "\n",
        "\n",
        "# Environment\n",
        "EPISODE_LENGTH = 1024\n",
        "N_ENVS = 16\n",
        "SEED = 42\n",
        "\n",
        "# SAC (SB3) hyperparameters\n",
        "SAC_KW = dict(\n",
        "    learning_rate=1e-4,\n",
        "    buffer_size=2_000_000,\n",
        "    learning_starts=10000,\n",
        "    batch_size=1024,\n",
        "    tau=0.005,\n",
        "    gamma=0.99,\n",
        "    train_freq=1,\n",
        "    gradient_steps=4,\n",
        "    ent_coef=0.05,\n",
        "    target_update_interval=1,\n",
        ")\n",
        "\n",
        "# Training run cadence\n",
        "TOTAL_TIMESTEPS = 2_000_000\n",
        "EVAL_FREQ_STEPS = 20000\n",
        "LOG_DIR = '/content/drive/MyDrive/timesnet_mnq/sac_logs'\n",
        "BEST_MODEL_PATH = os.path.join(LOG_DIR, 'best_model.zip')\n",
        "# Reward scaling factor to stabilize SAC entropy dynamics\n",
        "REWARD_SCALE = 5.0\n",
        "os.makedirs(LOG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54aa538d",
      "metadata": {
        "id": "54aa538d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Assuming DATA_DIR is correctly set in a previous cell\n",
        "# If not, you may need to define it here or run the configuration cell first.\n",
        "try:\n",
        "    with np.load(TRAIN_NPZ, allow_pickle=False) as Z:\n",
        "        print(f\"Keys in {TRAIN_NPZ}: {list(Z.keys())}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {TRAIN_NPZ} not found. Please ensure DATA_DIR is set correctly and the file exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2191fec",
      "metadata": {
        "id": "a2191fec"
      },
      "source": [
        "# 4) Imports & Utilities\n",
        "Helpers for data loading and metrics (Calmar, Sharpe, MDD).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f904ed4d",
      "metadata": {
        "id": "f904ed4d"
      },
      "outputs": [],
      "source": [
        "# 4) Imports & Utilities — loaders and metrics\n",
        "from typing import Optional\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _read_ohlc_csv(csv_path: str, resample_rule: Optional[str] = None):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    # Robust datetime detection: try common names, else scan all columns for parseable datetimes\n",
        "    time_col = None\n",
        "    for cand in ['timestamp', 'datetime', 'date', 'time']:\n",
        "        if cand in df.columns:\n",
        "            time_col = cand\n",
        "            break\n",
        "    if time_col is None and len(df.columns) > 0:\n",
        "        best_col = None\n",
        "        best_hits = -1\n",
        "        best_series = None\n",
        "        for c in df.columns:\n",
        "            s = df[c]\n",
        "            parsed = pd.to_datetime(s, errors='coerce', utc=True)\n",
        "            hits = int(parsed.notna().sum())\n",
        "            # Heuristic for epoch timestamps if parse failed heavily and dtype is numeric\n",
        "            if hits < int(0.8 * len(s)) and pd.api.types.is_numeric_dtype(s):\n",
        "                try:\n",
        "                    parsed_s = pd.to_datetime(s.astype('int64'), unit='ms', errors='coerce', utc=True)\n",
        "                    hits_ms = int(parsed_s.notna().sum())\n",
        "                    if hits_ms > hits:\n",
        "                        parsed, hits = parsed_s, hits_ms\n",
        "                except Exception:\n",
        "                    pass\n",
        "                try:\n",
        "                    parsed_s = pd.to_datetime(s.astype('int64'), unit='s', errors='coerce', utc=True)\n",
        "                    hits_s = int(parsed_s.notna().sum())\n",
        "                    if hits_s > hits:\n",
        "                        parsed, hits = parsed_s, hits_s\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if hits > best_hits:\n",
        "                best_col, best_hits, best_series = c, hits, parsed\n",
        "        if best_col is not None and best_hits >= int(0.8 * len(df)):\n",
        "            df[best_col] = best_series\n",
        "            time_col = best_col\n",
        "    if time_col is None:\n",
        "        raise TypeError(\n",
        "            'OHLC CSV must contain a datetime column (e.g., timestamp/datetime/date/time). Add/rename a time column to enable resampling.'\n",
        "        )\n",
        "    # Build DatetimeIndex and sort\n",
        "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce', utc=True)\n",
        "    df = df.sort_values(time_col).set_index(time_col)\n",
        "    if resample_rule:\n",
        "        # Match TimesNet resample semantics: right-closed, right-labeled bars\n",
        "        agg_base = {'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}\n",
        "        agg = {col: agg_base.get(col, 'last') for col in df.columns}\n",
        "        df = df.resample(resample_rule, label='right', closed='right').agg(agg)\n",
        "        df = df.dropna(how='any')\n",
        "    return df\n",
        "\n",
        "def _atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14) -> np.ndarray:\n",
        "    high = high.astype(np.float64)\n",
        "    low = low.astype(np.float64)\n",
        "    close = close.astype(np.float64)\n",
        "    n = len(close)\n",
        "    atr = np.empty(n, dtype=np.float64)\n",
        "    prev_close = close[0]\n",
        "    tr0 = max(high[0]-low[0], abs(high[0]-prev_close), abs(low[0]-prev_close))\n",
        "    atr[0] = tr0\n",
        "    for t in range(1, n):\n",
        "        tr = max(high[t]-low[t], abs(high[t]-close[t-1]), abs(low[t]-close[t-1]))\n",
        "        if t < period:\n",
        "            atr[t] = (atr[t-1]*t + tr) / (t+1)\n",
        "        else:\n",
        "            atr[t] = (atr[t-1]*(period-1) + tr) / period\n",
        "    return atr\n",
        "\n",
        "def load_split(npz_path: str, ohlc_csv_path: Optional[str] = None, resample_rule: Optional[str] = None, atr_period: int = 14):\n",
        "    with np.load(npz_path, allow_pickle=False) as Z:\n",
        "        obs = Z['obs'] if 'obs' in Z.files else Z['X']\n",
        "        start_idx = Z['start_idx']\n",
        "        end_idx = Z['end_idx']\n",
        "        close = Z['close'] if 'close' in Z.files else None\n",
        "        # Optional timestamps for alignment\n",
        "        ts = None\n",
        "        for _k in ['timestamp','timestamps','time','datetime','date']:\n",
        "            if _k in Z.files:\n",
        "                ts = Z[_k]\n",
        "                break\n",
        "        atr = None\n",
        "        for k in ['atr', 'ATR', 'atr_14', 'ATR_14']:\n",
        "            if k in Z.files:\n",
        "                atr = Z[k]\n",
        "                break\n",
        "    if close is None:\n",
        "        raise ValueError('NPZ must contain close series')\n",
        "    if ohlc_csv_path:\n",
        "        df = _read_ohlc_csv(ohlc_csv_path, resample_rule)\n",
        "        for col in ['high','low','close']:\n",
        "            if col not in df.columns:\n",
        "                raise ValueError(f'OHLC CSV must contain {col} column')\n",
        "        if ts is not None:\n",
        "            import pandas as pd\n",
        "            ts_idx = pd.to_datetime(ts, errors='coerce', utc=True)\n",
        "            aligned = df.reindex(ts_idx).ffill()\n",
        "            atr_ohlc = _atr_wilder(aligned['high'].to_numpy(), aligned['low'].to_numpy(), aligned['close'].to_numpy(), period=atr_period)\n",
        "            atr = atr_ohlc\n",
        "        else:\n",
        "            atr_ohlc = _atr_wilder(df['high'].to_numpy(), df['low'].to_numpy(), df['close'].to_numpy(), period=atr_period)\n",
        "            if len(atr_ohlc) != len(close):\n",
        "                import warnings\n",
        "                warnings.warn(f'ATR/OHLC length ({len(atr_ohlc)}) != close length ({len(close)}). Aligning by trimming/padding.')\n",
        "                if len(atr_ohlc) > len(close):\n",
        "                    atr_ohlc = atr_ohlc[-len(close):]\n",
        "                else:\n",
        "                    pad = np.full(len(close) - len(atr_ohlc), atr_ohlc[0] if len(atr_ohlc)>0 else 0.0, dtype=np.float64)\n",
        "                    atr_ohlc = np.concatenate([pad, atr_ohlc], axis=0)\n",
        "            atr = atr_ohlc\n",
        "    if atr is None:\n",
        "        raise ValueError(\"ATR not found. Provide ohlc_csv_path for ATR computation or include an 'atr' series in the NPZ.\")\n",
        "    return obs.astype(np.float32), start_idx.astype(np.int64), end_idx.astype(np.int64), close.astype(np.float64), atr.astype(np.float64)\n",
        "\n",
        "def synth_time_features(n: int, steps_per_day: int = STEPS_PER_DAY) -> np.ndarray:\n",
        "    t = np.arange(n, dtype=np.float32)\n",
        "    hour_phase = 2*np.pi * (t % steps_per_day) / max(steps_per_day, 1)\n",
        "    week_phase = 2*np.pi * (t % (steps_per_day*5)) / max(steps_per_day*5, 1)\n",
        "    return np.stack([np.sin(hour_phase), np.cos(hour_phase), np.sin(week_phase), np.cos(week_phase)], axis=1).astype(np.float32)\n",
        "\n",
        "def map_action_to_position(action: int) -> int:\n",
        "    # 0->short(-1), 1->flat(0), 2->long(+1)\n",
        "    return (-1 if action == 0 else (0 if action == 1 else 1))\n",
        "\n",
        "def pnl_from_price_change(prev_pos: int, price_change: float, point_value: float) -> float:\n",
        "    return float(prev_pos * price_change * point_value)\n",
        "\n",
        "def compute_drawdown(equity: np.ndarray) -> np.ndarray:\n",
        "    # Robust to negative equity: clamp ratio to [0,1] so DD ∈ [0,100%]\n",
        "    peaks = np.maximum.accumulate(np.maximum(equity, 1e-9))\n",
        "    ratio = np.divide(equity, peaks, out=np.zeros_like(peaks, dtype=np.float64), where=peaks>0)\n",
        "    dd = 1.0 - np.clip(ratio, 0.0, 1.0)\n",
        "    return dd\n",
        "\n",
        "def max_drawdown(equity: np.ndarray) -> float:\n",
        "    dd = compute_drawdown(equity)\n",
        "    return float(np.nanmax(dd) if len(dd) else 0.0)\n",
        "\n",
        "def cagr_from_equity(equity: np.ndarray) -> float:\n",
        "    n = len(equity)\n",
        "    if n < 2:\n",
        "        return 0.0\n",
        "    start = float(equity[0])\n",
        "    years = n / max(STEPS_PER_YEAR, 1)\n",
        "    if years <= 0 or start <= 0:\n",
        "        return 0.0\n",
        "    end = float(equity[-1])\n",
        "    if end <= 0:\n",
        "        return -1.0\n",
        "    ratio = end / start\n",
        "    return float(ratio ** (1.0 / years) - 1.0)\n",
        "\n",
        "def calmar_ratio(equity: np.ndarray) -> float:\n",
        "    mdd = max_drawdown(equity)\n",
        "    cagr = cagr_from_equity(equity)\n",
        "    if not np.isfinite(mdd) or not np.isfinite(cagr):\n",
        "        return 0.0\n",
        "    if mdd <= 1e-12:\n",
        "        return float('inf') if cagr > 0 else 0.0\n",
        "    return cagr / mdd\n",
        "\n",
        "def sharpe_ratio(returns: np.ndarray, risk_free=0.0) -> float:\n",
        "    if len(returns) < 2:\n",
        "        return 0.0\n",
        "    mean = float(np.mean(returns))\n",
        "    std = float(np.std(returns, ddof=1))\n",
        "    if std <= 1e-12:\n",
        "        return 0.0\n",
        "    ann_factor = math.sqrt(STEPS_PER_YEAR)\n",
        "    return float((mean - risk_free) / std * ann_factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "materialize_ohlc",
      "metadata": {
        "id": "materialize_ohlc"
      },
      "outputs": [],
      "source": [
        "# 3b) Materializar OHLC por split para ATR (alinha com os NPZ)\n",
        "# Use quando você tiver um único CSV base (mnq_complete_dataset.csv) e quiser gerar train/val/test CSVs.\n",
        "# Requer que os NPZ (TRAIN_NPZ/VAL_NPZ/TEST_NPZ) já existam no DATA_DIR.\n",
        "BASE_OHLC_CSV = os.path.join(RAW_DATA_DIR, 'mnq_complete_dataset.csv')\n",
        "if os.path.exists(BASE_OHLC_CSV):\n",
        "    df_all = _read_ohlc_csv(BASE_OHLC_CSV, resample_rule=OHLC_RESAMPLE_RULE)\n",
        "    def _len_close(npz_path):\n",
        "        with np.load(npz_path, allow_pickle=False) as Z:\n",
        "            return len(Z['close'] if 'close' in Z.files else Z['X'])\n",
        "    n_train = _len_close(TRAIN_NPZ)\n",
        "    n_val   = _len_close(VAL_NPZ)\n",
        "    n_test  = _len_close(TEST_NPZ)\n",
        "    total   = n_train + n_val + n_test\n",
        "    if len(df_all) < total:\n",
        "        raise ValueError(f'OHLC length {len(df_all)} < NPZ total {total}. Verifique CSV base e resample.')\n",
        "    # Compensa warm-up/limpezas no TimesNet: corta excesso do início para casar os comprimentos\n",
        "    excess = len(df_all) - total\n",
        "    if excess > 0:\n",
        "        df_all = df_all.iloc[excess:]\n",
        "    assert len(df_all) == total\n",
        "    os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
        "    _train_csv = os.path.join(RAW_DATA_DIR, 'mnq_train.csv')\n",
        "    _val_csv   = os.path.join(RAW_DATA_DIR, 'mnq_val.csv')\n",
        "    _test_csv  = os.path.join(RAW_DATA_DIR, 'mnq_test.csv')\n",
        "    # Preserve the time index as a column named 'timestamp' for downstream resampling/alignment\n",
        "    df_all = df_all.copy()\n",
        "    if df_all.index.name is None:\n",
        "        df_all.index.name = 'timestamp'\n",
        "    df_all.iloc[:n_train].to_csv(_train_csv, index=True)\n",
        "    df_all.iloc[n_train:n_train+n_val].to_csv(_val_csv, index=True)\n",
        "    df_all.iloc[n_train+n_val:].to_csv(_test_csv, index=True)\n",
        "    print('Wrote:', _train_csv, _val_csv, _test_csv)\n",
        "    # Atualiza caminhos usados pelo PPO\n",
        "    TRAIN_OHLC_CSV = _train_csv\n",
        "    VAL_OHLC_CSV   = _val_csv\n",
        "    TEST_OHLC_CSV  = _test_csv\n",
        "else:\n",
        "    print(f'[Aviso] BASE_OHLC_CSV não encontrado em: {BASE_OHLC_CSV}')\n",
        "    print('[Ação] Faça upload do mnq_complete_dataset.csv em RAW_DATA_DIR ou ajuste RAW_DATA_DIR/BASE_OHLC_CSV.')\n",
        "    print('[Ação] Se já possui train/val/test CSVs, defina TRAIN_OHLC_CSV/VAL_OHLC_CSV/TEST_OHLC_CSV e pule esta célula.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bsCOCr0ocrSs",
      "metadata": {
        "id": "bsCOCr0ocrSs"
      },
      "source": [
        "# 5) Environment (MNQEmbEnv)\n",
        "Training-only risk shaping on embeddings; no hard stops.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00036e3d",
      "metadata": {
        "id": "00036e3d"
      },
      "outputs": [],
      "source": [
        "# 5) Environment — MNQEmbEnv\n",
        "class MNQEmbEnv(gym.Env):\n",
        "    metadata = {'render.modes': []}\n",
        "    def __init__(\n",
        "        self,\n",
        "        npz_path: str,\n",
        "        meta_path: Optional[str] = None,\n",
        "        ohlc_csv_path: Optional[str] = None,\n",
        "        ohlc_resample_rule: Optional[str] = None,\n",
        "        episode_length: int = EPISODE_LENGTH,\n",
        "        point_value: float = POINT_VALUE_MNQ,\n",
        "        fixed_commission_usd: float = FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff: float = DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff: float = PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus: float = PATIENCE_BONUS,\n",
        "        choppy_atr_to_price_max: float = CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd: float = MAX_EPISODE_DRAWDOWN_USD,\n",
        "        dd_penalty_clip: float = DRAWDOWN_PENALTY_CLIP,\n",
        "        reward_scale: float = REWARD_SCALE,\n",
        "        seed: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.obs_raw, self.start_idx, self.end_idx, self.close, self.atr = load_split(npz_path, ohlc_csv_path=ohlc_csv_path, resample_rule=ohlc_resample_rule)\n",
        "        self.meta = {}\n",
        "        if meta_path and os.path.exists(meta_path):\n",
        "            with open(meta_path, 'r') as f:\n",
        "                self.meta = json.load(f)\n",
        "        self.T, self.D = self.obs_raw.shape\n",
        "        self.time_features = synth_time_features(self.T, STEPS_PER_DAY)\n",
        "        self.point_value = float(point_value)\n",
        "        self.fixed_commission = float(fixed_commission_usd)\n",
        "        self.dd_penalty = float(drawdown_penalty_coeff)\n",
        "        self.profit_bonus = float(profit_keeping_bonus_coeff)\n",
        "        self.patience_bonus_v = float(patience_bonus)\n",
        "        self.choppy_thr = float(choppy_atr_to_price_max)\n",
        "        self.max_episode_dd = float(max_episode_drawdown_usd)\n",
        "        self.episode_length = int(episode_length)\n",
        "        self.dd_penalty_clip = float(dd_penalty_clip)\n",
        "        self.reward_scale = float(reward_scale)\n",
        "\n",
        "        # Observation: [embedding D] + [time 4] + [position 1] + [portfolio 3]\n",
        "        self.obs_dim = self.D + 4 + 1 + 3\n",
        "        self.action_space = spaces.Box(low=np.array([-1.0], dtype=np.float32), high=np.array([1.0], dtype=np.float32), shape=(1,), dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_dim,), dtype=np.float32)\n",
        "        self._reset_state()\n",
        "\n",
        "    def _reset_state(self):\n",
        "        min_start = 1\n",
        "        max_start = max(min_start, self.T - self.episode_length - 2)\n",
        "        self.start = int(self.rng.integers(min_start, max_start+1)) if max_start > min_start else min_start\n",
        "        self.step_in_ep = 0\n",
        "        self.current_step = self.start\n",
        "        self.current_position = 0\n",
        "        self.entry_price = 0.0\n",
        "        self.best_price = 0.0\n",
        "        self.entry_step = 0\n",
        "        self.equity = 1000.0\n",
        "        self.peak_equity = self.equity\n",
        "        self.equity_series = [self.equity]\n",
        "        self.returns_series = []\n",
        "\n",
        "    def is_market_choppy(self, t: int) -> bool:\n",
        "        price = float(self.close[t])\n",
        "        if price <= 0:\n",
        "            return False\n",
        "        return (float(self.atr[t]) / price) <= self.choppy_thr\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        emb = self.obs_raw[self.current_step].astype(np.float32)\n",
        "        tf = self.time_features[self.current_step].astype(np.float32)\n",
        "        pf = np.array([self.current_position], dtype=np.float32)\n",
        "        # === Portfolio features ===\n",
        "        if self.current_position != 0:\n",
        "            atr_entry = float(max(self.atr[self.entry_step], 1e-9))\n",
        "            pnl_usd = (float(self.close[self.current_step]) - self.entry_price) * self.current_position * self.point_value\n",
        "            pnl_norm = float(pnl_usd) / max(atr_entry * self.point_value, 1e-9)\n",
        "            bars_in_trade = max(0, self.current_step - int(self.entry_step))\n",
        "        else:\n",
        "            pnl_norm = 0.0\n",
        "            bars_in_trade = 0\n",
        "        dur_norm = float(bars_in_trade) / max(self.episode_length, 1)\n",
        "        dd_usd = float(self.peak_equity - self.equity)\n",
        "        dd_norm = dd_usd / max(self.max_episode_dd, 1e-6)\n",
        "        portfolio_state = np.array([pnl_norm, dur_norm, dd_norm], dtype=np.float32)\n",
        "        return np.concatenate([emb, tf, pf, portfolio_state], axis=0)\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options=None):\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "        self._reset_state()\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # Map continuous action a∈[-1,1] to {-1,0,+1} via rounding\n",
        "        if isinstance(action, (list, tuple, np.ndarray)):\n",
        "            a = float(action[0])\n",
        "        else:\n",
        "            a = float(action)\n",
        "        a = float(np.clip(a, -1.0, 1.0))\n",
        "        new_pos = int(np.round(a))\n",
        "        prev_pos = self.current_position\n",
        "        t = self.current_step\n",
        "        prev_t = max(0, t-1)\n",
        "\n",
        "        # Price change and gross PnL for the previous position\n",
        "        price_change = float(self.close[t] - self.close[prev_t])\n",
        "        gross_pnl = pnl_from_price_change(prev_pos, price_change, self.point_value)\n",
        "\n",
        "        # Transaction costs: charge full round-trip cost only on new trade entry (from flat or reversal)\n",
        "        costs = 0.0\n",
        "        is_new_trade_entry = ((prev_pos == 0 and new_pos != 0) or\n",
        "                              (np.sign(new_pos) != np.sign(prev_pos) and new_pos != 0))\n",
        "        entry_intended_price = None\n",
        "        if is_new_trade_entry:\n",
        "            commission = self.fixed_commission\n",
        "            entry_intended_price = float(self.close[t])\n",
        "            costs = commission\n",
        "        base_reward = gross_pnl - costs\n",
        "\n",
        "        # Unrealized PnL of the active trade (based on prev_pos and prior entry)\n",
        "        if prev_pos != 0:\n",
        "            unrealized_pnl_prev = (float(self.close[t]) - self.entry_price) * prev_pos * self.point_value\n",
        "        else:\n",
        "            unrealized_pnl_prev = 0.0\n",
        "\n",
        "        # Risk penalties and behavioral incentives\n",
        "        # Normalized per-step drawdown penalty (only when a position is open)\n",
        "        if prev_pos != 0:\n",
        "            atr_entry = float(max(self.atr[self.entry_step], 1e-9))\n",
        "            dd_usd = max(0.0, -unrealized_pnl_prev)\n",
        "            dd_norm = dd_usd / max(atr_entry * self.point_value, 1e-9)\n",
        "        else:\n",
        "            dd_norm = 0.0\n",
        "        drawdown_penalty = self.dd_penalty * min(dd_norm, self.dd_penalty_clip)\n",
        "        current_trade_profit = max(0.0, unrealized_pnl_prev)\n",
        "        profit_keeping_bonus = self.profit_bonus * current_trade_profit\n",
        "        patience_bonus = (self.patience_bonus_v if self.is_market_choppy(t) and new_pos == 0 else\n",
        "                          (-self.patience_bonus_v if self.is_market_choppy(t) and new_pos != 0 else 0.0))\n",
        "        # Scale only the base PnL component, keep penalty terms outside to avoid collapse to flat policy\n",
        "        final_reward = (self.reward_scale * base_reward) - drawdown_penalty + profit_keeping_bonus + patience_bonus\n",
        "\n",
        "        # Update equity statistics\n",
        "        self.equity += gross_pnl - costs\n",
        "        self.returns_series.append((gross_pnl - costs) / max(abs(self.equity_series[-1]), 1e-9))\n",
        "        self.equity_series.append(self.equity)\n",
        "        self.peak_equity = max(self.peak_equity, self.equity)\n",
        "        episode_drawdown = self.peak_equity - self.equity\n",
        "        terminated = episode_drawdown > self.max_episode_dd\n",
        "\n",
        "        # Trade state management: handle new entries, exits, and reversals\n",
        "        position_flipped = (np.sign(new_pos) != np.sign(prev_pos))\n",
        "        if (prev_pos == 0 and new_pos != 0) or (position_flipped and new_pos != 0):\n",
        "            # New trade started (from flat or by reversal)\n",
        "            self.entry_price = float(self.close[t])\n",
        "            self.best_price = self.entry_price\n",
        "            self.entry_step = int(t)\n",
        "        elif new_pos == 0 and prev_pos != 0:\n",
        "            # Trade closed\n",
        "            self.entry_price = 0.0\n",
        "            self.best_price = 0.0\n",
        "            self.entry_step = int(self.current_step)\n",
        "        elif new_pos == prev_pos and new_pos != 0:\n",
        "            # Position maintained: update best price favorably\n",
        "            if new_pos > 0:\n",
        "                self.best_price = max(self.best_price, float(self.close[t]))\n",
        "            else:\n",
        "                self.best_price = min(self.best_price, float(self.close[t]))\n",
        "\n",
        "        # Advance step\n",
        "        self.current_position = new_pos\n",
        "        self.current_step += 1\n",
        "        self.step_in_ep += 1\n",
        "        truncated = (self.step_in_ep >= self.episode_length) or (self.current_step >= (self.T - 1))\n",
        "        obs = self._get_obs()\n",
        "        info = {\n",
        "            'gross_pnl': gross_pnl,\n",
        "            'costs': costs,\n",
        "            'equity': self.equity,\n",
        "            'drawdown': episode_drawdown,\n",
        "            'dd_norm_step': float(dd_norm),\n",
        "            'position': self.current_position,\n",
        "            'entry_intended_price': entry_intended_price,\n",
        "        }\n",
        "        return obs, float(final_reward), bool(terminated), bool(truncated), info\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc606ad",
      "metadata": {
        "id": "cfc606ad"
      },
      "source": [
        "# 6) Backtester\n",
        "Deterministic backtest with continuous action mapping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1199aa85",
      "metadata": {
        "id": "1199aa85"
      },
      "outputs": [],
      "source": [
        "# 6) Backtester — supports VecNormalize for obs.\n",
        "def run_backtest(model, env_like: MNQEmbEnv, deterministic=True, collect_reports=False, vecnormalize=None):\n",
        "    obs_raw = env_like.obs_raw\n",
        "    close = env_like.close\n",
        "    atr = env_like.atr\n",
        "    T = min(len(obs_raw), len(close), len(atr))\n",
        "    close = close[:T]\n",
        "    atr = atr[:T]\n",
        "    position = 0\n",
        "    entry_price = 0.0\n",
        "    entry_step = None\n",
        "    best_price = 0.0\n",
        "    equity = 1000.0\n",
        "    peak_equity = equity\n",
        "    equity_series = [equity]\n",
        "    step_returns = []\n",
        "    tf_all = synth_time_features(T, STEPS_PER_DAY)\n",
        "    trade_reports = [] if collect_reports else None\n",
        "    for t in range(1, T):\n",
        "        # === CONSTRUÇÃO DE ESTADO IDÊNTICA AO AMBIENTE DE TREINO ===\n",
        "        if position != 0:\n",
        "            atr_entry_idx = entry_step if entry_step is not None else t\n",
        "            atr_entry = float(max(atr[atr_entry_idx], 1e-9))\n",
        "            pnl_usd = (float(close[t]) - entry_price) * position * env_like.point_value\n",
        "            pnl_norm = pnl_usd / max(atr_entry * env_like.point_value, 1e-9)\n",
        "            bars_in_trade = max(0, t - atr_entry_idx)\n",
        "        else:\n",
        "            pnl_norm = 0.0\n",
        "            bars_in_trade = 0\n",
        "\n",
        "        dur_norm = float(bars_in_trade) / max(EPISODE_LENGTH, 1)\n",
        "        dd_usd = float(max(peak_equity - equity, 0.0))\n",
        "        dd_norm = dd_usd / max(MAX_EPISODE_DRAWDOWN_USD, 1e-6)\n",
        "\n",
        "        portfolio_state = np.array([pnl_norm, dur_norm, dd_norm], dtype=np.float32)\n",
        "        state = np.concatenate([\n",
        "            obs_raw[t].astype(np.float32),\n",
        "            tf_all[t].astype(np.float32),\n",
        "            np.array([position], dtype=np.float32),\n",
        "            portfolio_state\n",
        "        ])\n",
        "        # Apply VecNormalize stats if provided\n",
        "        if vecnormalize is not None:\n",
        "            mean = getattr(vecnormalize.obs_rms, 'mean', None)\n",
        "            var = getattr(vecnormalize.obs_rms, 'var', None)\n",
        "            eps = getattr(vecnormalize, 'epsilon', 1e-8)\n",
        "            clip = getattr(vecnormalize, 'clip_obs', 10.0)\n",
        "            if (mean is not None) and (var is not None):\n",
        "                state_n = np.clip((state - mean) / np.sqrt(var + eps), -clip, clip).astype(np.float32)\n",
        "            else:\n",
        "                state_n = state.astype(np.float32)\n",
        "        else:\n",
        "            state_n = state.astype(np.float32)\n",
        "        # ==========================================================\n",
        "\n",
        "        action, _ = model.predict(state_n, deterministic=deterministic)\n",
        "        a = float(action[0]) if isinstance(action, (list, tuple, np.ndarray)) else float(action)\n",
        "        a = float(np.clip(a, -1.0, 1.0))\n",
        "        desired_pos = int(np.round(a))\n",
        "\n",
        "        # 3) Execute decision (apply PnL for previous position)\n",
        "        prev_pos = position\n",
        "        price_change = float(close[t] - close[t-1])\n",
        "        gross_pnl = pnl_from_price_change(prev_pos, price_change, env_like.point_value)\n",
        "        costs = 0.0\n",
        "        is_new_trade_entry = ((prev_pos == 0 and desired_pos != 0) or (np.sign(desired_pos) != np.sign(prev_pos) and desired_pos != 0))\n",
        "        if is_new_trade_entry:\n",
        "            commission = env_like.fixed_commission\n",
        "            costs = commission\n",
        "            # Build trade report (entry)\n",
        "            if collect_reports:\n",
        "                intended_price = float(close[t])\n",
        "                trade_reports.append(dict(\n",
        "                    t=int(t),\n",
        "                    side=int(np.sign(desired_pos)),\n",
        "                    entry_intended_price=intended_price,\n",
        "                    commission_usd=float(commission),\n",
        "                    costs_charged_usd=float(costs),\n",
        "                ))\n",
        "        equity += gross_pnl - costs\n",
        "        peak_equity = max(peak_equity, equity)\n",
        "        equity_series.append(equity)\n",
        "        step_returns.append((gross_pnl - costs) / max(abs(equity_series[-2]), 1e-9))\n",
        "\n",
        "        # 4) Update trade state (entries, exits, reversals)\n",
        "        if (prev_pos == 0 and desired_pos != 0) or (np.sign(desired_pos) != np.sign(prev_pos) and desired_pos != 0):\n",
        "            entry_price = float(close[t])\n",
        "            entry_step = int(t)\n",
        "            best_price = entry_price\n",
        "        elif desired_pos == 0 and prev_pos != 0:\n",
        "            entry_price = 0.0\n",
        "            entry_step = None\n",
        "            best_price = 0.0\n",
        "        elif desired_pos == prev_pos and desired_pos != 0:\n",
        "            if desired_pos > 0:\n",
        "                best_price = max(best_price, float(close[t]))\n",
        "            else:\n",
        "                best_price = min(best_price, float(close[t]))\n",
        "        position = desired_pos\n",
        "\n",
        "    # Always return a 3-tuple for type stability; trade_reports may be None\n",
        "    return np.asarray(equity_series, dtype=np.float64), np.asarray(step_returns, dtype=np.float64), trade_reports\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Izp94ERhcrSt",
      "metadata": {
        "id": "Izp94ERhcrSt"
      },
      "source": [
        "# 7) Calmar Callback\n",
        "Early stopping by Calmar on validation backtests.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sYe3YHXOcrSu",
      "metadata": {
        "id": "sYe3YHXOcrSu"
      },
      "outputs": [],
      "source": [
        "# 7) Calmar Callback — validation selection\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.logger import Figure\n",
        "\n",
        "class CalmarCallback(BaseCallback):\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_env_ctor,\n",
        "        eval_env_kwargs,\n",
        "        eval_freq,\n",
        "        best_model_save_path,\n",
        "        verbose=1,\n",
        "        early_stop_patience=None,\n",
        "        early_stop_min_delta=0.0,\n",
        "        early_stop_target=None,\n",
        "        also_eval_test=False,\n",
        "        test_env_ctor=None,\n",
        "        test_env_kwargs=None,\n",
        "    ):\n",
        "        super().__init__(verbose)\n",
        "        self.eval_env_ctor = eval_env_ctor\n",
        "        self.eval_env_kwargs = eval_env_kwargs or {}\n",
        "        self.eval_freq = int(eval_freq)\n",
        "        self.best_model_save_path = best_model_save_path\n",
        "        self.best_calmar = -np.inf\n",
        "        # Early stopping config\n",
        "        self.early_patience = (int(early_stop_patience) if early_stop_patience is not None else None)\n",
        "        self.early_min_delta = float(early_stop_min_delta)\n",
        "        self.early_target = (float(early_stop_target) if early_stop_target is not None else None)\n",
        "        self._no_improve_evals = 0\n",
        "        # Optional second evaluation on TEST split\n",
        "        self.also_eval_test = bool(also_eval_test)\n",
        "        self.test_env_ctor = test_env_ctor\n",
        "        self.test_env_kwargs = test_env_kwargs or {}\n",
        "        # live logging defaults\n",
        "        self.log_tb = True\n",
        "        self.plot_figures = True\n",
        "        self.save_csv = False\n",
        "        try:\n",
        "            import os as _os\n",
        "            self.log_dir = _os.path.dirname(best_model_save_path)\n",
        "        except Exception:\n",
        "            self.log_dir = None\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.eval_freq != 0:\n",
        "            return True\n",
        "        eval_env = self.eval_env_ctor(**self.eval_env_kwargs)\n",
        "        # Collect trade_reports to log number of trades during validation\n",
        "        equity, step_returns, trade_reports = run_backtest(self.model, eval_env, deterministic=True, collect_reports=True, vecnormalize=self.model.get_vec_normalize_env())\n",
        "        calmar = calmar_ratio(equity)\n",
        "        sharpe = sharpe_ratio(step_returns)\n",
        "        mdd_v = max_drawdown(equity)\n",
        "        cagr_v = cagr_from_equity(equity)\n",
        "        trades_count = len(trade_reports) if trade_reports is not None else 0\n",
        "        if self.verbose:\n",
        "            print(f'[CalmarCallback] step={self.n_calls} calmar={calmar:.4f} best={self.best_calmar:.4f} | sharpe={sharpe:.3f} mdd={mdd_v:.2%} cagr={cagr_v:.2%} | trades={trades_count}')\n",
        "        # TensorBoard scalars + figures\n",
        "        if self.log_tb:\n",
        "            self.logger.record('eval/calmar', float(calmar))\n",
        "            self.logger.record('eval/sharpe', float(sharpe))\n",
        "            self.logger.record('eval/mdd', float(mdd_v))\n",
        "            self.logger.record('eval/cagr', float(cagr_v))\n",
        "            self.logger.record('eval/trades', int(trades_count))\n",
        "            if self.plot_figures:\n",
        "                fig1 = plt.figure(figsize=(8, 2.4))\n",
        "                plt.plot(equity)\n",
        "                plt.title('Equity (val)')\n",
        "                plt.grid(True)\n",
        "                self.logger.record('eval/equity_curve', Figure(fig1, close=True), exclude=('stdout',))\n",
        "                dd = compute_drawdown(equity)\n",
        "                fig2 = plt.figure(figsize=(8, 2.0))\n",
        "                plt.plot(dd)\n",
        "                plt.title('Drawdown (val)')\n",
        "                plt.grid(True)\n",
        "                self.logger.record('eval/drawdown_curve', Figure(fig2, close=True), exclude=('stdout',))\n",
        "        # Optional CSV snapshot\n",
        "        if self.save_csv and self.log_dir is not None:\n",
        "            try:\n",
        "                import os\n",
        "                import numpy as _np\n",
        "                os.makedirs(self.log_dir, exist_ok=True)\n",
        "                _np.savetxt(os.path.join(self.log_dir, f'equity_step_{self.n_calls}.csv'), equity, delimiter=',')\n",
        "            except Exception as e:\n",
        "                if self.verbose:\n",
        "                    print('[CalmarCallback] CSV save failed:', e)\n",
        "        # Optionally evaluate TEST split as well\n",
        "        if self.also_eval_test:\n",
        "            test_env = (self.test_env_ctor or self.eval_env_ctor)(**self.test_env_kwargs)\n",
        "            teq, tret, treports = run_backtest(self.model, test_env, deterministic=True, collect_reports=True, vecnormalize=self.model.get_vec_normalize_env())\n",
        "            tcalmar = calmar_ratio(teq)\n",
        "            tsharpe = sharpe_ratio(tret)\n",
        "            tmdd = max_drawdown(teq)\n",
        "            tcagr = cagr_from_equity(teq)\n",
        "            ttrades = len(treports) if treports is not None else 0\n",
        "            if self.verbose:\n",
        "                print(f'[CalmarCallback][TEST] step={self.n_calls} calmar={tcalmar:.4f} | sharpe={tsharpe:.3f} mdd={tmdd:.2%} cagr={tcagr:.2%} | trades={ttrades}')\n",
        "            if self.log_tb:\n",
        "                self.logger.record('test/calmar', float(tcalmar))\n",
        "                self.logger.record('test/sharpe', float(tsharpe))\n",
        "                self.logger.record('test/mdd', float(tmdd))\n",
        "                self.logger.record('test/cagr', float(tcagr))\n",
        "                self.logger.record('test/trades', int(ttrades))\n",
        "                if self.plot_figures:\n",
        "                    figt1 = plt.figure(figsize=(8, 2.4))\n",
        "                    plt.plot(teq)\n",
        "                    plt.title('Equity (test)')\n",
        "                    plt.grid(True)\n",
        "                    self.logger.record('test/equity_curve', Figure(figt1, close=True), exclude=('stdout',))\n",
        "                    tdd = compute_drawdown(teq)\n",
        "                    figt2 = plt.figure(figsize=(8, 2.0))\n",
        "                    plt.plot(tdd)\n",
        "                    plt.title('Drawdown (test)')\n",
        "                    plt.grid(True)\n",
        "                    self.logger.record('test/drawdown_curve', Figure(figt2, close=True), exclude=('stdout',))\n",
        "        improved = (calmar > (self.best_calmar + self.early_min_delta))\n",
        "        if improved:\n",
        "            self.best_calmar = calmar\n",
        "            self._no_improve_evals = 0\n",
        "            self.model.save(self.best_model_save_path)\n",
        "        else:\n",
        "            self._no_improve_evals += 1\n",
        "        # Early target stop\n",
        "        if (self.early_target is not None) and (calmar >= self.early_target):\n",
        "            if self.verbose:\n",
        "                print(f'[CalmarCallback] Early stop: target calmar {self.early_target:.4f} reached at step={self.n_calls}.')\n",
        "            return False\n",
        "        # Early patience stop\n",
        "        if (self.early_patience is not None) and (self._no_improve_evals >= self.early_patience):\n",
        "            if self.verbose:\n",
        "                print(f'[CalmarCallback] Early stop: no improvement in {self.early_patience} evals (best={self.best_calmar:.4f}).')\n",
        "            return False\n",
        "        return True\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lqrrzL21crSu",
      "metadata": {
        "id": "lqrrzL21crSu"
      },
      "source": [
        "# 8) Training — SB3 SAC\n",
        "Vectorized envs + VecNormalize, CalmarCallback, and learn() loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0edd20df",
      "metadata": {
        "id": "0edd20df"
      },
      "outputs": [],
      "source": [
        "# 8) Training — SB3 SAC\n",
        "from stable_baselines3 import SAC\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor, VecNormalize\n",
        "\n",
        "# Seed global RNGs for reproducibility\n",
        "import random\n",
        "import numpy as _np\n",
        "_np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "try:\n",
        "    import torch\n",
        "    torch.manual_seed(SEED)\n",
        "    try:\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "    except Exception:\n",
        "        pass\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "def make_train_env():\n",
        "    return MNQEmbEnv(\n",
        "        npz_path=TRAIN_NPZ, meta_path=META_JSON, ohlc_csv_path=TRAIN_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED\n",
        "    )\n",
        "\n",
        "def make_val_env():\n",
        "    return MNQEmbEnv(\n",
        "        npz_path=VAL_NPZ, meta_path=META_JSON, ohlc_csv_path=VAL_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED+1\n",
        "    )\n",
        "\n",
        "# Build vectorized env with observation normalization (reward unnormalized)\n",
        "base_env = DummyVecEnv([make_train_env for _ in range(N_ENVS)])\n",
        "vec_env = VecNormalize(base_env, norm_obs=True, norm_reward=False)\n",
        "vec_env = VecMonitor(vec_env, filename=None)\n",
        "\n",
        "# Policy with ReLU, moderate net_arch, AdamW (weight decay), and Dropout features extractor\n",
        "import torch\n",
        "from torch import nn\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "\n",
        "class DropoutExtractor(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space, features_dim=None, p=0.1):\n",
        "        super().__init__(observation_space, features_dim=\n",
        "               int(gym.spaces.utils.flatdim(observation_space)))\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(p=float(p))\n",
        "        self._features_dim = int(gym.spaces.utils.flatdim(observation_space))\n",
        "    def forward(self, observations):\n",
        "        return self.dropout(self.flatten(observations))\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    net_arch=[256, 256],\n",
        "    activation_fn=nn.ReLU,\n",
        "    optimizer_class=torch.optim.AdamW,\n",
        "    optimizer_kwargs=dict(weight_decay=1e-4),\n",
        "    features_extractor_class=DropoutExtractor,\n",
        "    features_extractor_kwargs=dict(p=0.2),\n",
        ")\n",
        "\n",
        "model = SAC('MlpPolicy', vec_env, policy_kwargs=policy_kwargs, tensorboard_log=LOG_DIR, seed=SEED, **SAC_KW)\n",
        "\n",
        "calmar_cb = CalmarCallback(\n",
        "    eval_env_ctor=MNQEmbEnv,\n",
        "    eval_env_kwargs=dict(\n",
        "        npz_path=VAL_NPZ, meta_path=META_JSON, ohlc_csv_path=VAL_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED+123\n",
        "    ),\n",
        "    eval_freq=EVAL_FREQ_STEPS,\n",
        "    best_model_save_path=BEST_MODEL_PATH,\n",
        "    verbose=1,\n",
        "    early_stop_patience=10,         # patience for Calmar early stopping\n",
        "    early_stop_min_delta=0.0,       # require strictly better Calmar\n",
        "    early_stop_target=None,         # or set a target Calmar to stop early\n",
        "    also_eval_test=True,\n",
        "    test_env_ctor=MNQEmbEnv,\n",
        "    test_env_kwargs=dict(\n",
        "        npz_path=TEST_NPZ, meta_path=META_JSON, ohlc_csv_path=TEST_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED+456\n",
        "    )\n",
        ")\n",
        "\n",
        "# Train with Calmar validation only (entropy schedule disabled for stability)\n",
        "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=calmar_cb, progress_bar=True)\n",
        "# Save VecNormalize stats for deterministic evaluation/backtesting\n",
        "try:\n",
        "    import os as _os\n",
        "    vec_path = _os.path.join(LOG_DIR, 'vecnormalize.pkl')\n",
        "    vec_env.save(vec_path)\n",
        "    print(f'[VecNormalize] saved stats to {vec_path}')\n",
        "except Exception as e:\n",
        "    print('[VecNormalize] save failed:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-BF0Js9wcrSu",
      "metadata": {
        "id": "-BF0Js9wcrSu"
      },
      "source": [
        "# 9) Final Backtest & Plots\n",
        "Evaluate best model (SAC) on the test split and visualize results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f60fdf",
      "metadata": {
        "id": "39f60fdf"
      },
      "outputs": [],
      "source": [
        "# 9) Final Backtest & Plots — test split evaluation\n",
        "from stable_baselines3 import SAC\n",
        "best_model = SAC.load(BEST_MODEL_PATH, device='auto')\n",
        "# Load VecNormalize stats saved after training to normalize observations in backtest\n",
        "from stable_baselines3.common.vec_env import VecNormalize as _VecNorm, DummyVecEnv as _DummyVecEnv\n",
        "vec_stats = None\n",
        "try:\n",
        "    # Attach stats to a dummy env with the same observation space\n",
        "    _dummy = _DummyVecEnv([make_val_env])\n",
        "    vec_stats = _VecNorm.load(os.path.join(LOG_DIR, 'vecnormalize.pkl'), _dummy)\n",
        "    vec_stats.training = False\n",
        "    vec_stats.norm_reward = False\n",
        "    print('[VecNormalize] loaded stats for backtest')\n",
        "except Exception as e:\n",
        "    print('[VecNormalize] load failed:', e)\n",
        "test_env = MNQEmbEnv(\n",
        "    npz_path=TEST_NPZ, meta_path=META_JSON, ohlc_csv_path=TEST_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=10**9,\n",
        "    point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "    drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "    profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "    patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "    max_episode_drawdown_usd=1e12, dd_penalty_clip=DRAWDOWN_PENALTY_CLIP, reward_scale=REWARD_SCALE, seed=SEED+999\n",
        ")\n",
        "equity, step_returns, trade_reports = run_backtest(best_model, test_env, deterministic=True, collect_reports=True, vecnormalize=vec_stats)\n",
        "# Log trade count and breakdown\n",
        "num_trades = len(trade_reports) if trade_reports is not None else 0\n",
        "num_longs = sum(1 for r in (trade_reports or []) if r.get('side', 0) > 0)\n",
        "num_shorts = sum(1 for r in (trade_reports or []) if r.get('side', 0) < 0)\n",
        "print(f'Trades: {num_trades} (long: {num_longs}, short: {num_shorts})')\n",
        "\n",
        "calmar = calmar_ratio(equity)\n",
        "sharpe = sharpe_ratio(step_returns)\n",
        "mdd = max_drawdown(equity)\n",
        "cagr = cagr_from_equity(equity)\n",
        "print(f'Test Calmar: {calmar:.3f} | Sharpe: {sharpe:.3f} | MDD: {mdd:.2%} | CAGR: {cagr:.2%}')\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(equity)\n",
        "plt.title('Equity Curve (Test)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "dd = compute_drawdown(equity)\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.plot(dd)\n",
        "plt.title('Drawdown (Test)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tb_viewer_hdr",
      "metadata": {
        "id": "tb_viewer_hdr"
      },
      "source": [
        "# 10) TensorBoard Viewer\n",
        "Visualize training and evaluation logs recorded by SB3.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tb_viewer_cell",
      "metadata": {
        "id": "tb_viewer_cell"
      },
      "outputs": [],
      "source": [
        "# 10) TensorBoard Viewer — launch inside notebook\n",
        "%load_ext tensorboard\n",
        "print(f\"Using LOG_DIR={LOG_DIR}\")\n",
        "%tensorboard --logdir \"$LOG_DIR\" --port 6006\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}