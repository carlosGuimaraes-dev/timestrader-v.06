{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc944697",
   "metadata": {
    "id": "bc944697"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50c71d",
   "metadata": {
    "id": "1a50c71d"
   },
   "outputs": [],
   "source": [
    "CSV_PATH = \"mnq_complete_dataset.csv\"  #@param {type:\"string\"}\n",
    "# Features to use (OHLCV + 7 TA indicators; all lowercase)\n",
    "INDICATORS_TO_USE = ['open', 'high', 'low', 'close', 'volume','atr_14', 'adx_14', 'ema_9', 'ema_21', 'vwap', 'rsi_21', 'stochk_14_3_3']  #@param {type:\"raw\"}\n",
    "# Normalization: 'StandardScaler' or 'MinMaxScaler'\n",
    "NORMALIZATION_TYPE = \"StandardScaler\"  #@param [\"StandardScaler\", \"MinMaxScaler\"]\n",
    "# Resample timeframe (e.g., '5T' for 5 minutes, None to disable)\n",
    "RESAMPLE_RULE = '5T'  #@param {type:\"raw\"}\n",
    "# Chronological split ratios (must sum to 1.0)\n",
    "TRAIN_VALID_TEST_SPLIT = [0.7, 0.15, 0.15]  #@param {type:\"raw\"}\n",
    "# Sequence/window length for model input\n",
    "SEQ_LEN = 512  #@param {type:\"integer\"}\n",
    "# Step size between windows (1 = full overlap)\n",
    "WINDOW_STRIDE = 1  #@param {type:\"integer\"}\n",
    "# TimesBlock\n",
    "TOP_K_PERIODS = 3  #@param {type:\"integer\"}  # k in paper\n",
    "EMBED_DIM = 256     # 2D backbone output channels\n",
    "EMBED_H = 8\n",
    "EMBED_W = 8\n",
    "DROPOUT_RATE = 0.3  #@param {type:\"number\"}\n",
    "# Training\n",
    "NUM_EPOCHS = 100  #@param {type:\"integer\"}\n",
    "BATCH_SIZE = 1024  #@param {type:\"integer\"}\n",
    "LEARNING_RATE = 1e-3  # scheduler max_lr\n",
    "WEIGHT_DECAY = 0.005  #@param {type:\"number\"}\n",
    "PATIENCE = 30  #@param {type:\"integer\"}\n",
    "GRAD_ACCUM_STEPS = 1  #@param {type:\"integer\"}\n",
    "# LR Scheduler (OneCycle)\n",
    "USE_SCHEDULER = True  #@param {type:\"boolean\"}\n",
    "ONECYCLE_PCT_START = 0.15  #@param {type:\"number\"}\n",
    "ONECYCLE_DIV_FACTOR = 25.0  #@param {type:\"number\"}\n",
    "ONECYCLE_FINAL_DIV = 100.0  #@param {type:\"number\"}\n",
    "# Gradient clipping\n",
    "CLIP_GRAD_NORM = 1.0  #@param {type:\"number\"}\n",
    "# Train-only augmentation\n",
    "AUG_NOISE_STD = 0.02  #@param {type:\"number\"}\n",
    "# DataLoader performance\n",
    "DATALOADER_WORKERS = 4  #@param {type:\"integer\"}\n",
    "PIN_MEMORY = True  #@param {type:\"boolean\"}\n",
    "PERSISTENT_WORKERS = True  #@param {type:\"boolean\"}\n",
    "PREFETCH_FACTOR = 2  #@param {type:\"integer\"}\n",
    "# Precision & memory format (A100)\n",
    "USE_BF16 = True  #@param {type:\"boolean\"}\n",
    "CHANNELS_LAST = True  #@param {type:\"boolean\"}\n",
    "PRINT_GPU_MEM = True  #@param {type:\"boolean\"}\n",
    "# Checkpointing\n",
    "CHECKPOINT_PATH = \"/content/drive/MyDrive/timesnet_mnq/checkpoints/best.pt\"  #@param {type:\"string\"}\n",
    "# Logs e histÃ³rico\n",
    "LOG_DIR = \"/content/drive/MyDrive/timesnet_mnq/logs\"  #@param {type:\"string\"}\n",
    "HISTORY_PATH = LOG_DIR + \"/training_history.jsonl\"\n",
    "# Resume training if checkpoint exists\n",
    "RESUME_TRAINING = False  #@param {type:\"boolean\"}\n",
    "# Device\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044ea06",
   "metadata": {
    "id": "8044ea06"
   },
   "source": [
    "### Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080a8f2",
   "metadata": {
    "id": "0080a8f2"
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: I001\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Config -> seq_len={SEQ_LEN}, batch_size={BATCH_SIZE}, k={TOP_K_PERIODS}, embed_dim={EMBED_DIM}, HxW={EMBED_H}x{EMBED_W}, dropout={DROPOUT_RATE}\")\n",
    "print(f\"Optim -> AdamW max_lr={LEARNING_RATE}, wd={WEIGHT_DECAY}, patience={PATIENCE}, grad_accum={GRAD_ACCUM_STEPS}\")\n",
    "print(f\"Loader -> workers={DATALOADER_WORKERS}, pin_memory={PIN_MEMORY}, persistent={PERSISTENT_WORKERS}, prefetch={PREFETCH_FACTOR}\")\n",
    "print(f\"Device -> {DEVICE} | bf16={USE_BF16} | channels_last={CHANNELS_LAST}\")\n",
    "\n",
    "# Ensure pandas-ta is available early\n",
    "try:\n",
    "    import pandas_ta as ta  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pandas-ta'])\n",
    "    import pandas_ta as ta  # noqa: F401\n",
    "\n",
    "def _fmt_gb(bytes_val):\n",
    "    try:\n",
    "        return f\"{bytes_val/1e9:.2f} GB\"\n",
    "    except Exception:\n",
    "        return str(bytes_val)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"GPU -> {torch.cuda.get_device_name(0)} | VRAM total: {_fmt_gb(props.total_memory)}\")\n",
    "\n",
    "try:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "except Exception:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7205188e",
   "metadata": {
    "id": "7205188e"
   },
   "source": [
    "### Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3cd4ba",
   "metadata": {
    "id": "4a3cd4ba"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
    "print(f\"Checkpoints dir -> {os.path.dirname(CHECKPOINT_PATH)}\")\n",
    "print(f\"Checkpoint file -> {CHECKPOINT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a44f48",
   "metadata": {
    "id": "e2a44f48"
   },
   "source": [
    "### Data Loading & Chronological Splits (no leakage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119ba241",
   "metadata": {
    "id": "119ba241"
   },
   "outputs": [],
   "source": [
    "def _find_datetime_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    candidates = ['datetime', 'date', 'time', 'timestamp', 'ts']\n",
    "    cols = [c for c in df.columns]\n",
    "    for c in cols:\n",
    "        if c.lower() in candidates:\n",
    "            return c\n",
    "    return None\n",
    "def _find_col_ci(df: pd.DataFrame, name: str) -> Optional[str]:\n",
    "    for c in df.columns:\n",
    "        if c.lower() == name.lower():\n",
    "            return c\n",
    "    return None\n",
    "def _has_col_ci(df: pd.DataFrame, name: str) -> bool:\n",
    "    return any(c.lower() == name.lower() for c in df.columns)\n",
    "def add_ta_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Normalize base column names to lowercase for consistency\n",
    "    rename_map = {}\n",
    "    for nm in ['Open','High','Low','Close','Volume']:\n",
    "        c = _find_col_ci(df, nm)\n",
    "        if c is not None:\n",
    "            rename_map[c] = nm.lower()\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "    required = ['high','low','close','volume']\n",
    "    if any(not _has_col_ci(df, r) for r in required):\n",
    "        raise ValueError('Missing OHLCV columns (high, low, close, volume) to compute pandas-ta indicators.')\n",
    "    # Ensure datetime index for VWAP\n",
    "    time_col = _find_datetime_column(df)\n",
    "    if time_col is not None:\n",
    "        try:\n",
    "            df[time_col] = pd.to_datetime(df[time_col])\n",
    "            df = df.set_index(time_col)\n",
    "        except Exception:\n",
    "            pass # Keep going if datetime conversion fails, but VWAP might fail\n",
    "    # Compute indicators only if a lowercase target is not already present\n",
    "    if not _has_col_ci(df, 'atr_14'):\n",
    "        df.ta.atr(high='high', low='low', close='close', length=14, append=True)\n",
    "    if not _has_col_ci(df, 'adx_14'):\n",
    "        df.ta.adx(high='high', low='low', close='close', length=14, append=True)\n",
    "    if not _has_col_ci(df, 'ema_9'):\n",
    "        df.ta.ema(close='close', length=9, append=True)\n",
    "    if not _has_col_ci(df, 'ema_21'):\n",
    "        df.ta.ema(close='close', length=21, append=True)\n",
    "    if not _has_col_ci(df, 'vwap'):\n",
    "        df.ta.vwap(high='high', low='low', close='close', volume='volume', append=True)\n",
    "    if not _has_col_ci(df, 'rsi_21'):\n",
    "        df.ta.rsi(close='close', length=21, append=True)\n",
    "    if not _has_col_ci(df, 'stochk_14_3_3'):\n",
    "        df.ta.stoch(high='high', low='low', close='close', k=14, d=3, smooth_k=3, append=True)\n",
    "    # Rename known pandas-ta outputs to lowercase canonical names if needed\n",
    "    lower_map = {}\n",
    "    # ATR - pandas-ta outputs ATRr_14\n",
    "    if 'ATRr_14' in df.columns and 'atr_14' not in df.columns:\n",
    "        lower_map['ATRr_14'] = 'atr_14'\n",
    "    # ADX\n",
    "    if 'ADX_14' in df.columns and 'adx_14' not in df.columns:\n",
    "        lower_map['ADX_14'] = 'adx_14'\n",
    "    # EMAs\n",
    "    if 'EMA_9' in df.columns and 'ema_9' not in df.columns:\n",
    "        lower_map['EMA_9'] = 'ema_9'\n",
    "    if 'EMA_21' in df.columns and 'ema_21' not in df.columns:\n",
    "        lower_map['EMA_21'] = 'ema_21'\n",
    "    # VWAP\n",
    "    if 'VWAP' in df.columns and 'vwap' not in df.columns:\n",
    "        lower_map['VWAP'] = 'vwap'\n",
    "    if 'VWAP_D' in df.columns and 'vwap' not in df.columns: # Added to handle VWAP_D\n",
    "        lower_map['VWAP_D'] = 'vwap'\n",
    "    # RSI\n",
    "    if 'RSI_21' in df.columns and 'rsi_21' not in df.columns:\n",
    "        lower_map['RSI_21'] = 'rsi_21'\n",
    "    # STOCH K/D (we'll expose K by default)\n",
    "    if 'STOCHk_14_3_3' in df.columns and 'stochk_14_3_3' not in df.columns:\n",
    "        lower_map['STOCHk_14_3_3'] = 'stochk_14_3_3'\n",
    "    if lower_map:\n",
    "        df = df.rename(columns=lower_map)\n",
    "    return df\n",
    "def load_mnq_csv(csv_path: str, indicators: list[str], resample_rule: Optional[str] = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    time_col = _find_datetime_column(df)\n",
    "    if time_col is not None:\n",
    "        try:\n",
    "            df[time_col] = pd.to_datetime(df[time_col])\n",
    "            df = df.sort_values(by=time_col, ascending=True).reset_index(drop=True)\n",
    "        except Exception:\n",
    "            df = df.reset_index(drop=True)\n",
    "    else:\n",
    "        df = df.reset_index(drop=True)\n",
    "    # Normalize base OHLCV column names before optional resample\n",
    "    rename_map = {}\n",
    "    for nm in ['Open', 'High', 'Low', 'Close', 'Volume']:\n",
    "        c = _find_col_ci(df, nm)\n",
    "        if c is not None:\n",
    "            rename_map[c] = nm.lower()\n",
    "    if rename_map:\n",
    "        df = df.rename(columns=rename_map)\n",
    "    if resample_rule and time_col is not None:\n",
    "        df = df.set_index(time_col)\n",
    "        agg_base = {\n",
    "            'open': 'first',\n",
    "            'high': 'max',\n",
    "            'low': 'min',\n",
    "            'close': 'last',\n",
    "            'volume': 'sum'\n",
    "        }\n",
    "        agg = {col: agg_base.get(col, 'last') for col in df.columns}\n",
    "        df = df.resample(resample_rule, label='right', closed='right').agg(agg)\n",
    "        df = df.dropna(how='any').reset_index()\n",
    "        # Update time_col in case resample changed its dtype/name\n",
    "        time_col = _find_datetime_column(df)\n",
    "    # Create TA indicators and ensure lowercase canon names\n",
    "    df = add_ta_indicators(df)\n",
    "    # Select requested features (lowercase)\n",
    "    missing = [c for c in indicators if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns after TA creation: {missing}. Available: {list(df.columns)}\")\n",
    "    xdf = df[indicators].copy()\n",
    "    # Clean: numeric, remove inf, drop NaNs to avoid leakage via backward fill\n",
    "    for c in xdf.columns:\n",
    "        xdf[c] = pd.to_numeric(xdf[c], errors='coerce')\n",
    "    xdf = xdf.replace([np.inf, -np.inf], np.nan)\n",
    "    xdf = xdf.dropna()\n",
    "    return xdf\n",
    "def chronological_split(arr: np.ndarray, ratios: list[float]) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    assert abs(sum(ratios) - 1.0) < 1e-6, \"TRAIN_VALID_TEST_SPLIT must sum to 1.0\"\n",
    "    T = len(arr)\n",
    "    n_train = int(T * ratios[0])\n",
    "    n_val = int(T * ratios[1])\n",
    "    train = arr[:n_train]\n",
    "    val = arr[n_train:n_train+n_val]\n",
    "    test = arr[n_train+n_val:]\n",
    "    return train, val, test\n",
    "def make_scaler(norm_type: str):\n",
    "    if norm_type == \"StandardScaler\":\n",
    "        return StandardScaler()\n",
    "    elif norm_type == \"MinMaxScaler\":\n",
    "        return MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"NORMALIZATION_TYPE must be 'StandardScaler' or 'MinMaxScaler'.\")\n",
    "def fit_transform_splits(train_arr: np.ndarray, val_arr: np.ndarray, test_arr: np.ndarray, scaler):\n",
    "    # Fit only on training data\n",
    "    scaler.fit(train_arr)\n",
    "    train_scaled = scaler.transform(train_arr)\n",
    "    val_scaled = scaler.transform(val_arr)\n",
    "    test_scaled = scaler.transform(test_arr)\n",
    "    return train_scaled, val_scaled, test_scaled\n",
    "# Load CSV\n",
    "print(f'Loading CSV: {CSV_PATH}')\n",
    "df = load_mnq_csv(CSV_PATH, INDICATORS_TO_USE, RESAMPLE_RULE)\n",
    "print(f'Features selected: {len(INDICATORS_TO_USE)} | {INDICATORS_TO_USE}')\n",
    "data = df.values.astype(np.float32)\n",
    "print(f'Total rows after TA + cleanup: {len(df)} | Feature dim: {data.shape[1]}')\n",
    "# Chronological split (no shuffling)\n",
    "train_raw, val_raw, test_raw = chronological_split(data, TRAIN_VALID_TEST_SPLIT)\n",
    "print(f'Split -> train: {train_raw.shape}, val: {val_raw.shape}, test: {test_raw.shape}')\n",
    "# Train-only normalization\n",
    "scaler = make_scaler(NORMALIZATION_TYPE)\n",
    "train_scaled, val_scaled, test_scaled = fit_transform_splits(train_raw, val_raw, test_raw, scaler)\n",
    "if NORMALIZATION_TYPE == 'StandardScaler':\n",
    "    means = scaler.mean_\n",
    "    stds = scaler.scale_ if hasattr(scaler, 'scale_') else np.sqrt(scaler.var_)\n",
    "    print(f'Scaler(Standard) -> mean range [{means.min():.4f}, {means.max():.4f}] | std range [{stds.min():.4f}, {stds.max():.4f}]')\n",
    "else:\n",
    "    mins = scaler.data_min_\n",
    "    maxs = scaler.data_max_\n",
    "    print(f'Scaler(MinMax) -> min range [{mins.min():.4f}, {mins.max():.4f}] | max range [{maxs.min():.4f}, {maxs.max():.4f}]')\n",
    "def _count_windows(n, L, s):\n",
    "    return max(0, (n - L) // s + 1)\n",
    "nw_train = _count_windows(len(train_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
    "nw_val = _count_windows(len(val_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
    "nw_test = _count_windows(len(test_scaled), SEQ_LEN, WINDOW_STRIDE)\n",
    "print(f'Windows -> train: {nw_train}, val: {nw_val}, test: {nw_test} | stride={WINDOW_STRIDE}, seq_len={SEQ_LEN}')\n",
    "train_scaled.shape, val_scaled.shape, test_scaled.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1bfd75",
   "metadata": {
    "id": "fc1bfd75"
   },
   "source": [
    "### Dataset & DataLoaders (windowed, no shuffle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432e2afc",
   "metadata": {
    "id": "432e2afc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class MNQ_Dataset(Dataset):\n",
    "    def __init__(self, arr_2d: np.ndarray, seq_len: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        self.x = arr_2d\n",
    "        self.seq_len = int(seq_len)\n",
    "        self.stride = int(stride)\n",
    "        self.T = len(arr_2d)\n",
    "        self.C = arr_2d.shape[1]\n",
    "        if self.T < self.seq_len:\n",
    "            raise ValueError(f\"Not enough timesteps ({self.T}) for seq_len={self.seq_len}\")\n",
    "        # Number of windows using stride\n",
    "        self.idxs = list(range(0, self.T - self.seq_len + 1, self.stride))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.idxs[idx]\n",
    "        end = start + self.seq_len\n",
    "        window = self.x[start:end]  # shape [seq_len, C]\n",
    "        # model expects [L, C], training target == input window (self-supervised)\n",
    "        return torch.from_numpy(window).float()\n",
    "\n",
    "def make_loader(arr: np.ndarray, seq_len: int, stride: int, batch_size: int) -> DataLoader:\n",
    "    ds = MNQ_Dataset(arr, seq_len=seq_len, stride=stride)\n",
    "    # No shuffling to avoid any perceived leakage; data is chronologically windowed already\n",
    "\n",
    "    # Ajuste automÃ¡tico de batch para evitar Ã©poca vazia\n",
    "    bs = int(batch_size)\n",
    "    nwin = len(ds)\n",
    "    if nwin < bs:\n",
    "        print(f'WARN: batch_size {bs} > windows {nwin}; ajustando para evitar Ã©poca vazia.')\n",
    "        bs = max(1, nwin)\n",
    "\n",
    "    # Try multi-worker loader; if it fails, fallback to single-process\n",
    "    try:\n",
    "        _test_ldr = DataLoader(\n",
    "            ds,\n",
    "            batch_size=bs,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=DATALOADER_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=(PERSISTENT_WORKERS and DATALOADER_WORKERS>0),\n",
    "            prefetch_factor=(PREFETCH_FACTOR if DATALOADER_WORKERS>0 else 2),\n",
    "        )\n",
    "        # Trigger workers by fetching one batch\n",
    "        _it = iter(_test_ldr)\n",
    "        _ = next(_it)\n",
    "        del _it, _\n",
    "        # Rebuild fresh loader for actual training\n",
    "        ldr = DataLoader(\n",
    "            ds,\n",
    "            batch_size=bs,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=DATALOADER_WORKERS,\n",
    "            pin_memory=PIN_MEMORY,\n",
    "            persistent_workers=(PERSISTENT_WORKERS and DATALOADER_WORKERS>0),\n",
    "            prefetch_factor=(PREFETCH_FACTOR if DATALOADER_WORKERS>0 else 2),\n",
    "        )\n",
    "    except Exception as _e:\n",
    "        print(f\"WARN: DataLoader workers failed ({type(_e).__name__}: {_e}); FALLBACK to single-process DataLoader\")\n",
    "        ldr = DataLoader(ds, batch_size=bs, shuffle=False, drop_last=False, num_workers=0, pin_memory=False)\n",
    "    return ldr\n",
    "\n",
    "\n",
    "train_loader = make_loader(train_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
    "val_loader = make_loader(val_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
    "test_loader = make_loader(test_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n",
    "\n",
    "tw, vw, tew = len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)\n",
    "tb, vb, teb = len(train_loader), len(val_loader), len(test_loader)\n",
    "print(f'DataLoaders -> windows (train/val/test): {tw}/{vw}/{tew} | batches: {tb}/{vb}/{teb} | batch_size={BATCH_SIZE}')\n",
    "print(f'Window shape -> L={SEQ_LEN}, C={train_scaled.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29d00a1",
   "metadata": {
    "id": "c29d00a1"
   },
   "source": [
    "### Model: TimesBlock + Inception + Feature Extractor + Decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q_0Oj0KHtrEF",
   "metadata": {
    "id": "q_0Oj0KHtrEF"
   },
   "outputs": [],
   "source": [
    "class InceptionBlock2D(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        # Split out_channels across branches\n",
    "        base, rem = divmod(out_channels, 4)\n",
    "        branch_sizes = [base + (1 if i < rem else 0) for i in range(4)]\n",
    "        b1, b2, b3, b4 = branch_sizes\n",
    "\n",
    "        self.branch1x1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, b1, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(b1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        red3 = max(in_channels // 2, 8)\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red3, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(red3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(red3, b2, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(b2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        red5 = max(in_channels // 2, 8)\n",
    "        self.branch5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, red5, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(red5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(red5, b3, kernel_size=5, padding=2, bias=False),\n",
    "            nn.BatchNorm2d(b3),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, b4, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(b4),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        o1 = self.branch1x1(x)\n",
    "        o2 = self.branch3x3(x)\n",
    "        o3 = self.branch5x5(x)\n",
    "        o4 = self.branch_pool(x)\n",
    "        out = torch.cat([o1, o2, o3, o4], dim=1)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class TimesBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    FFT-based period discovery + 1D->2D folding + 2D Inception backbone + weighted fusion.\n",
    "    Input:  x [B, L, C]\n",
    "    Output: embedding [B, E, H, W]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.k = k\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_h = embed_h\n",
    "        self.embed_w = embed_w\n",
    "        self.backbone = InceptionBlock2D(in_channels, embed_dim, dropout=dropout)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _find_topk_periods(self, x_bc_l: torch.Tensor, k: int) -> tuple[list[int], torch.Tensor]:\n",
    "        # x_bc_l: [B, C, L]\n",
    "        B, C, L = x_bc_l.shape\n",
    "        xf = torch.fft.rfft(x_bc_l, dim=-1)  # [B, C, L//2 + 1]\n",
    "        amp = xf.abs().mean(dim=(0, 1))      # [L//2 + 1], averaged over batch & channels\n",
    "        if amp.shape[0] <= 1:\n",
    "            return [L], torch.tensor([1.0], device=x_bc_l.device)\n",
    "\n",
    "        amp[0] = 0.0  # ignore DC\n",
    "        k_eff = min(k, amp.shape[0]-1)\n",
    "        vals, idxs = torch.topk(amp, k=k_eff, largest=True, sorted=True)\n",
    "        periods = []\n",
    "        for idx in idxs.tolist():\n",
    "            p = int(round(L / max(idx, 1)))\n",
    "            p = max(p, 2)\n",
    "            periods.append(p)\n",
    "        # Softmax weights from amplitudes\n",
    "        w = torch.softmax(vals, dim=0)\n",
    "        return periods, w\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, C]\n",
    "        B, L, C = x.shape\n",
    "        x_bc_l = x.permute(0, 2, 1).contiguous()  # [B, C, L]\n",
    "\n",
    "        periods, weights = self._find_topk_periods(x_bc_l, self.k)\n",
    "        feats = None\n",
    "        for i, p in enumerate(periods):\n",
    "            pad_len = (p - (L % p)) % p\n",
    "            x_pad = F.pad(x_bc_l, (0, pad_len), mode='constant', value=0.0)  # [B,C,Lp]\n",
    "            Lp = x_pad.shape[-1]\n",
    "            w_ = Lp // p\n",
    "            # Fold: [B, C, p, w_]\n",
    "            x_2d = x_pad.view(B, C, w_, p).transpose(2, 3).contiguous()\n",
    "\n",
    "            z = self.backbone(x_2d)  # [B, E, h, w]\n",
    "            z = F.adaptive_avg_pool2d(z, (self.embed_h, self.embed_w))  # [B,E,H,W]\n",
    "            z = z * weights[i].view(1, 1, 1, 1)  # weight this period\n",
    "\n",
    "            feats = z if feats is None else (feats + z)\n",
    "\n",
    "        return feats  # [B, E, H, W]\n",
    "\n",
    "class TimesNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.block = TimesBlock(in_channels, k, embed_dim, embed_h, embed_w, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, L, C]\n",
    "        emb = self.block(x)\n",
    "        return self.dropout(emb)  # [B, E, H, W]\n",
    "\n",
    "class ModelWithDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper for self-supervised training (reconstruction).\n",
    "    During inference, use extractor only.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, seq_len: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float=0.1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.extractor = TimesNetFeatureExtractor(\n",
    "            in_channels=in_channels,\n",
    "            k=k,\n",
    "            embed_dim=embed_dim,\n",
    "            embed_h=embed_h,\n",
    "            embed_w=embed_w,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        flat_size = embed_dim * embed_h * embed_w\n",
    "        hidden = max(512, flat_size // 2)\n",
    "\n",
    "        # Using Sequential with explicit layers instead of Flatten\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(flat_size, hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(hidden, seq_len * in_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: [B, L, C]\n",
    "        emb = self.extractor(x)  # [B, E, H, W]\n",
    "        # Explicitly permute to channels_first before flattening with view\n",
    "        if emb.dim() == 4:\n",
    "             # Assuming channels_last if not channels_first or original channels\n",
    "             emb = emb.permute(0, 1, 2, 3).contiguous() # Ensure channels_first (no-op if already)\n",
    "\n",
    "        # Debug: print(f\"Shape before flattening: {emb.shape}\")\n",
    "        emb_flat = emb.view(emb.size(0), -1) # Explicit flatten\n",
    "        rec = self.decoder(emb_flat).view(x.shape[0], self.seq_len, self.in_channels)\n",
    "        return rec, emb\n",
    "\n",
    "    def strip_decoder(self):\n",
    "        self.decoder = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ae284",
   "metadata": {
    "id": "595ae284"
   },
   "source": [
    "### Training Utilities: EarlyStopping, Checkpointing, Train/Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40cbc82",
   "metadata": {
    "id": "d40cbc82"
   },
   "outputs": [],
   "source": [
    "\n",
    "from contextlib import nullcontext\n",
    "\n",
    "DEFAULT_USE_AMP = bool(\n",
    "    torch.cuda.is_available()\n",
    "    and (DEVICE == 'cuda')\n",
    "    and USE_BF16\n",
    "    and getattr(torch.cuda, 'is_bf16_supported', lambda: False)()\n",
    ")\n",
    "DEFAULT_AMP_DTYPE = torch.bfloat16 if DEFAULT_USE_AMP else None\n",
    "# Backwards-compatible aliases used elsewhere in the notebook\n",
    "USE_AMP = DEFAULT_USE_AMP\n",
    "AMP_DTYPE = DEFAULT_AMP_DTYPE\n",
    "\n",
    "\n",
    "def _resolve_amp(amp_enabled: bool | None = None, amp_dtype: torch.dtype | None = None) -> tuple[bool, torch.dtype | None]:\n",
    "    enabled = DEFAULT_USE_AMP if amp_enabled is None else bool(amp_enabled)\n",
    "    dtype = DEFAULT_AMP_DTYPE if amp_dtype is None else amp_dtype\n",
    "    if not enabled:\n",
    "        dtype = None\n",
    "    return enabled, dtype\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 7, min_delta: float = 0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = None\n",
    "        self.num_bad = 0\n",
    "\n",
    "    def step(self, value: float) -> bool:\n",
    "        if self.best is None or value < self.best - self.min_delta:\n",
    "            self.best = value\n",
    "            self.num_bad = 0\n",
    "            return False  # keep training\n",
    "        self.num_bad += 1\n",
    "        return self.num_bad >= self.patience\n",
    "\n",
    "\n",
    "def save_checkpoint(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, epoch: int, best_val: float, extra: dict | None = None):\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_loss': best_val,\n",
    "    }\n",
    "    if extra:\n",
    "        state['extra'] = extra\n",
    "    torch.save(state, path)\n",
    "\n",
    "\n",
    "def load_checkpoint_if_any(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, resume: bool):\n",
    "    start_epoch = 1\n",
    "    best_val = float('inf')\n",
    "    if resume and os.path.exists(path):\n",
    "        ckpt = torch.load(path, map_location='cpu')\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "        optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
    "        start_epoch = ckpt.get('epoch', 1) + 1\n",
    "        best_val = ckpt.get('best_val_loss', float('inf'))\n",
    "        if (not math.isfinite(best_val)) or (best_val <= 0):\n",
    "            print('WARN: invalid best_val in checkpoint; ignoring it')\n",
    "            best_val = float('inf')\n",
    "        print(f'Resuming from epoch {start_epoch-1} with best_val={best_val:.6f}')\n",
    "    return start_epoch, best_val\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler=None,\n",
    "    clip_grad_norm: float | None = None,\n",
    "    aug_noise_std: float = 0.0,\n",
    "    *,\n",
    "    grad_accum_steps: int | None = None,\n",
    "    amp_enabled: bool | None = None,\n",
    "    amp_dtype: torch.dtype | None = None,\n",
    "    scaler: torch.cuda.amp.GradScaler | None = None,\n",
    "    max_batches: int | None = None,\n",
    "):\n",
    "    model.train()\n",
    "    mse = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    seen = 0\n",
    "    grad_accum = max(int(grad_accum_steps or GRAD_ACCUM_STEPS), 1)\n",
    "    use_amp, resolved_dtype = _resolve_amp(amp_enabled=amp_enabled, amp_dtype=amp_dtype)\n",
    "    ctx_factory = (\n",
    "        lambda: torch.autocast(device_type='cuda', dtype=resolved_dtype)\n",
    "        if use_amp else nullcontext\n",
    "    )\n",
    "    use_scaler = scaler is not None and getattr(scaler, 'is_enabled', lambda: True)()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step_idx, xb in enumerate(loader):\n",
    "        if max_batches is not None and step_idx >= max_batches:\n",
    "            break\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        if aug_noise_std and aug_noise_std > 0:\n",
    "            noise = torch.randn_like(xb) * float(aug_noise_std)\n",
    "            xb = xb + noise\n",
    "        with ctx_factory():\n",
    "            rec, _ = model(xb)\n",
    "            loss = mse(rec, xb) / grad_accum\n",
    "        loss_value = float(loss.detach().item())\n",
    "        if use_scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        should_step = ((step_idx + 1) % grad_accum) == 0\n",
    "        if should_step:\n",
    "            if clip_grad_norm is not None:\n",
    "                if use_scaler:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(clip_grad_norm))\n",
    "            if use_scaler:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "        total_loss += loss_value * grad_accum * xb.size(0)\n",
    "        seen += xb.size(0)\n",
    "    return total_loss / max(seen, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model,\n",
    "    loader,\n",
    "    device,\n",
    "    aug_noise_std: float = 0.0,\n",
    "    *,\n",
    "    amp_enabled: bool | None = None,\n",
    "    amp_dtype: torch.dtype | None = None,\n",
    "    max_batches: int | None = None,\n",
    "):\n",
    "    model.eval()\n",
    "    mse = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "    seen = 0\n",
    "    use_amp, resolved_dtype = _resolve_amp(amp_enabled=amp_enabled, amp_dtype=amp_dtype)\n",
    "    ctx_factory = (\n",
    "        lambda: torch.autocast(device_type='cuda', dtype=resolved_dtype)\n",
    "        if use_amp else nullcontext\n",
    "    )\n",
    "    for step_idx, xb in enumerate(loader):\n",
    "        if max_batches is not None and step_idx >= max_batches:\n",
    "            break\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with ctx_factory():\n",
    "            rec, _ = model(xb)\n",
    "            loss = mse(rec, xb)\n",
    "        total_loss += float(loss.detach().item()) * xb.size(0)\n",
    "        seen += xb.size(0)\n",
    "    return total_loss / max(seen, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51358a74",
   "metadata": {
    "id": "51358a74"
   },
   "source": [
    "### Initialize Model, Optimizer, and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b714d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMP selection from config (no Optuna)\n",
    "_use_amp_dtype = AMP_DTYPE if 'AMP_DTYPE' in globals() else (\n",
    "    torch.bfloat16 if (('USE_BF16' in globals()) and USE_BF16 and (DEVICE=='cuda') and torch.cuda.is_available()) else None\n",
    ")\n",
    "_amp_enabled = bool((_use_amp_dtype is not None) and (DEVICE=='cuda') and torch.cuda.is_available())\n",
    "try:\n",
    "    _scaler = torch.amp.GradScaler('cuda', enabled=(_use_amp_dtype is torch.float16))\n",
    "except Exception:\n",
    "    _scaler = torch.cuda.amp.GradScaler(enabled=(_use_amp_dtype is torch.float16))\n",
    "\n",
    "in_channels = train_scaled.shape[1]\n",
    "print(f'In-channels (features): {in_channels}')\n",
    "model = ModelWithDecoder(\n",
    "    in_channels=in_channels,\n",
    "    seq_len=SEQ_LEN,\n",
    "    k=TOP_K_PERIODS,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    embed_h=EMBED_H,\n",
    "    embed_w=EMBED_W,\n",
    "    dropout=DROPOUT_RATE\n",
    ").to(DEVICE)\n",
    "if CHANNELS_LAST and (DEVICE == 'cuda'):\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Model params: {params/1e6:.2f}M')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "start_epoch, best_val = load_checkpoint_if_any(CHECKPOINT_PATH, model, optimizer, resume=RESUME_TRAINING)\n",
    "\n",
    "# Ensure the optimizer's initial LR is set before creating the scheduler when resuming\n",
    "if RESUME_TRAINING and os.path.exists(CHECKPOINT_PATH):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = LEARNING_RATE\n",
    "\n",
    "scheduler = None\n",
    "if USE_SCHEDULER:\n",
    "    steps_per_epoch = max(1, (len(train_loader) + max(int(GRAD_ACCUM_STEPS),1) - 1) // max(int(GRAD_ACCUM_STEPS),1))\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=LEARNING_RATE, steps_per_epoch=steps_per_epoch, epochs=NUM_EPOCHS,\n",
    "        pct_start=ONECYCLE_PCT_START, div_factor=ONECYCLE_DIV_FACTOR, final_div_factor=ONECYCLE_FINAL_DIV,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "\n",
    "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "training_history = []\n",
    "early = EarlyStopping(patience=PATIENCE, min_delta=0.0)\n",
    "# If best_val is invalid (e.g., 0.0, inf, nan), compute a baseline on val set\n",
    "if (not math.isfinite(best_val)) or (best_val <= 0):\n",
    "    try:\n",
    "        best_val = evaluate(model, val_loader, DEVICE, aug_noise_std=0.0, amp_enabled=_amp_enabled, amp_dtype=_use_amp_dtype)\n",
    "        print(f\"Baseline val before training: {best_val:.6f}\")\n",
    "    except Exception as e:\n",
    "        print(\"WARN: failed to compute baseline val:\", e)\n",
    "\n",
    "tb, vb = len(train_loader), len(val_loader)\n",
    "tw, vw = len(train_loader.dataset), len(val_loader.dataset)\n",
    "est_opt_steps = (tb + max(int(GRAD_ACCUM_STEPS),1) - 1) // max(int(GRAD_ACCUM_STEPS),1)\n",
    "print(f\"Starting training at epoch {start_epoch} on {DEVICE} | train windows={tw}, batches/epoch={tb}, grad_accum={GRAD_ACCUM_STEPS}, est_opt_steps/epoch={est_opt_steps}\")\n",
    "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
    "    if DEVICE == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    ep_start = time.perf_counter()\n",
    "    train_loss = train_one_epoch(\n",
    "        model, train_loader, optimizer, DEVICE,\n",
    "        scheduler=scheduler, clip_grad_norm=CLIP_GRAD_NORM, aug_noise_std=AUG_NOISE_STD,\n",
    "        grad_accum_steps=GRAD_ACCUM_STEPS, amp_enabled=_amp_enabled, amp_dtype=_use_amp_dtype, scaler=_scaler\n",
    "    )\n",
    "    val_loss = evaluate(model, val_loader, DEVICE, aug_noise_std=0.0, amp_enabled=_amp_enabled, amp_dtype=_use_amp_dtype)\n",
    "    ep_time = time.perf_counter() - ep_start\n",
    "    throughput = len(train_loader.dataset) / max(ep_time, 1e-9)\n",
    "\n",
    "    improved = val_loss < best_val - 1e-12\n",
    "    if improved:\n",
    "        best_val = val_loss\n",
    "        save_checkpoint(\n",
    "            CHECKPOINT_PATH, model, optimizer, epoch, best_val,\n",
    "            extra={\n",
    "                \"INDICATORS_TO_USE\": INDICATORS_TO_USE,\n",
    "                \"NORMALIZATION_TYPE\": NORMALIZATION_TYPE,\n",
    "                \"SEQ_LEN\": SEQ_LEN,\n",
    "                \"TOP_K_PERIODS\": TOP_K_PERIODS,\n",
    "                \"EMBED_DIM\": EMBED_DIM,\n",
    "                \"EMBED_H\": EMBED_H,\n",
    "                \"EMBED_W\": EMBED_W,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    stop = early.step(val_loss)\n",
    "    lr = optimizer.param_groups[0].get('lr', None)\n",
    "    msg = f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f} | {throughput:.0f} samp/s\"\n",
    "    if lr is not None:\n",
    "        msg += f\" | lr={lr:.2e}\"\n",
    "    if improved:\n",
    "        msg += \" | [saved]\"\n",
    "    if PRINT_GPU_MEM and (DEVICE == 'cuda'):\n",
    "        peak = torch.cuda.max_memory_allocated() / 1e9\n",
    "        msg += f\" | GPU peak mem: {peak:.2f} GB\"\n",
    "    print(msg + f\" | patience {early.num_bad}/{PATIENCE}\")\n",
    "\n",
    "    # Save 'last' checkpoint every epoch for robust resume\n",
    "    try:\n",
    "        _last_path = CHECKPOINT_PATH.replace(\"best.pt\", \"last.pt\")\n",
    "        save_checkpoint(_last_path, model, optimizer, epoch, best_val)\n",
    "    except Exception as e:\n",
    "        print(\"WARN: failed to save last checkpoint:\", e)\n",
    "\n",
    "    # Log epoch metrics\n",
    "    try:\n",
    "        import json as _json\n",
    "        rec = {\n",
    "            'epoch': int(epoch),\n",
    "            'train_loss': float(train_loss),\n",
    "            'val_loss': float(val_loss),\n",
    "            'lr': float(lr) if lr is not None else None,\n",
    "            'throughput': float(throughput),\n",
    "            'epoch_time': float(ep_time),\n",
    "            'gpu_peak_gb': float(peak) if (PRINT_GPU_MEM and (DEVICE=='cuda')) else None,\n",
    "            'improved': bool(improved),\n",
    "            'best_val': float(best_val)\n",
    "        }\n",
    "        training_history.append(rec)\n",
    "        with open(HISTORY_PATH, 'a') as f:\n",
    "            f.write(_json.dumps(rec) + '\\n')\n",
    "    except Exception as e:\n",
    "        print('WARN: falha ao gravar histÃ³rico:', e)\n",
    "\n",
    "    if stop:\n",
    "        print(f\"Early stopping at epoch {epoch}. Best val: {best_val:.6f}\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9b7db",
   "metadata": {
    "id": "94b9b7db"
   },
   "source": [
    "### Verification: Embedding-only forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d28ad",
   "metadata": {
    "id": "167d28ad"
   },
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "model = ModelWithDecoder(\n",
    "    in_channels=in_channels,\n",
    "    seq_len=SEQ_LEN,\n",
    "    k=TOP_K_PERIODS,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    embed_h=EMBED_H,\n",
    "    embed_w=EMBED_W,\n",
    "    dropout=DROPOUT_RATE\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n",
    "model.eval()\n",
    "\n",
    "# Remove temporary reconstruction decoder\n",
    "model.strip_decoder()\n",
    "assert model.decoder is None\n",
    "\n",
    "# Take a sample batch from test set and compute embeddings\n",
    "xb = next(iter(test_loader))\n",
    "xb = xb.to(DEVICE, non_blocking=True)\n",
    "with torch.no_grad():\n",
    "    ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n",
    "    with ctx:\n",
    "        # Use the extractor directly for inference-only embeddings\n",
    "        emb = model.extractor(xb)  # [B, E, H, W]\n",
    "\n",
    "print(\"Embedding tensor shape:\", tuple(emb.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171dad07",
   "metadata": {
    "id": "171dad07"
   },
   "source": [
    "### Embeddings: Export para PPO Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec9b158",
   "metadata": {
    "id": "8ec9b158"
   },
   "outputs": [],
   "source": [
    "import json as _json\n",
    "import os\n",
    "from datetime import datetime as _dt\n",
    "\n",
    "import numpy as _np\n",
    "\n",
    "# === Exportar embeddings para consumo por PPO Agent ===\n",
    "# DiretÃ³rio de saÃ­da (ajuste se necessÃ¡rio)\n",
    "EMBED_SAVE_DIR = \"/content/drive/MyDrive/timesnet_mnq/embeddings\"  #@param {type:\"string\"}\n",
    "EMBED_POOLING_EXPORT = 'avg'  #@param [\"avg\", \"flatten\"]\n",
    "os.makedirs(EMBED_SAVE_DIR, exist_ok=True)\n",
    "print(f\"Salvando em: {EMBED_SAVE_DIR}\")\n",
    "@torch.no_grad()\n",
    "def _extract_embeddings(loader, model, device, pooling: str = 'avg'):\n",
    "    model.eval()\n",
    "    vecs = []\n",
    "    ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n",
    "    for xb in loader:\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        with ctx:\n",
    "            emb = model.extractor(xb)  # [B,E,H,W]\n",
    "        if pooling == 'avg':\n",
    "            v = emb.mean(dim=(2,3))  # [B,E]\n",
    "        else:\n",
    "            v = emb.view(emb.size(0), -1)\n",
    "        vecs.append(v.detach().to('cpu').numpy())\n",
    "    X = _np.concatenate(vecs, axis=0)\n",
    "    return X.astype(_np.float32, copy=False)\n",
    "def _export_split(name: str, loader, raw_split: _np.ndarray, close_idx: int):\n",
    "    # Extrai embeddings em ordem de dataset\n",
    "    obs = _extract_embeddings(loader, model, DEVICE, pooling=EMBED_POOLING_EXPORT)\n",
    "    ds = loader.dataset\n",
    "    start_idx = _np.asarray(ds.idxs, dtype=_np.int64)\n",
    "    end_idx = start_idx + int(SEQ_LEN) - 1\n",
    "    # ConsistÃªncia\n",
    "    if obs.shape[0] != len(start_idx):\n",
    "        print(f\"WARN: obs len {obs.shape[0]} != windows {len(start_idx)}; ajustando para o mÃ­nimo comum.\")\n",
    "        n = min(obs.shape[0], len(start_idx))\n",
    "        obs = obs[:n]\n",
    "        start_idx = start_idx[:n]\n",
    "        end_idx = end_idx[:n]\n",
    "    # SÃ©rie de preÃ§os para recompensas na Env PPO\n",
    "    if close_idx is not None and raw_split is not None and raw_split.ndim==2:\n",
    "        close = raw_split[:, close_idx].astype(_np.float64, copy=False)\n",
    "    else:\n",
    "        close = None\n",
    "    out_path = os.path.join(EMBED_SAVE_DIR, f\"{name}_emb_{EMBED_POOLING_EXPORT}.npz\")\n",
    "    save = {\n",
    "        'obs': obs,              # float32 [T,D]\n",
    "        'X': obs,                # alias\n",
    "        'start_idx': start_idx,  # int64 [T]\n",
    "        'end_idx': end_idx,      # int64 [T]\n",
    "    }\n",
    "    if close is not None:\n",
    "        save['close'] = close    # float64 [N]\n",
    "    _np.savez_compressed(out_path, **save)\n",
    "    print(f\"{name}: salvo obs{obs.shape} -> {out_path}\")\n",
    "    return out_path\n",
    "# Determina o Ã­ndice da coluna 'close' para exportarmos a sÃ©rie de preÃ§os\n",
    "try:\n",
    "    _close_idx = INDICATORS_TO_USE.index('close')\n",
    "except Exception:\n",
    "    _close_idx = None\n",
    "splits = {\n",
    "    'train': (train_loader, train_raw),\n",
    "    'val':   (val_loader,   val_raw),\n",
    "    'test':  (test_loader,  test_raw),\n",
    "}\n",
    "paths = {}\n",
    "for _name, (_loader, _raw) in splits.items():\n",
    "    paths[_name] = _export_split(_name, _loader, _raw, _close_idx)\n",
    "# Metadados para consumidores downstream (Env PPO)\n",
    "first_split = next(iter(paths.values())) if paths else None\n",
    "if first_split is not None:\n",
    "    with _np.load(first_split, allow_pickle=False) as _tmp:\n",
    "        _export_dim = int(_tmp['obs'].shape[1])\n",
    "else:\n",
    "    _export_dim = int(EMBED_DIM) if EMBED_POOLING_EXPORT == 'avg' else int(EMBED_DIM) * int(EMBED_H) * int(EMBED_W)\n",
    "meta = {\n",
    "    'schema_version': 1,\n",
    "    'created_at': _dt.utcnow().isoformat()+'Z',\n",
    "    'seq_len': int(SEQ_LEN),\n",
    "    'window_stride': int(WINDOW_STRIDE),\n",
    "    'pooling': EMBED_POOLING_EXPORT,\n",
    "    'embed_dim': int(_export_dim),   # dimensÃ£o do vetor 'obs'\n",
    "    'embed_hw': [int(EMBED_H), int(EMBED_W)],\n",
    "    'features': INDICATORS_TO_USE,\n",
    "    'normalization': NORMALIZATION_TYPE,\n",
    "    'timesnet_checkpoint_path': CHECKPOINT_PATH,\n",
    "    'splits': paths,\n",
    "    'index_semantics': \"end_idx indexa o vetor 'close' do mesmo split\"\n",
    "}\n",
    "meta_path = os.path.join(EMBED_SAVE_DIR, 'embeddings_meta.json')\n",
    "with open(meta_path, 'w') as f:\n",
    "    _json.dump(meta, f, indent=2)\n",
    "print(f\"Meta salvo em: {meta_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
