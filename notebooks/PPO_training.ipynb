{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWqhE-eMcrSn"
      },
      "source": [
        "# 1) Setup & Install\n",
        "Install dependencies for Gymnasium and Stable-Baselines3.\n"
      ],
      "id": "XWqhE-eMcrSn"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "91149d46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91149d46",
        "outputId": "3cf115a2-036e-483d-d7c3-66fb77a9b6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping dopamine-rl as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping gym as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1) Setup & Install — deps for RL and plotting\n",
        "# Pin SB3 to a non-yanked release and avoid dopamine-rl conflict with Gymnasium 0.29.x\n",
        "!pip -q install gymnasium==0.29.1 stable-baselines3==2.3.1 tensorboard matplotlib pandas\n",
        "!pip -q uninstall -y dopamine-rl || true\n",
        "!pip -q uninstall -y gym || true\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QptH4HRcrSp"
      },
      "source": [
        "# 2) Google Drive Mount\n",
        "Access TimesNet embeddings stored in Drive.\n"
      ],
      "id": "9QptH4HRcrSp"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etn6wwtWcrSq",
        "outputId": "b1f7c5f5-56fa-48a8-a72d-8a4b09ca3d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 2) Google Drive Mount — access data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "id": "Etn6wwtWcrSq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8FFJ5m-crSq"
      },
      "source": [
        "# 3) Configuration\n",
        "Paths, market specifics, costs, rewards, Guardian thresholds, and PPO hyperparameters.\n"
      ],
      "id": "y8FFJ5m-crSq"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a53e0313",
      "metadata": {
        "id": "a53e0313"
      },
      "outputs": [],
      "source": [
        "# 3) Configuration — paths and parameters\n",
        "import os\n",
        "\n",
        "# Data paths (TimesNet embeddings)\n",
        "DATA_DIR = '/content/drive/MyDrive/timesnet_mnq/embeddings/'  # Corrected to be a directory\n",
        "TRAIN_NPZ = os.path.join(DATA_DIR, 'train_emb_avg.npz')\n",
        "VAL_NPZ   = os.path.join(DATA_DIR, 'val_emb_avg.npz')\n",
        "TEST_NPZ  = os.path.join(DATA_DIR, 'test_emb_avg.npz')\n",
        "META_JSON = os.path.join(DATA_DIR, 'embeddings_meta.json')\n",
        "\n",
        "# OHLC paths for ATR only (no direct use in observations)\n",
        "RAW_DATA_DIR = '/content'  # where you upload mnq_complete_dataset.csv in Colab\n",
        "TRAIN_OHLC_CSV = os.path.join(RAW_DATA_DIR, 'mnq_train.csv')\n",
        "VAL_OHLC_CSV   = os.path.join(RAW_DATA_DIR, 'mnq_val.csv')\n",
        "TEST_OHLC_CSV  = os.path.join(RAW_DATA_DIR, 'mnq_test.csv')\n",
        "OHLC_RESAMPLE_RULE = '5min'  # aggregate 1-min to 5-min to match TimesNet (pandas>=2 warning safe)\n",
        "\n",
        "# Bar timing (5-minute bars) and annualization\n",
        "BAR_SECONDS = 300\n",
        "STEPS_PER_DAY = 288  # 24*60/5\n",
        "STEPS_PER_YEAR = int(STEPS_PER_DAY * 252)\n",
        "\n",
        "# Market specifics\n",
        "POINT_VALUE_MNQ = 2.0      # USD per point\n",
        "TICK_SIZE = 0.25\n",
        "TICK_VALUE_USD = POINT_VALUE_MNQ * TICK_SIZE  # $0.50 per tick\n",
        "\n",
        "# Costs (defaults tuned)\n",
        "FIXED_COMMISSION_USD = 1.00\n",
        "ATR_SLIPPAGE_MULTIPLIER = 0.25  # fraction of ATR in points\n",
        "\n",
        "# Risk parameters (training env termination)\n",
        "MAX_EPISODE_DRAWDOWN_USD = 500.0\n",
        "\n",
        "# Reward policy coefficients (scaled to be impactful)\n",
        "# For a $20 drawdown: penalty = 0.005 * (20**2) = 2.0\n",
        "# For a $20 profit:   bonus   = 0.001 * 20      = 0.02\n",
        "DRAWDOWN_PENALTY_COEFF = 0.005\n",
        "PROFIT_KEEPING_BONUS_COEFF = 0.001\n",
        "PATIENCE_BONUS = 0.5\n",
        "CHOPPY_ATR_TO_PRICE_MAX = 0.002\n",
        "\n",
        "# Guardian thresholds (backtest only, ATR-based)\n",
        "GUARDIAN_SL_ATR_MULTIPLIER = 2.0\n",
        "GUARDIAN_TS_ATR_MULTIPLIER = 2.0\n",
        "GUARDIAN_BE_ACTIVATION_ATR_MULTIPLIER = 1.5\n",
        "GUARDIAN_BE_PLUS_TICKS = 2  # +2 ticks beyond entry when BE activates\n",
        "\n",
        "# Environment\n",
        "EPISODE_LENGTH = 1024\n",
        "N_ENVS = 8\n",
        "SEED = 42\n",
        "\n",
        "# PPO (SB3) hyperparameters (MVP defaults)\n",
        "PPO_KW = dict(\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_range=0.2,\n",
        "    ent_coef=0.0,\n",
        "    vf_coef=0.5,\n",
        "    max_grad_norm=0.5,\n",
        ")\n",
        "\n",
        "# Training run cadence\n",
        "TOTAL_TIMESTEPS = 1_000_000\n",
        "EVAL_FREQ_STEPS = 20480\n",
        "LOG_DIR = '/content/drive/MyDrive/timesnet_mnq/ppo_logs'\n",
        "BEST_MODEL_PATH = os.path.join(LOG_DIR, 'best_model.zip')\n",
        "os.makedirs(LOG_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54aa538d"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Assuming DATA_DIR is correctly set in a previous cell\n",
        "# If not, you may need to define it here or run the configuration cell first.\n",
        "try:\n",
        "    with np.load(TRAIN_NPZ, allow_pickle=False) as Z:\n",
        "        print(f\"Keys in {TRAIN_NPZ}: {list(Z.keys())}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {TRAIN_NPZ} not found. Please ensure DATA_DIR is set correctly and the file exists.\")"
      ],
      "id": "54aa538d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a2191fec",
      "metadata": {
        "id": "a2191fec"
      },
      "source": [
        "# 4) Imports & Utilities\n",
        "Helpers for data loading and metrics (Calmar, Sharpe, MDD).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f904ed4d",
      "metadata": {
        "id": "f904ed4d"
      },
      "outputs": [],
      "source": [
        "# 4) Imports & Utilities — loaders and metrics\n",
        "import json, math\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional\n",
        "\n",
        "def _read_ohlc_csv(csv_path: str, resample_rule: Optional[str] = None):\n",
        "    import pandas as pd\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "    if 'timestamp' in df.columns:\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df = df.sort_values('timestamp').set_index('timestamp')\n",
        "    if resample_rule:\n",
        "        # Match TimesNet resample semantics: right-closed, right-labeled bars\n",
        "        agg_base = {'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}\n",
        "        agg = {col: agg_base.get(col, 'last') for col in df.columns}\n",
        "        df = df.resample(resample_rule, label='right', closed='right').agg(agg)\n",
        "        df = df.dropna(how='any').reset_index()\n",
        "    return df\n",
        "\n",
        "def _atr_wilder(high: np.ndarray, low: np.ndarray, close: np.ndarray, period: int = 14) -> np.ndarray:\n",
        "    high = high.astype(np.float64)\n",
        "    low = low.astype(np.float64)\n",
        "    close = close.astype(np.float64)\n",
        "    n = len(close)\n",
        "    atr = np.empty(n, dtype=np.float64)\n",
        "    prev_close = close[0]\n",
        "    tr0 = max(high[0]-low[0], abs(high[0]-prev_close), abs(low[0]-prev_close))\n",
        "    atr[0] = tr0\n",
        "    for t in range(1, n):\n",
        "        tr = max(high[t]-low[t], abs(high[t]-close[t-1]), abs(low[t]-close[t-1]))\n",
        "        if t < period:\n",
        "            atr[t] = (atr[t-1]*t + tr) / (t+1)\n",
        "        else:\n",
        "            atr[t] = (atr[t-1]*(period-1) + tr) / period\n",
        "    return atr\n",
        "\n",
        "def load_split(npz_path: str, ohlc_csv_path: Optional[str] = None, resample_rule: Optional[str] = None, atr_period: int = 14):\n",
        "    with np.load(npz_path, allow_pickle=False) as Z:\n",
        "        obs = Z['obs'] if 'obs' in Z.files else Z['X']\n",
        "        start_idx = Z['start_idx']\n",
        "        end_idx = Z['end_idx']\n",
        "        close = Z['close'] if 'close' in Z.files else None\n",
        "        atr = None\n",
        "        for k in ['atr', 'ATR', 'atr_14', 'ATR_14']:\n",
        "            if k in Z.files:\n",
        "                atr = Z[k]\n",
        "                break\n",
        "    if close is None:\n",
        "        raise ValueError('NPZ must contain close series')\n",
        "    if ohlc_csv_path:\n",
        "        df = _read_ohlc_csv(ohlc_csv_path, resample_rule)\n",
        "        for col in ['high','low','close']:\n",
        "            if col not in df.columns:\n",
        "                raise ValueError(f'OHLC CSV must contain {col} column')\n",
        "        atr_ohlc = _atr_wilder(df['high'].to_numpy(), df['low'].to_numpy(), df['close'].to_numpy(), period=atr_period)\n",
        "        if len(atr_ohlc) != len(close):\n",
        "            raise ValueError(f'Length mismatch between OHLC ({len(atr_ohlc)}) and NPZ close ({len(close)}). Provide split-matching OHLC CSV or adjust resample_rule.')\n",
        "        atr = atr_ohlc\n",
        "    if atr is None:\n",
        "        raise ValueError(\"ATR not found. Provide ohlc_csv_path for ATR computation or include an 'atr' series in the NPZ.\")\n",
        "    return obs.astype(np.float32), start_idx.astype(np.int64), end_idx.astype(np.int64), close.astype(np.float64), atr.astype(np.float64)\n",
        "\n",
        "def synth_time_features(n: int, steps_per_day: int = STEPS_PER_DAY) -> np.ndarray:\n",
        "    t = np.arange(n, dtype=np.float32)\n",
        "    hour_phase = 2*np.pi * (t % steps_per_day) / max(steps_per_day, 1)\n",
        "    week_phase = 2*np.pi * (t % (steps_per_day*5)) / max(steps_per_day*5, 1)\n",
        "    return np.stack([np.sin(hour_phase), np.cos(hour_phase), np.sin(week_phase), np.cos(week_phase)], axis=1).astype(np.float32)\n",
        "\n",
        "def map_action_to_position(action: int) -> int:\n",
        "    # 0->short(-1), 1->flat(0), 2->long(+1)\n",
        "    return (-1 if action == 0 else (0 if action == 1 else 1))\n",
        "\n",
        "def pnl_from_price_change(prev_pos: int, price_change: float, point_value: float) -> float:\n",
        "    return float(prev_pos * price_change * point_value)\n",
        "\n",
        "def compute_drawdown(equity: np.ndarray) -> np.ndarray:\n",
        "    peaks = np.maximum.accumulate(equity)\n",
        "    dd = (peaks - equity) / np.maximum(peaks, 1e-9)\n",
        "    return dd\n",
        "\n",
        "def max_drawdown(equity: np.ndarray) -> float:\n",
        "    dd = compute_drawdown(equity)\n",
        "    return float(np.nanmax(dd) if len(dd) else 0.0)\n",
        "\n",
        "def cagr_from_equity(equity: np.ndarray) -> float:\n",
        "    if len(equity) < 2 or equity[0] <= 0:\n",
        "        return 0.0\n",
        "    years = len(equity) / max(STEPS_PER_YEAR, 1)\n",
        "    if years <= 0:\n",
        "        return 0.0\n",
        "    return float((equity[-1] / equity[0]) ** (1.0 / years) - 1.0)\n",
        "\n",
        "def calmar_ratio(equity: np.ndarray) -> float:\n",
        "    mdd = max_drawdown(equity)\n",
        "    cagr = cagr_from_equity(equity)\n",
        "    if mdd <= 0:\n",
        "        return float('inf') if cagr > 0 else 0.0\n",
        "    return cagr / mdd\n",
        "\n",
        "def sharpe_ratio(returns: np.ndarray, risk_free=0.0) -> float:\n",
        "    if len(returns) < 2:\n",
        "        return 0.0\n",
        "    mean = float(np.mean(returns))\n",
        "    std = float(np.std(returns, ddof=1))\n",
        "    if std <= 1e-12:\n",
        "        return 0.0\n",
        "    ann_factor = math.sqrt(STEPS_PER_YEAR)\n",
        "    return float((mean - risk_free) / std * ann_factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "materialize_ohlc"
      },
      "outputs": [],
      "source": [
        "# 3b) Materializar OHLC por split para ATR (alinha com os NPZ)\n",
        "# Use quando você tiver um único CSV base (mnq_complete_dataset.csv) e quiser gerar train/val/test CSVs.\n",
        "# Requer que os NPZ (TRAIN_NPZ/VAL_NPZ/TEST_NPZ) já existam no DATA_DIR.\n",
        "import numpy as np, pandas as pd\n",
        "BASE_OHLC_CSV = os.path.join(RAW_DATA_DIR, 'mnq_complete_dataset.csv')\n",
        "if os.path.exists(BASE_OHLC_CSV):\n",
        "    df_all = _read_ohlc_csv(BASE_OHLC_CSV, resample_rule=OHLC_RESAMPLE_RULE)\n",
        "    def _len_close(npz_path):\n",
        "        with np.load(npz_path, allow_pickle=False) as Z:\n",
        "            return len(Z['close'] if 'close' in Z.files else Z['X'])\n",
        "    n_train = _len_close(TRAIN_NPZ)\n",
        "    n_val   = _len_close(VAL_NPZ)\n",
        "    n_test  = _len_close(TEST_NPZ)\n",
        "    total   = n_train + n_val + n_test\n",
        "    if len(df_all) < total:\n",
        "        raise ValueError(f'OHLC length {len(df_all)} < NPZ total {total}. Verifique CSV base e resample.')\n",
        "    # Compensa warm-up/limpezas no TimesNet: corta excesso do início para casar os comprimentos\n",
        "    excess = len(df_all) - total\n",
        "    if excess > 0:\n",
        "        df_all = df_all.iloc[excess:]\n",
        "    assert len(df_all) == total\n",
        "    os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
        "    _train_csv = os.path.join(RAW_DATA_DIR, 'mnq_train.csv')\n",
        "    _val_csv   = os.path.join(RAW_DATA_DIR, 'mnq_val.csv')\n",
        "    _test_csv  = os.path.join(RAW_DATA_DIR, 'mnq_test.csv')\n",
        "    df_all.iloc[:n_train].to_csv(_train_csv, index=False)\n",
        "    df_all.iloc[n_train:n_train+n_val].to_csv(_val_csv, index=False)\n",
        "    df_all.iloc[n_train+n_val:].to_csv(_test_csv, index=False)\n",
        "    print('Wrote:', _train_csv, _val_csv, _test_csv)\n",
        "    # Atualiza caminhos usados pelo PPO\n",
        "    TRAIN_OHLC_CSV = _train_csv\n",
        "    VAL_OHLC_CSV   = _val_csv\n",
        "    TEST_OHLC_CSV  = _test_csv\n",
        "else:\n",
        "    print('BASE_OHLC_CSV não encontrado; pule esta célula se já tiver os CSVs por split.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsCOCr0ocrSs"
      },
      "source": [
        "# 5) Environment (MNQEmbEnv)\n",
        "Training-only risk shaping on embeddings; no hard stops.\n"
      ],
      "id": "bsCOCr0ocrSs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00036e3d",
      "metadata": {
        "id": "00036e3d"
      },
      "outputs": [],
      "source": [
        "# 5) Environment — MNQEmbEnv\n",
        "class MNQEmbEnv(gym.Env):\n",
        "    metadata = {'render.modes': []}\n",
        "    def __init__(\n",
        "        self,\n",
        "        npz_path: str,\n",
        "        meta_path: Optional[str] = None,\n",
        "        ohlc_csv_path: Optional[str] = None,\n",
        "        ohlc_resample_rule: Optional[str] = None,\n",
        "        episode_length: int = EPISODE_LENGTH,\n",
        "        point_value: float = POINT_VALUE_MNQ,\n",
        "        fixed_commission_usd: float = FIXED_COMMISSION_USD,\n",
        "        atr_slippage_multiplier: float = ATR_SLIPPAGE_MULTIPLIER,\n",
        "        drawdown_penalty_coeff: float = DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff: float = PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus: float = PATIENCE_BONUS,\n",
        "        choppy_atr_to_price_max: float = CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd: float = MAX_EPISODE_DRAWDOWN_USD,\n",
        "        seed: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.obs_raw, self.start_idx, self.end_idx, self.close, self.atr = load_split(npz_path, ohlc_csv_path=ohlc_csv_path, resample_rule=ohlc_resample_rule)\n",
        "        self.meta = {}\n",
        "        if meta_path and os.path.exists(meta_path):\n",
        "            with open(meta_path, 'r') as f:\n",
        "                self.meta = json.load(f)\n",
        "        self.T, self.D = self.obs_raw.shape\n",
        "        self.time_features = synth_time_features(self.T, STEPS_PER_DAY)\n",
        "        self.point_value = float(point_value)\n",
        "        self.fixed_commission = float(fixed_commission_usd)\n",
        "        self.atr_slip_mult = float(atr_slippage_multiplier)\n",
        "        self.dd_penalty = float(drawdown_penalty_coeff)\n",
        "        self.profit_bonus = float(profit_keeping_bonus_coeff)\n",
        "        self.patience_bonus_v = float(patience_bonus)\n",
        "        self.choppy_thr = float(choppy_atr_to_price_max)\n",
        "        self.max_episode_dd = float(max_episode_drawdown_usd)\n",
        "        self.episode_length = int(episode_length)\n",
        "\n",
        "        # Observation: [embedding D] + [time 4] + [position 1] + [portfolio 3]\n",
        "        self.obs_dim = self.D + 4 + 1 + 3\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.obs_dim,), dtype=np.float32)\n",
        "        self._reset_state()\n",
        "\n",
        "    def _reset_state(self):\n",
        "        min_start = 1\n",
        "        max_start = max(min_start, self.T - self.episode_length - 2)\n",
        "        self.start = int(self.rng.integers(min_start, max_start+1)) if max_start > min_start else min_start\n",
        "        self.step_in_ep = 0\n",
        "        self.current_step = self.start\n",
        "        self.current_position = 0\n",
        "        self.entry_price = 0.0\n",
        "        self.best_price = 0.0\n",
        "        self.entry_step = 0\n",
        "        self.equity = 1000.0\n",
        "        self.peak_equity = self.equity\n",
        "        self.equity_series = [self.equity]\n",
        "        self.returns_series = []\n",
        "\n",
        "    def is_market_choppy(self, t: int) -> bool:\n",
        "        price = float(self.close[t])\n",
        "        if price <= 0:\n",
        "            return False\n",
        "        return (float(self.atr[t]) / price) <= self.choppy_thr\n",
        "\n",
        "    def _get_obs(self) -> np.ndarray:\n",
        "        emb = self.obs_raw[self.current_step].astype(np.float32)\n",
        "        tf = self.time_features[self.current_step].astype(np.float32)\n",
        "        pf = np.array([self.current_position], dtype=np.float32)\n",
        "        # === Portfolio features ===\n",
        "        if self.current_position != 0:\n",
        "            atr_entry = float(max(self.atr[self.entry_step], 1e-9))\n",
        "            pnl_usd = (float(self.close[self.current_step]) - self.entry_price) * self.current_position * self.point_value\n",
        "            pnl_norm = float(pnl_usd) / max(atr_entry * self.point_value, 1e-9)\n",
        "            bars_in_trade = max(0, self.current_step - int(self.entry_step))\n",
        "        else:\n",
        "            pnl_norm = 0.0\n",
        "            bars_in_trade = 0\n",
        "        dur_norm = float(bars_in_trade) / max(self.episode_length, 1)\n",
        "        dd_usd = float(self.peak_equity - self.equity)\n",
        "        dd_norm = dd_usd / max(self.max_episode_dd, 1e-6)\n",
        "        portfolio_state = np.array([pnl_norm, dur_norm, dd_norm], dtype=np.float32)\n",
        "        return np.concatenate([emb, tf, pf, portfolio_state], axis=0)\n",
        "\n",
        "    def reset(self, *, seed: Optional[int] = None, options=None):\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "        self._reset_state()\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action: int):\n",
        "        new_pos = map_action_to_position(int(action))\n",
        "        prev_pos = self.current_position\n",
        "        t = self.current_step\n",
        "        prev_t = max(0, t-1)\n",
        "\n",
        "        # Price change and gross PnL for the previous position\n",
        "        price_change = float(self.close[t] - self.close[prev_t])\n",
        "        gross_pnl = pnl_from_price_change(prev_pos, price_change, self.point_value)\n",
        "\n",
        "        # Transaction costs: charge full round-trip cost only on new trade entry (from flat or reversal)\n",
        "        costs = 0.0\n",
        "        is_new_trade_entry = ((prev_pos == 0 and new_pos != 0) or\n",
        "                              (np.sign(new_pos) != np.sign(prev_pos) and new_pos != 0))\n",
        "        if is_new_trade_entry:\n",
        "            commission = self.fixed_commission\n",
        "            slip_points = self.atr_slip_mult * float(self.atr[t])\n",
        "            slippage = slip_points * self.point_value\n",
        "            costs = commission + slippage\n",
        "        base_reward = gross_pnl - costs\n",
        "\n",
        "        # Unrealized PnL of the active trade (based on prev_pos and prior entry)\n",
        "        if prev_pos != 0:\n",
        "            unrealized_pnl_prev = (float(self.close[t]) - self.entry_price) * prev_pos * self.point_value\n",
        "        else:\n",
        "            unrealized_pnl_prev = 0.0\n",
        "\n",
        "        # Risk penalties and behavioral incentives\n",
        "        current_trade_drawdown = max(0.0, -unrealized_pnl_prev)\n",
        "        drawdown_penalty = self.dd_penalty * (current_trade_drawdown ** 2)\n",
        "        current_trade_profit = max(0.0, unrealized_pnl_prev)\n",
        "        profit_keeping_bonus = self.profit_bonus * current_trade_profit\n",
        "        patience_bonus = (self.patience_bonus_v if self.is_market_choppy(t) and new_pos == 0 else\n",
        "                          (-self.patience_bonus_v if self.is_market_choppy(t) and new_pos != 0 else 0.0))\n",
        "        final_reward = base_reward - drawdown_penalty + profit_keeping_bonus + patience_bonus\n",
        "\n",
        "        # Update equity statistics\n",
        "        self.equity += gross_pnl - costs\n",
        "        self.returns_series.append((gross_pnl - costs) / max(abs(self.equity_series[-1]), 1e-9))\n",
        "        self.equity_series.append(self.equity)\n",
        "        self.peak_equity = max(self.peak_equity, self.equity)\n",
        "        episode_drawdown = self.peak_equity - self.equity\n",
        "        terminated = episode_drawdown > self.max_episode_dd\n",
        "\n",
        "        # Trade state management: handle new entries, exits, and reversals\n",
        "        position_flipped = (np.sign(new_pos) != np.sign(prev_pos))\n",
        "        if (prev_pos == 0 and new_pos != 0) or (position_flipped and new_pos != 0):\n",
        "            # New trade started (from flat or by reversal)\n",
        "            self.entry_price = float(self.close[t])\n",
        "            self.best_price = self.entry_price\n",
        "            self.entry_step = int(t)\n",
        "        elif new_pos == 0 and prev_pos != 0:\n",
        "            # Trade closed\n",
        "            self.entry_price = 0.0\n",
        "            self.best_price = 0.0\n",
        "            self.entry_step = int(self.current_step)\n",
        "        elif new_pos == prev_pos and new_pos != 0:\n",
        "            # Position maintained: update best price favorably\n",
        "            if new_pos > 0:\n",
        "                self.best_price = max(self.best_price, float(self.close[t]))\n",
        "            else:\n",
        "                self.best_price = min(self.best_price, float(self.close[t]))\n",
        "\n",
        "        # Advance step\n",
        "        self.current_position = new_pos\n",
        "        self.current_step += 1\n",
        "        self.step_in_ep += 1\n",
        "        truncated = (self.step_in_ep >= self.episode_length) or (self.current_step >= (self.T - 1))\n",
        "        obs = self._get_obs()\n",
        "        info = {\n",
        "            'gross_pnl': gross_pnl,\n",
        "            'costs': costs,\n",
        "            'equity': self.equity,\n",
        "            'drawdown': episode_drawdown,\n",
        "            'position': self.current_position,\n",
        "        }\n",
        "        return obs, float(final_reward), bool(terminated), bool(truncated), info\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfc606ad",
      "metadata": {
        "id": "cfc606ad"
      },
      "source": [
        "# 6) Guardian & Backtester\n",
        "Guardian applies SL/TS/BE in inference; backtester runs full pass.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1199aa85",
      "metadata": {
        "id": "1199aa85"
      },
      "outputs": [],
      "source": [
        "# 6) Guardian & Backtester — inference safety layer\n",
        "class Guardian:\n",
        "    def __init__(self, point_value=POINT_VALUE_MNQ,\n",
        "                 sl_atr_mult=GUARDIAN_SL_ATR_MULTIPLIER,\n",
        "                 ts_atr_mult=GUARDIAN_TS_ATR_MULTIPLIER,\n",
        "                 be_activation_atr_mult=GUARDIAN_BE_ACTIVATION_ATR_MULTIPLIER,\n",
        "                 be_plus_ticks=GUARDIAN_BE_PLUS_TICKS):\n",
        "        self.point_value = float(point_value)\n",
        "        self.sl_mult = float(sl_atr_mult)\n",
        "        self.ts_mult = float(ts_atr_mult)\n",
        "        self.be_act_mult = float(be_activation_atr_mult)\n",
        "        self.be_plus_ticks = int(be_plus_ticks)\n",
        "\n",
        "def run_backtest(model, env_like: MNQEmbEnv, guardian: Guardian, deterministic=True):\n",
        "    obs_raw = env_like.obs_raw\n",
        "    close = env_like.close\n",
        "    atr = env_like.atr\n",
        "    T = min(len(obs_raw), len(close), len(atr))\n",
        "    close = close[:T]\n",
        "    atr = atr[:T]\n",
        "    position = 0\n",
        "    entry_price = 0.0\n",
        "    entry_step = None\n",
        "    best_price = 0.0\n",
        "    be_activated = False\n",
        "    stop_level = None\n",
        "    equity = 1000.0\n",
        "    peak_equity = equity\n",
        "    equity_series = [equity]\n",
        "    step_returns = []\n",
        "    tf_all = synth_time_features(T, STEPS_PER_DAY)\n",
        "    for t in range(1, T):\n",
        "        # 1) Guardian updates and veto (if position open)\n",
        "        if position != 0:\n",
        "            # Initial hard stop if just opened\n",
        "            if stop_level is None:\n",
        "                if position > 0:\n",
        "                    stop_level = entry_price - guardian.sl_mult * float(atr[t])\n",
        "                else:\n",
        "                    stop_level = entry_price + guardian.sl_mult * float(atr[t])\n",
        "            # Break-even activation\n",
        "            be_trigger = guardian.be_act_mult * float(atr[t])\n",
        "            be_offset = guardian.be_plus_ticks * TICK_SIZE\n",
        "            if not be_activated:\n",
        "                if position > 0 and (float(close[t]) - entry_price) >= be_trigger:\n",
        "                    stop_level = max(stop_level, entry_price + be_offset)\n",
        "                    be_activated = True\n",
        "                elif position < 0 and (entry_price - float(close[t])) >= be_trigger:\n",
        "                    stop_level = min(stop_level, entry_price - be_offset)\n",
        "                    be_activated = True\n",
        "            # Trailing stop\n",
        "            if position > 0:\n",
        "                trail = best_price - guardian.ts_mult * float(atr[t])\n",
        "                stop_level = max(stop_level, trail)\n",
        "            else:\n",
        "                trail = best_price + guardian.ts_mult * float(atr[t])\n",
        "                stop_level = min(stop_level, trail)\n",
        "            # Veto: close if price crosses stop\n",
        "            if (position > 0 and float(close[t]) <= stop_level) or (position < 0 and float(close[t]) >= stop_level):\n",
        "                # Apply PnL for this step (no extra costs; paid on entry)\n",
        "                price_change = float(close[t] - close[t-1])\n",
        "                gross_pnl = pnl_from_price_change(position, price_change, env_like.point_value)\n",
        "                equity += gross_pnl\n",
        "                peak_equity = max(peak_equity, equity)\n",
        "                equity_series.append(equity)\n",
        "                step_returns.append(gross_pnl / max(abs(equity_series[-2]), 1e-9))\n",
        "                # Close position and reset trade state\n",
        "                position = 0\n",
        "                entry_price = 0.0\n",
        "                entry_step = None\n",
        "                best_price = 0.0\n",
        "                be_activated = False\n",
        "                stop_level = None\n",
        "                continue\n",
        "\n",
        "        # 2) PPO acts if Guardian did not veto\n",
        "        if position != 0:\n",
        "            atr_entry_idx = entry_step if entry_step is not None else t\n",
        "            atr_entry = float(max(atr[atr_entry_idx], 1e-9))\n",
        "            pnl_usd = (float(close[t]) - entry_price) * position * env_like.point_value\n",
        "            pnl_norm = pnl_usd / max(atr_entry * env_like.point_value, 1e-9)\n",
        "            bars_in_trade = max(0, t - atr_entry_idx)\n",
        "        else:\n",
        "            pnl_norm = 0.0\n",
        "            bars_in_trade = 0\n",
        "        dur_norm = float(bars_in_trade) / max(env_like.episode_length, 1)\n",
        "        dd_usd = float(max(peak_equity - equity, 0.0))\n",
        "        dd_norm = dd_usd / max(env_like.max_episode_dd, 1e-6)\n",
        "        portfolio_state = np.array([pnl_norm, dur_norm, dd_norm], dtype=np.float32)\n",
        "        state = np.concatenate([obs_raw[t].astype(np.float32), tf_all[t].astype(np.float32), np.array([position], np.float32), portfolio_state])\n",
        "        action, _ = model.predict(state, deterministic=deterministic)\n",
        "        desired_pos = map_action_to_position(int(action))\n",
        "\n",
        "        # 3) Execute decision (apply PnL for previous position)\n",
        "        prev_pos = position\n",
        "        price_change = float(close[t] - close[t-1])\n",
        "        gross_pnl = pnl_from_price_change(prev_pos, price_change, env_like.point_value)\n",
        "        costs = 0.0\n",
        "        is_new_trade_entry = ((prev_pos == 0 and desired_pos != 0) or (np.sign(desired_pos) != np.sign(prev_pos) and desired_pos != 0))\n",
        "        if is_new_trade_entry:\n",
        "            commission = env_like.fixed_commission\n",
        "            slip_points = env_like.atr_slip_mult * float(atr[t])\n",
        "            slippage = slip_points * env_like.point_value\n",
        "            costs = commission + slippage\n",
        "        equity += gross_pnl - costs\n",
        "        peak_equity = max(peak_equity, equity)\n",
        "        equity_series.append(equity)\n",
        "        step_returns.append((gross_pnl - costs) / max(abs(equity_series[-2]), 1e-9))\n",
        "\n",
        "        # 4) Update trade state (entries, exits, reversals)\n",
        "        if (prev_pos == 0 and desired_pos != 0) or (np.sign(desired_pos) != np.sign(prev_pos) and desired_pos != 0):\n",
        "            entry_price = float(close[t])\n",
        "            entry_step = int(t)\n",
        "            best_price = entry_price\n",
        "            be_activated = False\n",
        "            stop_level = None\n",
        "        elif desired_pos == 0 and prev_pos != 0:\n",
        "            entry_price = 0.0\n",
        "            entry_step = None\n",
        "            best_price = 0.0\n",
        "            be_activated = False\n",
        "            stop_level = None\n",
        "        elif desired_pos == prev_pos and desired_pos != 0:\n",
        "            if desired_pos > 0:\n",
        "                best_price = max(best_price, float(close[t]))\n",
        "            else:\n",
        "                best_price = min(best_price, float(close[t]))\n",
        "        position = desired_pos\n",
        "\n",
        "    return np.asarray(equity_series, dtype=np.float64), np.asarray(step_returns, dtype=np.float64)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Izp94ERhcrSt"
      },
      "source": [
        "# 7) Calmar Callback\n",
        "Early stopping by Calmar on validation backtests.\n"
      ],
      "id": "Izp94ERhcrSt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYe3YHXOcrSu"
      },
      "outputs": [],
      "source": [
        "# 7) Calmar Callback — validation selection\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class CalmarCallback(BaseCallback):\n",
        "    def __init__(self, eval_env_ctor, eval_env_kwargs, eval_freq, best_model_save_path, guardian_kwargs=None, verbose=1):\n",
        "        super().__init__(verbose)\n",
        "        self.eval_env_ctor = eval_env_ctor\n",
        "        self.eval_env_kwargs = eval_env_kwargs or {}\n",
        "        self.eval_freq = int(eval_freq)\n",
        "        self.best_model_save_path = best_model_save_path\n",
        "        self.guardian_kwargs = guardian_kwargs or {}\n",
        "        self.best_calmar = -np.inf\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.n_calls % self.eval_freq != 0:\n",
        "            return True\n",
        "        eval_env = self.eval_env_ctor(**self.eval_env_kwargs)\n",
        "        guardian = Guardian(**self.guardian_kwargs)\n",
        "        equity, _ = run_backtest(self.model, eval_env, guardian, deterministic=True)\n",
        "        calmar = calmar_ratio(equity)\n",
        "        if self.verbose:\n",
        "            print(f'[CalmarCallback] step={self.n_calls} calmar={calmar:.4f} best={self.best_calmar:.4f}')\n",
        "        if calmar > self.best_calmar:\n",
        "            self.best_calmar = calmar\n",
        "            self.model.save(self.best_model_save_path)\n",
        "        return True\n"
      ],
      "id": "sYe3YHXOcrSu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqrrzL21crSu"
      },
      "source": [
        "# 8) Training — SB3 PPO\n",
        "Vectorized envs, CalmarCallback, and learn() loop.\n"
      ],
      "id": "lqrrzL21crSu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0edd20df",
      "metadata": {
        "id": "0edd20df"
      },
      "outputs": [],
      "source": [
        "# 8) Training — SB3 PPO\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
        "\n",
        "def make_train_env():\n",
        "    return MNQEmbEnv(\n",
        "        npz_path=TRAIN_NPZ, meta_path=META_JSON, ohlc_csv_path=TRAIN_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        atr_slippage_multiplier=ATR_SLIPPAGE_MULTIPLIER,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, seed=SEED\n",
        "    )\n",
        "\n",
        "def make_val_env():\n",
        "    return MNQEmbEnv(\n",
        "        npz_path=VAL_NPZ, meta_path=META_JSON, ohlc_csv_path=VAL_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        atr_slippage_multiplier=ATR_SLIPPAGE_MULTIPLIER,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, seed=SEED+1\n",
        "    )\n",
        "\n",
        "vec_env = VecMonitor(DummyVecEnv([make_train_env for _ in range(N_ENVS)]), filename=None)\n",
        "policy_kwargs = dict(net_arch=[256, 256])\n",
        "\n",
        "model = PPO('MlpPolicy', vec_env, policy_kwargs=policy_kwargs, tensorboard_log=LOG_DIR, seed=SEED, **PPO_KW)\n",
        "\n",
        "calmar_cb = CalmarCallback(\n",
        "    eval_env_ctor=MNQEmbEnv,\n",
        "    eval_env_kwargs=dict(\n",
        "        npz_path=VAL_NPZ, meta_path=META_JSON, ohlc_csv_path=VAL_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=EPISODE_LENGTH,\n",
        "        point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "        atr_slippage_multiplier=ATR_SLIPPAGE_MULTIPLIER,\n",
        "        drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "        profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "        patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "        max_episode_drawdown_usd=MAX_EPISODE_DRAWDOWN_USD, seed=SEED+123\n",
        "    ),\n",
        "    eval_freq=EVAL_FREQ_STEPS,\n",
        "    best_model_save_path=BEST_MODEL_PATH,\n",
        "    guardian_kwargs=dict(\n",
        "        point_value=POINT_VALUE_MNQ,\n",
        "        sl_atr_mult=GUARDIAN_SL_ATR_MULTIPLIER,\n",
        "        ts_atr_mult=GUARDIAN_TS_ATR_MULTIPLIER,\n",
        "        be_activation_atr_mult=GUARDIAN_BE_ACTIVATION_ATR_MULTIPLIER,\n",
        "        be_plus_ticks=GUARDIAN_BE_PLUS_TICKS\n",
        "    ),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=calmar_cb, progress_bar=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BF0Js9wcrSu"
      },
      "source": [
        "# 9) Final Backtest & Plots\n",
        "Evaluate best model on the test split with Guardian and visualize results.\n"
      ],
      "id": "-BF0Js9wcrSu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f60fdf",
      "metadata": {
        "id": "39f60fdf"
      },
      "outputs": [],
      "source": [
        "# 9) Final Backtest & Plots — test split evaluation\n",
        "from stable_baselines3 import PPO\n",
        "best_model = PPO.load(BEST_MODEL_PATH, device='auto')\n",
        "test_env = MNQEmbEnv(\n",
        "    npz_path=TEST_NPZ, meta_path=META_JSON, ohlc_csv_path=TEST_OHLC_CSV, ohlc_resample_rule=OHLC_RESAMPLE_RULE, episode_length=10**9,\n",
        "    point_value=POINT_VALUE_MNQ, fixed_commission_usd=FIXED_COMMISSION_USD,\n",
        "    atr_slippage_multiplier=ATR_SLIPPAGE_MULTIPLIER,\n",
        "    drawdown_penalty_coeff=DRAWDOWN_PENALTY_COEFF,\n",
        "    profit_keeping_bonus_coeff=PROFIT_KEEPING_BONUS_COEFF,\n",
        "    patience_bonus=PATIENCE_BONUS, choppy_atr_to_price_max=CHOPPY_ATR_TO_PRICE_MAX,\n",
        "    max_episode_drawdown_usd=1e12, seed=SEED+999\n",
        ")\n",
        "guardian = Guardian(point_value=POINT_VALUE_MNQ,\n",
        "                     sl_atr_mult=GUARDIAN_SL_ATR_MULTIPLIER,\n",
        "                     ts_atr_mult=GUARDIAN_TS_ATR_MULTIPLIER,\n",
        "                     be_activation_atr_mult=GUARDIAN_BE_ACTIVATION_ATR_MULTIPLIER,\n",
        "                     be_plus_ticks=GUARDIAN_BE_PLUS_TICKS)\n",
        "equity, step_returns = run_backtest(best_model, test_env, guardian, deterministic=True)\n",
        "\n",
        "calmar = calmar_ratio(equity)\n",
        "sharpe = sharpe_ratio(step_returns)\n",
        "mdd = max_drawdown(equity)\n",
        "cagr = cagr_from_equity(equity)\n",
        "print(f'Test Calmar: {calmar:.3f} | Sharpe: {sharpe:.3f} | MDD: {mdd:.2%} | CAGR: {cagr:.2%}')\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.plot(equity)\n",
        "plt.title('Equity Curve (Test)')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "dd = compute_drawdown(equity)\n",
        "plt.figure(figsize=(12,3))\n",
        "plt.plot(dd)\n",
        "plt.title('Drawdown (Test)')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
