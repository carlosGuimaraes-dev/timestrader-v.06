{"cells": [{"cell_type": "markdown", "id": "bc944697", "metadata": {"id": "bc944697"}, "source": ["### Configuration"]}, {"cell_type": "code", "execution_count": null, "id": "1a50c71d", "metadata": {"id": "1a50c71d"}, "outputs": [], "source": ["CSV_PATH = \"mnq_complete_dataset.csv\"  #@param {type:\"string\"}\n", "# Features to use (OHLCV + 7 TA indicators; all lowercase)\n", "INDICATORS_TO_USE = ['open', 'high', 'low', 'close', 'volume','atr_14', 'adx_14', 'ema_9', 'ema_21', 'vwap', 'rsi_21', 'stochk_14_3_3']  #@param {type:\"raw\"}\n", "# Normalization: 'StandardScaler' or 'MinMaxScaler'\n", "NORMALIZATION_TYPE = \"StandardScaler\"  #@param [\"StandardScaler\", \"MinMaxScaler\"]\n", "# Resample timeframe (e.g., '5T' for 5 minutes, None to disable)\n", "RESAMPLE_RULE = '5T'  #@param {type:\"raw\"}\n", "# Chronological split ratios (must sum to 1.0)\n", "TRAIN_VALID_TEST_SPLIT = [0.7, 0.15, 0.15]  #@param {type:\"raw\"}\n", "# Sequence/window length for model input\n", "SEQ_LEN = 512  #@param {type:\"integer\"}\n", "# Step size between windows (1 = full overlap)\n", "WINDOW_STRIDE = 32  #@param {type:\"integer\"}\n", "# TimesBlock\n", "TOP_K_PERIODS = 3  #@param {type:\"integer\"}  # k in paper\n", "EMBED_DIM = 256     # 2D backbone output channels\n", "EMBED_H = 8\n", "EMBED_W = 8\n", "DROPOUT_RATE = 0.3  #@param {type:\"number\"}\n", "# Training\n", "NUM_EPOCHS = 100  #@param {type:\"integer\"}\n", "BATCH_SIZE = 2048  #@param {type:\"integer\"}\n", "LEARNING_RATE = 1e-3  # scheduler max_lr\n", "WEIGHT_DECAY = 0.005  #@param {type:\"number\"}\n", "PATIENCE = 30  #@param {type:\"integer\"}\n", "GRAD_ACCUM_STEPS = 1  #@param {type:\"integer\"}\n", "# LR Scheduler (OneCycle)\n", "USE_SCHEDULER = True  #@param {type:\"boolean\"}\n", "ONECYCLE_PCT_START = 0.15  #@param {type:\"number\"}\n", "ONECYCLE_DIV_FACTOR = 25.0  #@param {type:\"number\"}\n", "ONECYCLE_FINAL_DIV = 100.0  #@param {type:\"number\"}\n", "# Gradient clipping\n", "CLIP_GRAD_NORM = 1.0  #@param {type:\"number\"}\n", "# Train-only augmentation\n", "AUG_NOISE_STD = 0.02  #@param {type:\"number\"}\n", "# DataLoader performance\n", "DATALOADER_WORKERS = 8  #@param {type:\"integer\"}\n", "PIN_MEMORY = True  #@param {type:\"boolean\"}\n", "PERSISTENT_WORKERS = True  #@param {type:\"boolean\"}\n", "PREFETCH_FACTOR = 4  #@param {type:\"integer\"}\n", "# Precision & memory format (A100)\n", "USE_BF16 = True  #@param {type:\"boolean\"}\n", "CHANNELS_LAST = True  #@param {type:\"boolean\"}\n", "PRINT_GPU_MEM = True  #@param {type:\"boolean\"}\n", "# Checkpointing\n", "CHECKPOINT_PATH = \"/content/drive/MyDrive/timesnet_mnq/checkpoints/best.pt\"  #@param {type:\"string\"}\n", "# Logs e histÃ³rico\n", "LOG_DIR = \"/content/drive/MyDrive/timesnet_mnq/logs\"  #@param {type:\"string\"}\n", "HISTORY_PATH = LOG_DIR + \"/training_history.jsonl\"\n", "# Resume training if checkpoint exists\n", "RESUME_TRAINING = True  #@param {type:\"boolean\"}\n", "# Device\n", "import os\n", "import torch\n", "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "print(f\"Config -> seq_len={SEQ_LEN}, batch_size={BATCH_SIZE}, k={TOP_K_PERIODS}, embed_dim={EMBED_DIM}, HxW={EMBED_H}x{EMBED_W}, dropout={DROPOUT_RATE}\")\n", "print(f\"Optim -> AdamW max_lr={LEARNING_RATE}, wd={WEIGHT_DECAY}, patience={PATIENCE}, grad_accum={GRAD_ACCUM_STEPS}\")\n", "print(f\"Loader -> workers={DATALOADER_WORKERS}, pin_memory={PIN_MEMORY}, persistent={PERSISTENT_WORKERS}, prefetch={PREFETCH_FACTOR}\")\n", "print(f\"Device -> {DEVICE} | bf16={USE_BF16} | channels_last={CHANNELS_LAST}\")\n"]}, {"cell_type": "markdown", "id": "8044ea06", "metadata": {"id": "8044ea06"}, "source": ["### Setup & Imports"]}, {"cell_type": "code", "execution_count": null, "id": "0080a8f2", "metadata": {"id": "0080a8f2"}, "outputs": [], "source": ["import os\n", "import math\n", "import json\n", "import sys, subprocess\n", "import time\n", "import numpy as np\n", "import pandas as pd\n", "from typing import List, Tuple, Optional\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "from torch.utils.data import Dataset, DataLoader\n", "\n", "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n", "from datetime import datetime\n", "\n", "# Ensure pandas-ta is available early\n", "try:\n", "    import pandas_ta as ta\n", "except Exception:\n", "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'pandas-ta'])\n", "    import pandas_ta as ta\n", "\n", "import contextlib\n", "\n", "def _fmt_gb(bytes_val):\n", "    try:\n", "        return f\"{bytes_val/1e9:.2f} GB\"\n", "    except Exception:\n", "        return str(bytes_val)\n", "\n", "if torch.cuda.is_available():\n", "    props = torch.cuda.get_device_properties(0)\n", "    print(f\"GPU -> {torch.cuda.get_device_name(0)} | VRAM total: {_fmt_gb(props.total_memory)}\")\n", "\n", "try:\n", "    torch.backends.cudnn.benchmark = True\n", "    torch.set_float32_matmul_precision('high')\n", "except Exception:\n", "    pass\n"]}, {"cell_type": "markdown", "id": "7205188e", "metadata": {"id": "7205188e"}, "source": ["### Mount Google Drive"]}, {"cell_type": "code", "execution_count": null, "id": "4a3cd4ba", "metadata": {"id": "4a3cd4ba"}, "outputs": [], "source": ["from google.colab import drive\n", "drive.mount('/content/drive', force_remount=True)\n", "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n", "print(f\"Checkpoints dir -> {os.path.dirname(CHECKPOINT_PATH)}\")\n", "print(f\"Checkpoint file -> {CHECKPOINT_PATH}\")\n"]}, {"cell_type": "markdown", "id": "e2a44f48", "metadata": {"id": "e2a44f48"}, "source": ["### Data Loading & Chronological Splits (no leakage)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "119ba241", "metadata": {"id": "119ba241"}, "outputs": [], "source": ["def _find_datetime_column(df: pd.DataFrame) -> Optional[str]:\n", "    candidates = ['datetime', 'date', 'time', 'timestamp', 'ts']\n", "    cols = [c for c in df.columns]\n", "    for c in cols:\n", "        if c.lower() in candidates:\n", "            return c\n", "    return None\n", "def _find_col_ci(df: pd.DataFrame, name: str) -> Optional[str]:\n", "    for c in df.columns:\n", "        if c.lower() == name.lower():\n", "            return c\n", "    return None\n", "def _has_col_ci(df: pd.DataFrame, name: str) -> bool:\n", "    return any(c.lower() == name.lower() for c in df.columns)\n", "def add_ta_indicators(df: pd.DataFrame) -> pd.DataFrame:\n", "    # Normalize base column names to lowercase for consistency\n", "    rename_map = {}\n", "    for nm in ['Open','High','Low','Close','Volume']:\n", "        c = _find_col_ci(df, nm)\n", "        if c is not None:\n", "            rename_map[c] = nm.lower()\n", "    if rename_map:\n", "        df = df.rename(columns=rename_map)\n", "    required = ['high','low','close','volume']\n", "    if any(not _has_col_ci(df, r) for r in required):\n", "        raise ValueError('Missing OHLCV columns (high, low, close, volume) to compute pandas-ta indicators.')\n", "    # Ensure datetime index for VWAP\n", "    time_col = _find_datetime_column(df)\n", "    if time_col is not None:\n", "        try:\n", "            df[time_col] = pd.to_datetime(df[time_col])\n", "            df = df.set_index(time_col)\n", "        except Exception:\n", "            pass # Keep going if datetime conversion fails, but VWAP might fail\n", "    # Compute indicators only if a lowercase target is not already present\n", "    if not _has_col_ci(df, 'atr_14'):\n", "        df.ta.atr(high='high', low='low', close='close', length=14, append=True)\n", "    if not _has_col_ci(df, 'adx_14'):\n", "        df.ta.adx(high='high', low='low', close='close', length=14, append=True)\n", "    if not _has_col_ci(df, 'ema_9'):\n", "        df.ta.ema(close='close', length=9, append=True)\n", "    if not _has_col_ci(df, 'ema_21'):\n", "        df.ta.ema(close='close', length=21, append=True)\n", "    if not _has_col_ci(df, 'vwap'):\n", "        df.ta.vwap(high='high', low='low', close='close', volume='volume', append=True)\n", "    if not _has_col_ci(df, 'rsi_21'):\n", "        df.ta.rsi(close='close', length=21, append=True)\n", "    if not _has_col_ci(df, 'stochk_14_3_3'):\n", "        df.ta.stoch(high='high', low='low', close='close', k=14, d=3, smooth_k=3, append=True)\n", "    # Rename known pandas-ta outputs to lowercase canonical names if needed\n", "    lower_map = {}\n", "    # ATR - pandas-ta outputs ATRr_14\n", "    if 'ATRr_14' in df.columns and 'atr_14' not in df.columns:\n", "        lower_map['ATRr_14'] = 'atr_14'\n", "    # ADX\n", "    if 'ADX_14' in df.columns and 'adx_14' not in df.columns:\n", "        lower_map['ADX_14'] = 'adx_14'\n", "    # EMAs\n", "    if 'EMA_9' in df.columns and 'ema_9' not in df.columns:\n", "        lower_map['EMA_9'] = 'ema_9'\n", "    if 'EMA_21' in df.columns and 'ema_21' not in df.columns:\n", "        lower_map['EMA_21'] = 'ema_21'\n", "    # VWAP\n", "    if 'VWAP' in df.columns and 'vwap' not in df.columns:\n", "        lower_map['VWAP'] = 'vwap'\n", "    if 'VWAP_D' in df.columns and 'vwap' not in df.columns: # Added to handle VWAP_D\n", "        lower_map['VWAP_D'] = 'vwap'\n", "    # RSI\n", "    if 'RSI_21' in df.columns and 'rsi_21' not in df.columns:\n", "        lower_map['RSI_21'] = 'rsi_21'\n", "    # STOCH K/D (we'll expose K by default)\n", "    if 'STOCHk_14_3_3' in df.columns and 'stochk_14_3_3' not in df.columns:\n", "        lower_map['STOCHk_14_3_3'] = 'stochk_14_3_3'\n", "    if lower_map:\n", "        df = df.rename(columns=lower_map)\n", "    return df\n", "def load_mnq_csv(csv_path: str, indicators: List[str], resample_rule: Optional[str] = None) -> pd.DataFrame:\n", "    df = pd.read_csv(csv_path)\n", "    time_col = _find_datetime_column(df)\n", "    if time_col is not None:\n", "        try:\n", "            df[time_col] = pd.to_datetime(df[time_col])\n", "            df = df.sort_values(by=time_col, ascending=True).reset_index(drop=True)\n", "        except Exception:\n", "            df = df.reset_index(drop=True)\n", "    else:\n", "        df = df.reset_index(drop=True)\n", "    # Normalize base OHLCV column names before optional resample\n", "    rename_map = {}\n", "    for nm in ['Open', 'High', 'Low', 'Close', 'Volume']:\n", "        c = _find_col_ci(df, nm)\n", "        if c is not None:\n", "            rename_map[c] = nm.lower()\n", "    if rename_map:\n", "        df = df.rename(columns=rename_map)\n", "    if resample_rule and time_col is not None:\n", "        df = df.set_index(time_col)\n", "        agg_base = {\n", "            'open': 'first',\n", "            'high': 'max',\n", "            'low': 'min',\n", "            'close': 'last',\n", "            'volume': 'sum'\n", "        }\n", "        agg = {col: agg_base.get(col, 'last') for col in df.columns}\n", "        df = df.resample(resample_rule, label='right', closed='right').agg(agg)\n", "        df = df.dropna(how='any').reset_index()\n", "        # Update time_col in case resample changed its dtype/name\n", "        time_col = _find_datetime_column(df)\n", "    # Create TA indicators and ensure lowercase canon names\n", "    df = add_ta_indicators(df)\n", "    # Select requested features (lowercase)\n", "    missing = [c for c in indicators if c not in df.columns]\n", "    if missing:\n", "        raise ValueError(f\"Missing required columns after TA creation: {missing}. Available: {list(df.columns)}\")\n", "    xdf = df[indicators].copy()\n", "    # Clean: numeric, remove inf, drop NaNs to avoid leakage via backward fill\n", "    for c in xdf.columns:\n", "        xdf[c] = pd.to_numeric(xdf[c], errors='coerce')\n", "    xdf = xdf.replace([np.inf, -np.inf], np.nan)\n", "    xdf = xdf.dropna()\n", "    return xdf\n", "def chronological_split(arr: np.ndarray, ratios: List[float]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n", "    assert abs(sum(ratios) - 1.0) < 1e-6, \"TRAIN_VALID_TEST_SPLIT must sum to 1.0\"\n", "    T = len(arr)\n", "    n_train = int(T * ratios[0])\n", "    n_val = int(T * ratios[1])\n", "    n_test = T - n_train - n_val\n", "    train = arr[:n_train]\n", "    val = arr[n_train:n_train+n_val]\n", "    test = arr[n_train+n_val:]\n", "    return train, val, test\n", "def make_scaler(norm_type: str):\n", "    if norm_type == \"StandardScaler\":\n", "        return StandardScaler()\n", "    elif norm_type == \"MinMaxScaler\":\n", "        return MinMaxScaler()\n", "    else:\n", "        raise ValueError(\"NORMALIZATION_TYPE must be 'StandardScaler' or 'MinMaxScaler'.\")\n", "def fit_transform_splits(train_arr: np.ndarray, val_arr: np.ndarray, test_arr: np.ndarray, scaler):\n", "    # Fit only on training data\n", "    scaler.fit(train_arr)\n", "    train_scaled = scaler.transform(train_arr)\n", "    val_scaled = scaler.transform(val_arr)\n", "    test_scaled = scaler.transform(test_arr)\n", "    return train_scaled, val_scaled, test_scaled\n", "# Load CSV\n", "print(f'Loading CSV: {CSV_PATH}')\n", "df = load_mnq_csv(CSV_PATH, INDICATORS_TO_USE, RESAMPLE_RULE)\n", "print(f'Features selected: {len(INDICATORS_TO_USE)} | {INDICATORS_TO_USE}')\n", "data = df.values.astype(np.float32)\n", "print(f'Total rows after TA + cleanup: {len(df)} | Feature dim: {data.shape[1]}')\n", "# Chronological split (no shuffling)\n", "train_raw, val_raw, test_raw = chronological_split(data, TRAIN_VALID_TEST_SPLIT)\n", "print(f'Split -> train: {train_raw.shape}, val: {val_raw.shape}, test: {test_raw.shape}')\n", "# Train-only normalization\n", "scaler = make_scaler(NORMALIZATION_TYPE)\n", "train_scaled, val_scaled, test_scaled = fit_transform_splits(train_raw, val_raw, test_raw, scaler)\n", "if NORMALIZATION_TYPE == 'StandardScaler':\n", "    means = scaler.mean_\n", "    stds = scaler.scale_ if hasattr(scaler, 'scale_') else np.sqrt(scaler.var_)\n", "    print(f'Scaler(Standard) -> mean range [{means.min():.4f}, {means.max():.4f}] | std range [{stds.min():.4f}, {stds.max():.4f}]')\n", "else:\n", "    mins = scaler.data_min_\n", "    maxs = scaler.data_max_\n", "    print(f'Scaler(MinMax) -> min range [{mins.min():.4f}, {mins.max():.4f}] | max range [{maxs.min():.4f}, {maxs.max():.4f}]')\n", "def _count_windows(n, L, s):\n", "    return max(0, (n - L) // s + 1)\n", "nw_train = _count_windows(len(train_scaled), SEQ_LEN, WINDOW_STRIDE)\n", "nw_val = _count_windows(len(val_scaled), SEQ_LEN, WINDOW_STRIDE)\n", "nw_test = _count_windows(len(test_scaled), SEQ_LEN, WINDOW_STRIDE)\n", "print(f'Windows -> train: {nw_train}, val: {nw_val}, test: {nw_test} | stride={WINDOW_STRIDE}, seq_len={SEQ_LEN}')\n", "train_scaled.shape, val_scaled.shape, test_scaled.shape\n"]}, {"cell_type": "markdown", "id": "fc1bfd75", "metadata": {"id": "fc1bfd75"}, "source": ["### Dataset & DataLoaders (windowed, no shuffle)\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "432e2afc", "metadata": {"id": "432e2afc"}, "outputs": [], "source": ["class MNQ_Dataset(Dataset):\n", "    def __init__(self, arr_2d: np.ndarray, seq_len: int, stride: int = 1):\n", "        super().__init__()\n", "        self.x = arr_2d\n", "        self.seq_len = int(seq_len)\n", "        self.stride = int(stride)\n", "        self.T = len(arr_2d)\n", "        self.C = arr_2d.shape[1]\n", "        if self.T < self.seq_len:\n", "            raise ValueError(f\"Not enough timesteps ({self.T}) for seq_len={self.seq_len}\")\n", "        # Number of windows using stride\n", "        self.idxs = list(range(0, self.T - self.seq_len + 1, self.stride))\n", "\n", "    def __len__(self):\n", "        return len(self.idxs)\n", "\n", "    def __getitem__(self, idx):\n", "        start = self.idxs[idx]\n", "        end = start + self.seq_len\n", "        window = self.x[start:end]  # shape [seq_len, C]\n", "        # model expects [L, C], training target == input window (self-supervised)\n", "        return torch.from_numpy(window).float()\n", "\n", "def make_loader(arr: np.ndarray, seq_len: int, stride: int, batch_size: int) -> DataLoader:\n", "    ds = MNQ_Dataset(arr, seq_len=seq_len, stride=stride)\n", "    # No shuffling to avoid any perceived leakage; data is chronologically windowed already\n", "\n", "    # Ajuste automÃ¡tico de batch para evitar Ã©poca vazia\n", "    bs = int(batch_size)\n", "    nwin = len(ds)\n", "    if nwin < bs:\n", "        print(f'WARN: batch_size {bs} > windows {nwin}; ajustando para evitar Ã©poca vazia.')\n", "        bs = max(1, nwin)\n", "\n", "    return DataLoader(ds, batch_size=bs, shuffle=False, drop_last=False, num_workers=DATALOADER_WORKERS, pin_memory=PIN_MEMORY, persistent_workers=(PERSISTENT_WORKERS and DATALOADER_WORKERS>0), prefetch_factor=(PREFETCH_FACTOR if DATALOADER_WORKERS>0 else 2))\n", "\n", "\n", "train_loader = make_loader(train_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n", "val_loader = make_loader(val_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n", "test_loader = make_loader(test_scaled, SEQ_LEN, WINDOW_STRIDE, BATCH_SIZE)\n", "\n", "tw, vw, tew = len(train_loader.dataset), len(val_loader.dataset), len(test_loader.dataset)\n", "tb, vb, teb = len(train_loader), len(val_loader), len(test_loader)\n", "print(f'DataLoaders -> windows (train/val/test): {tw}/{vw}/{tew} | batches: {tb}/{vb}/{teb} | batch_size={BATCH_SIZE}')\n", "print(f'Window shape -> L={SEQ_LEN}, C={train_scaled.shape[1]}')\n"]}, {"cell_type": "markdown", "id": "c29d00a1", "metadata": {"id": "c29d00a1"}, "source": ["### Model: TimesBlock + Inception + Feature Extractor + Decoder\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "q_0Oj0KHtrEF", "metadata": {"id": "q_0Oj0KHtrEF"}, "outputs": [], "source": ["class InceptionBlock2D(nn.Module):\n", "    def __init__(self, in_channels: int, out_channels: int, dropout: float = 0.1):\n", "        super().__init__()\n", "        # Split out_channels across branches\n", "        base, rem = divmod(out_channels, 4)\n", "        branch_sizes = [base + (1 if i < rem else 0) for i in range(4)]\n", "        b1, b2, b3, b4 = branch_sizes\n", "\n", "        self.branch1x1 = nn.Sequential(\n", "            nn.Conv2d(in_channels, b1, kernel_size=1, bias=False),\n", "            nn.BatchNorm2d(b1),\n", "            nn.ReLU(inplace=True),\n", "        )\n", "\n", "        red3 = max(in_channels // 2, 8)\n", "        self.branch3x3 = nn.Sequential(\n", "            nn.Conv2d(in_channels, red3, kernel_size=1, bias=False),\n", "            nn.BatchNorm2d(red3),\n", "            nn.ReLU(inplace=True),\n", "            nn.Conv2d(red3, b2, kernel_size=3, padding=1, bias=False),\n", "            nn.BatchNorm2d(b2),\n", "            nn.ReLU(inplace=True),\n", "        )\n", "\n", "        red5 = max(in_channels // 2, 8)\n", "        self.branch5x5 = nn.Sequential(\n", "            nn.Conv2d(in_channels, red5, kernel_size=1, bias=False),\n", "            nn.BatchNorm2d(red5),\n", "            nn.ReLU(inplace=True),\n", "            nn.Conv2d(red5, b3, kernel_size=5, padding=2, bias=False),\n", "            nn.BatchNorm2d(b3),\n", "            nn.ReLU(inplace=True),\n", "        )\n", "\n", "        self.branch_pool = nn.Sequential(\n", "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n", "            nn.Conv2d(in_channels, b4, kernel_size=1, bias=False),\n", "            nn.BatchNorm2d(b4),\n", "            nn.ReLU(inplace=True),\n", "        )\n", "\n", "        self.dropout = nn.Dropout(p=dropout)\n", "\n", "    def forward(self, x):\n", "        o1 = self.branch1x1(x)\n", "        o2 = self.branch3x3(x)\n", "        o3 = self.branch5x5(x)\n", "        o4 = self.branch_pool(x)\n", "        out = torch.cat([o1, o2, o3, o4], dim=1)\n", "        return self.dropout(out)\n", "\n", "class TimesBlock(nn.Module):\n", "    \"\"\"\n", "    FFT-based period discovery + 1D->2D folding + 2D Inception backbone + weighted fusion.\n", "    Input:  x [B, L, C]\n", "    Output: embedding [B, E, H, W]\n", "    \"\"\"\n", "    def __init__(self, in_channels: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float = 0.1):\n", "        super().__init__()\n", "        self.in_channels = in_channels\n", "        self.k = k\n", "        self.embed_dim = embed_dim\n", "        self.embed_h = embed_h\n", "        self.embed_w = embed_w\n", "        self.backbone = InceptionBlock2D(in_channels, embed_dim, dropout=dropout)\n", "\n", "    @torch.no_grad()\n", "    def _find_topk_periods(self, x_bc_l: torch.Tensor, k: int) -> Tuple[List[int], torch.Tensor]:\n", "        # x_bc_l: [B, C, L]\n", "        B, C, L = x_bc_l.shape\n", "        xf = torch.fft.rfft(x_bc_l, dim=-1)  # [B, C, L//2 + 1]\n", "        amp = xf.abs().mean(dim=(0, 1))      # [L//2 + 1], averaged over batch & channels\n", "        if amp.shape[0] <= 1:\n", "            return [L], torch.tensor([1.0], device=x_bc_l.device)\n", "\n", "        amp[0] = 0.0  # ignore DC\n", "        k_eff = min(k, amp.shape[0]-1)\n", "        vals, idxs = torch.topk(amp, k=k_eff, largest=True, sorted=True)\n", "        periods = []\n", "        for idx in idxs.tolist():\n", "            p = int(round(L / max(idx, 1)))\n", "            p = max(p, 2)\n", "            periods.append(p)\n", "        # Softmax weights from amplitudes\n", "        w = torch.softmax(vals, dim=0)\n", "        return periods, w\n", "\n", "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n", "        # x: [B, L, C]\n", "        B, L, C = x.shape\n", "        x_bc_l = x.permute(0, 2, 1).contiguous()  # [B, C, L]\n", "\n", "        periods, weights = self._find_topk_periods(x_bc_l, self.k)\n", "        feats = None\n", "        for i, p in enumerate(periods):\n", "            pad_len = (p - (L % p)) % p\n", "            x_pad = F.pad(x_bc_l, (0, pad_len), mode='constant', value=0.0)  # [B,C,Lp]\n", "            Lp = x_pad.shape[-1]\n", "            w_ = Lp // p\n", "            # Fold: [B, C, p, w_]\n", "            x_2d = x_pad.view(B, C, w_, p).transpose(2, 3).contiguous()\n", "\n", "            z = self.backbone(x_2d)  # [B, E, h, w]\n", "            z = F.adaptive_avg_pool2d(z, (self.embed_h, self.embed_w))  # [B,E,H,W]\n", "            z = z * weights[i].view(1, 1, 1, 1)  # weight this period\n", "\n", "            feats = z if feats is None else (feats + z)\n", "\n", "        return feats  # [B, E, H, W]\n", "\n", "class TimesNetFeatureExtractor(nn.Module):\n", "    def __init__(self, in_channels: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float=0.1):\n", "        super().__init__()\n", "        self.block = TimesBlock(in_channels, k, embed_dim, embed_h, embed_w, dropout=dropout)\n", "        self.dropout = nn.Dropout(p=dropout)\n", "\n", "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n", "        # x: [B, L, C]\n", "        emb = self.block(x)\n", "        return self.dropout(emb)  # [B, E, H, W]\n", "\n", "class ModelWithDecoder(nn.Module):\n", "    \"\"\"\n", "    Wrapper for self-supervised training (reconstruction).\n", "    During inference, use extractor only.\n", "    \"\"\"\n", "    def __init__(self, in_channels: int, seq_len: int, k: int, embed_dim: int, embed_h: int, embed_w: int, dropout: float=0.1):\n", "        super().__init__()\n", "        self.in_channels = in_channels\n", "        self.seq_len = seq_len\n", "\n", "        self.extractor = TimesNetFeatureExtractor(\n", "            in_channels=in_channels,\n", "            k=k,\n", "            embed_dim=embed_dim,\n", "            embed_h=embed_h,\n", "            embed_w=embed_w,\n", "            dropout=dropout\n", "        )\n", "        flat_size = embed_dim * embed_h * embed_w\n", "        hidden = max(512, flat_size // 2)\n", "\n", "        # Using Sequential with explicit layers instead of Flatten\n", "        self.decoder = nn.Sequential(\n", "            nn.Linear(flat_size, hidden),\n", "            nn.ReLU(inplace=True),\n", "            nn.Dropout(p=dropout),\n", "            nn.Linear(hidden, seq_len * in_channels)\n", "        )\n", "\n", "    def forward(self, x: torch.Tensor):\n", "        # x: [B, L, C]\n", "        emb = self.extractor(x)  # [B, E, H, W]\n", "        # Explicitly permute to channels_first before flattening with view\n", "        if emb.dim() == 4:\n", "             # Assuming channels_last if not channels_first or original channels\n", "             emb = emb.permute(0, 1, 2, 3).contiguous() # Ensure channels_first (no-op if already)\n", "\n", "        # Debug: print(f\"Shape before flattening: {emb.shape}\")\n", "        emb_flat = emb.view(emb.size(0), -1) # Explicit flatten\n", "        rec = self.decoder(emb_flat).view(x.shape[0], self.seq_len, self.in_channels)\n", "        return rec, emb\n", "\n", "    def strip_decoder(self):\n", "        self.decoder = None"]}, {"cell_type": "markdown", "id": "595ae284", "metadata": {"id": "595ae284"}, "source": ["### Training Utilities: EarlyStopping, Checkpointing, Train/Eval"]}, {"cell_type": "code", "execution_count": null, "id": "d40cbc82", "metadata": {"id": "d40cbc82"}, "outputs": [], "source": ["from contextlib import nullcontext\n", "import math\n", "USE_AMP = bool(torch.cuda.is_available() and (DEVICE == 'cuda') and USE_BF16 and getattr(torch.cuda, 'is_bf16_supported', lambda: False)())\n", "AMP_DTYPE = torch.bfloat16 if USE_AMP else None\n", "\n", "class EarlyStopping:\n", "    def __init__(self, patience: int = 7, min_delta: float = 0.0):\n", "        self.patience = patience\n", "        self.min_delta = min_delta\n", "        self.best = None\n", "        self.num_bad = 0\n", "\n", "    def step(self, value: float) -> bool:\n", "        if self.best is None or value < self.best - self.min_delta:\n", "            self.best = value\n", "            self.num_bad = 0\n", "            return False  # no stop\n", "        else:\n", "            self.num_bad += 1\n", "            return self.num_bad >= self.patience\n", "\n", "def save_checkpoint(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, epoch: int, best_val: float, extra: dict=None):\n", "    state = {\n", "        \"epoch\": epoch,\n", "        \"model_state_dict\": model.state_dict(),\n", "        \"optimizer_state_dict\": optimizer.state_dict(),\n", "        \"best_val_loss\": best_val,\n", "    }\n", "    if extra:\n", "        state[\"extra\"] = extra\n", "    torch.save(state, path)\n", "\n", "def load_checkpoint_if_any(path: str, model: nn.Module, optimizer: torch.optim.Optimizer, resume: bool):\n", "    start_epoch = 1\n", "    best_val = float(\"inf\")\n", "    if resume and os.path.exists(path):\n", "        ckpt = torch.load(path, map_location=\"cpu\")\n", "        model.load_state_dict(ckpt[\"model_state_dict\"])\n", "        optimizer.load_state_dict(ckpt[\"optimizer_state_dict\"])\n", "        start_epoch = ckpt.get(\"epoch\", 1) + 1\n", "        best_val = ckpt.get(\"best_val_loss\", float(\"inf\"))\n", "        if (not math.isfinite(best_val)) or (best_val <= 0):\n", "            print(\"WARN: invalid best_val in checkpoint; ignoring it\")\n", "            best_val = float(\"inf\")\n", "        print(f\"Resuming from epoch {start_epoch-1} with best_val={best_val:.6f}\")\n", "    return start_epoch, best_val\n", "\n", "def train_one_epoch(model, loader, optimizer, device, scheduler=None, clip_grad_norm: float=None, aug_noise_std: float=0.0):\n", "    model.train()\n", "    mse = nn.MSELoss()\n", "    total_loss = 0.0\n", "    n = 0\n", "    optimizer.zero_grad(set_to_none=True)\n", "    for i, xb in enumerate(loader):\n", "        xb = xb.to(device, non_blocking=True)\n", "        if aug_noise_std and aug_noise_std > 0:\n", "            noise = torch.randn_like(xb) * float(aug_noise_std)\n", "            xb = xb + noise\n", "        ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if USE_AMP else nullcontext()\n", "        with ctx:\n", "            rec, emb = model(xb)\n", "            loss = mse(rec, xb) / max(int(GRAD_ACCUM_STEPS), 1)\n", "        loss.backward()\n", "        if (i + 1) % max(int(GRAD_ACCUM_STEPS), 1) == 0:\n", "            if clip_grad_norm is not None:\n", "                torch.nn.utils.clip_grad_norm_(model.parameters(), float(clip_grad_norm))\n", "            optimizer.step()\n", "            if scheduler is not None:\n", "                scheduler.step()\n", "            optimizer.zero_grad(set_to_none=True)\n", "        # Accumulate reporting with true scale\n", "        total_loss += (loss.item() * max(int(GRAD_ACCUM_STEPS), 1)) * xb.size(0)\n", "        n += xb.size(0)\n", "    return total_loss / max(n, 1)\n", "\n", "@torch.no_grad()\n", "def evaluate(model, loader, device, aug_noise_std: float=0.0):\n", "    model.eval()\n", "    mse = nn.MSELoss()\n", "    total_loss = 0.0\n", "    n = 0\n", "    for xb in loader:\n", "        xb = xb.to(device, non_blocking=True)\n", "        # Augmentation should only be applied during training, remove this line\n", "        # if aug_noise_std and aug_noise_std > 0:\n", "        #     noise = torch.randn_like(xb) * float(aug_noise_std)\n", "        #     xb = xb + noise\n", "        ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if USE_AMP else nullcontext()\n", "        with ctx:\n", "            rec, emb = model(xb)\n", "            loss = mse(rec, xb)\n", "        total_loss += loss.item() * xb.size(0)\n", "        n += xb.size(0)\n", "    return total_loss / max(n, 1)"]}, {"cell_type": "markdown", "id": "51358a74", "metadata": {"id": "51358a74"}, "source": ["### Initialize Model, Optimizer, and Train"]}, {"cell_type": "code", "execution_count": null, "id": "4375a4f0", "metadata": {"id": "4375a4f0"}, "outputs": [], "source": ["in_channels = train_scaled.shape[1]\n", "print(f'In-channels (features): {in_channels}')\n", "model = ModelWithDecoder(\n", "    in_channels=in_channels,\n", "    seq_len=SEQ_LEN,\n", "    k=TOP_K_PERIODS,\n", "    embed_dim=EMBED_DIM,\n", "    embed_h=EMBED_H,\n", "    embed_w=EMBED_W,\n", "    dropout=DROPOUT_RATE\n", ").to(DEVICE)\n", "if CHANNELS_LAST and (DEVICE == 'cuda'):\n", "    model = model.to(memory_format=torch.channels_last)\n", "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "print(f'Model params: {params/1e6:.2f}M')\n", "\n", "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n", "\n", "start_epoch, best_val = load_checkpoint_if_any(CHECKPOINT_PATH, model, optimizer, resume=RESUME_TRAINING)\n", "\n", "# Ensure the optimizer's initial LR is set before creating the scheduler when resuming\n", "if RESUME_TRAINING and os.path.exists(CHECKPOINT_PATH):\n", "    for param_group in optimizer.param_groups:\n", "        param_group['lr'] = LEARNING_RATE\n", "\n", "scheduler = None\n", "if USE_SCHEDULER:\n", "    steps_per_epoch = max(1, (len(train_loader) + max(int(GRAD_ACCUM_STEPS),1) - 1) // max(int(GRAD_ACCUM_STEPS),1))\n", "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n", "        optimizer, max_lr=LEARNING_RATE, steps_per_epoch=steps_per_epoch, epochs=NUM_EPOCHS,\n", "        pct_start=ONECYCLE_PCT_START, div_factor=ONECYCLE_DIV_FACTOR, final_div_factor=ONECYCLE_FINAL_DIV,\n", "        anneal_strategy='cos'\n", "    )\n", "\n", "\n", "os.makedirs(os.path.dirname(CHECKPOINT_PATH), exist_ok=True)\n", "os.makedirs(LOG_DIR, exist_ok=True)\n", "training_history = []\n", "early = EarlyStopping(patience=PATIENCE, min_delta=0.0)\n", "# If best_val is invalid (e.g., 0.0, inf, nan), compute a baseline on val set\n", "if (not math.isfinite(best_val)) or (best_val <= 0):\n", "    try:\n", "        best_val = evaluate(model, val_loader, DEVICE, aug_noise_std=0.0)\n", "        print(f\"Baseline val before training: {best_val:.6f}\")\n", "    except Exception as e:\n", "        print(\"WARN: failed to compute baseline val:\", e)\n", "\n", "tb, vb = len(train_loader), len(val_loader)\n", "tw, vw = len(train_loader.dataset), len(val_loader.dataset)\n", "est_opt_steps = (tb + max(int(GRAD_ACCUM_STEPS),1) - 1) // max(int(GRAD_ACCUM_STEPS),1)\n", "print(f\"Starting training at epoch {start_epoch} on {DEVICE} | train windows={tw}, batches/epoch={tb}, grad_accum={GRAD_ACCUM_STEPS}, est_opt_steps/epoch={est_opt_steps}\")\n", "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n", "    if DEVICE == 'cuda':\n", "        torch.cuda.reset_peak_memory_stats()\n", "    ep_start = time.perf_counter()\n", "    train_loss = train_one_epoch(model, train_loader, optimizer, DEVICE, scheduler=scheduler, clip_grad_norm=CLIP_GRAD_NORM, aug_noise_std=AUG_NOISE_STD)\n", "    val_loss = evaluate(model, val_loader, DEVICE, aug_noise_std=0.0) # Pass 0.0 for augmentation noise during evaluation\n", "    ep_time = time.perf_counter() - ep_start\n", "    throughput = len(train_loader.dataset) / max(ep_time, 1e-9)\n", "\n", "    improved = val_loss < best_val - 1e-12\n", "    if improved:\n", "        best_val = val_loss\n", "        save_checkpoint(\n", "            CHECKPOINT_PATH, model, optimizer, epoch, best_val,\n", "            extra={\n", "                \"INDICATORS_TO_USE\": INDICATORS_TO_USE,\n", "                \"NORMALIZATION_TYPE\": NORMALIZATION_TYPE,\n", "                \"SEQ_LEN\": SEQ_LEN,\n", "                \"TOP_K_PERIODS\": TOP_K_PERIODS,\n", "                \"EMBED_DIM\": EMBED_DIM,\n", "                \"EMBED_H\": EMBED_H,\n", "                \"EMBED_W\": EMBED_W,\n", "            }\n", "        )\n", "\n", "    stop = early.step(val_loss)\n", "    lr = optimizer.param_groups[0].get('lr', None)\n", "    msg = f\"Epoch {epoch:03d} | train {train_loss:.6f} | val {val_loss:.6f} | {throughput:.0f} samp/s\"\n", "    if lr is not None:\n", "        msg += f\" | lr={lr:.2e}\"\n", "    if improved:\n", "        msg += \" | [saved]\"\n", "    if PRINT_GPU_MEM and (DEVICE == 'cuda'):\n", "        peak = torch.cuda.max_memory_allocated() / 1e9\n", "        msg += f\" | GPU peak mem: {peak:.2f} GB\"\n", "    print(msg + f\" | patience {early.num_bad}/{PATIENCE}\")\n", "    # Save 'last' checkpoint every epoch for robust resume\n", "    try:\n", "        _last_path = CHECKPOINT_PATH.replace(\"best.pt\", \"last.pt\")\n", "        save_checkpoint(_last_path, model, optimizer, epoch, best_val)\n", "    except Exception as e:\n", "        print(\"WARN: failed to save last checkpoint:\", e)\n", "    # Log epoch metrics\n", "    try:\n", "        import json as _json\n", "        rec = {\n", "            'epoch': int(epoch),\n", "            'train_loss': float(train_loss),\n", "            'val_loss': float(val_loss),\n", "            'lr': float(lr) if lr is not None else None,\n", "            'throughput': float(throughput),\n", "            'epoch_time': float(ep_time),\n", "            'gpu_peak_gb': float(peak) if (PRINT_GPU_MEM and (DEVICE=='cuda')) else None,\n", "            'improved': bool(improved),\n", "            'best_val': float(best_val)\n", "        }\n", "        training_history.append(rec)\n", "        with open(HISTORY_PATH, 'a') as f:\n", "            f.write(_json.dumps(rec) + '\\n')\n", "    except Exception as e:\n", "        print('WARN: falha ao gravar histÃ³rico:', e)\n", "    if stop:\n", "        print(f\"Early stopping at epoch {epoch}. Best val: {best_val:.6f}\")\n", "        break"]}, {"cell_type": "markdown", "id": "94b9b7db", "metadata": {"id": "94b9b7db"}, "source": ["### Verification: Embedding-only forward"]}, {"cell_type": "code", "execution_count": null, "id": "167d28ad", "metadata": {"id": "167d28ad"}, "outputs": [], "source": ["\n", "# Load best checkpoint\n", "ckpt = torch.load(CHECKPOINT_PATH, map_location=\"cpu\")\n", "model = ModelWithDecoder(\n", "    in_channels=in_channels,\n", "    seq_len=SEQ_LEN,\n", "    k=TOP_K_PERIODS,\n", "    embed_dim=EMBED_DIM,\n", "    embed_h=EMBED_H,\n", "    embed_w=EMBED_W,\n", "    dropout=DROPOUT_RATE\n", ")\n", "model.load_state_dict(ckpt[\"model_state_dict\"], strict=True)\n", "model = model.to(DEVICE)\n", "model.eval()\n", "\n", "# Remove temporary reconstruction decoder\n", "model.strip_decoder()\n", "assert model.decoder is None\n", "\n", "# Take a sample batch from test set and compute embeddings\n", "xb = next(iter(test_loader))\n", "xb = xb.to(DEVICE, non_blocking=True)\n", "with torch.no_grad():\n", "    ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n", "    with ctx:\n", "        # Use the extractor directly for inference-only embeddings\n", "        emb = model.extractor(xb)  # [B, E, H, W]\n", "\n", "print(\"Embedding tensor shape:\", tuple(emb.shape))\n"]}, {"cell_type": "markdown", "id": "4209340c", "metadata": {"id": "4209340c"}, "source": ["### Probing: ConfiguraÃ§Ã£o"]}, {"cell_type": "code", "execution_count": null, "id": "8937a8db", "metadata": {"id": "8937a8db"}, "outputs": [], "source": ["\n", "# === Probing configuration ===\n", "PROBE_HORIZON = 1  # steps ahead for label (adjust as needed)\n", "PROBE_RETURN_COL = 'close'  # which column to compute returns from\n", "PROBE_POOLING = 'flatten'  # 'avg' over HxW or 'flatten'\n", "PROBE_REG_C = 0.5\n", "PROBE_MAX_ITER = 1000\n", "PROBE_RANDOM_STATE = 42\n", "PROBE_PCA_COMPONENTS = 256\n", "\n", "# Regime detection windows (based on returns of PROBE_RETURN_COL)\n", "REG_TREND_WIN = 128  # steps for trend proxy\n", "REG_VOL_WIN = 128    # steps for realized volatility\n"]}, {"cell_type": "markdown", "id": "f50a8fb7", "metadata": {"id": "f50a8fb7"}, "source": ["### Probing: FunÃ§Ãµes UtilitÃ¡rias\n"]}, {"cell_type": "code", "execution_count": null, "id": "1183568a", "metadata": {"id": "1183568a"}, "outputs": [], "source": ["\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.decomposition import PCA\n", "from sklearn.metrics import roc_auc_score, average_precision_score\n", "from scipy.stats import spearmanr\n", "\n", "\n", "def compute_forward_returns(raw_split: np.ndarray, close_idx: int, horizon: int) -> np.ndarray:\n", "    close = raw_split[:, close_idx].astype(np.float64)\n", "    # log returns for stability\n", "    lr = np.zeros_like(close)\n", "    if horizon >= 1:\n", "        lr[:-horizon] = np.log(close[horizon:] / close[:-horizon])\n", "        lr[-horizon:] = np.nan\n", "    return lr\n", "\n", "\n", "def ds_valid_range(ds, horizon: int, total_len: int):\n", "    # windows whose end + horizon is within array\n", "    val_mask = []\n", "    for s in ds.idxs:\n", "        e = s + ds.seq_len\n", "        val_mask.append(e + horizon <= total_len)\n", "    return np.array(val_mask, dtype=bool)\n", "\n", "\n", "def window_labels_for_ds(ds, raw_split: np.ndarray, close_idx: int, horizon: int):\n", "    fwd_ret = compute_forward_returns(raw_split, close_idx, horizon)\n", "    mask = ds_valid_range(ds, horizon, len(raw_split))\n", "    labels = []\n", "    rets = []\n", "    for ok, s in zip(mask, ds.idxs):\n", "        if not ok:\n", "            break\n", "        e = s + ds.seq_len\n", "        r = fwd_ret[e-1]  # return immediately after the window end\n", "        labels.append(1 if r > 0 else 0)\n", "        rets.append(r)\n", "    return np.array(labels, dtype=np.int64), np.array(rets, dtype=np.float64), mask.sum()\n", "\n", "\n", "def extract_embeddings(loader, model, device, pooling: str = 'avg'):\n", "    model.eval()\n", "    vecs = []\n", "    with torch.no_grad():\n", "        ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n", "        for xb in loader:\n", "            xb = xb.to(device, non_blocking=True)\n", "            with ctx:\n", "                emb = model.extractor(xb)  # [B,E,H,W]\n", "            if pooling == 'avg':\n", "                v = emb.mean(dim=(2,3))  # [B,E]\n", "            else:\n", "                v = emb.view(emb.size(0), -1)\n", "            vecs.append(v.detach().to('cpu').numpy())\n", "    return np.concatenate(vecs, axis=0)\n", "\n", "\n", "def make_regimes(raw_split: np.ndarray, close_idx: int, trend_win: int, vol_win: int, quantiles_source: Optional[np.ndarray]=None):\n", "    close = raw_split[:, close_idx].astype(np.float64)\n", "    lr = np.zeros_like(close)\n", "    lr[1:] = np.log(close[1:] / close[:-1])\n", "    # trend/vol proxies on target split\n", "    trend = pd.Series(lr).rolling(trend_win, min_periods=trend_win//2).mean().to_numpy()\n", "    vol = pd.Series(lr).rolling(vol_win, min_periods=vol_win//2).std(ddof=0).to_numpy()\n", "    # thresholds from reference (e.g., train) to avoid forward-looking bias\n", "    if quantiles_source is None:\n", "        qt_trend, qt_vol = trend, vol\n", "    else:\n", "        ref_close = quantiles_source[:, close_idx].astype(np.float64)\n", "        ref_lr = np.zeros_like(ref_close)\n", "        ref_lr[1:] = np.log(ref_close[1:] / ref_close[:-1])\n", "        qt_trend = pd.Series(ref_lr).rolling(trend_win, min_periods=trend_win//2).mean().to_numpy()\n", "        qt_vol = pd.Series(ref_lr).rolling(vol_win, min_periods=vol_win//2).std(ddof=0).to_numpy()\n", "    t_lo, t_hi = np.nanquantile(qt_trend, [0.33, 0.67])\n", "    v_lo, v_hi = np.nanquantile(qt_vol, [0.33, 0.67])\n", "    trend_reg = np.where(trend <= t_lo, -1, np.where(trend >= t_hi, 1, 0))  # -1 bear, 0 neutral, 1 bull\n", "    vol_reg = np.where(vol <= v_lo, 0, np.where(vol >= v_hi, 2, 1))  # 0 low,1 mid,2 high\n", "    return trend_reg, vol_reg\n", "\n", "\n", "\n", "def eval_probe(y_true, y_score, fwd_returns):\n", "    out = {}\n", "    out['roc_auc'] = roc_auc_score(y_true, y_score)\n", "    out['pr_auc'] = average_precision_score(y_true, y_score)\n", "    # IC: Spearman between predicted prob and realized return\n", "    vmask = ~np.isnan(fwd_returns)\n", "    ic, _ = spearmanr(y_score[vmask], fwd_returns[vmask])\n", "    out['ic'] = float(ic)\n", "    # ICIR: mean/std of rolling-50 ICs for stability\n", "    if vmask.sum() > 100:\n", "        import numpy as _np\n", "        wins = 50\n", "        ics = []\n", "        for i in range(0, vmask.sum()-wins+1):\n", "            seg = slice(i, i+wins)\n", "            c, _ = spearmanr(y_score[vmask][seg], fwd_returns[vmask][seg])\n", "            if _np.isfinite(c):\n", "                ics.append(c)\n", "        if len(ics) >= 2:\n", "            out['icir'] = float(_np.mean(ics) / ( _np.std(ics) + 1e-12))\n", "        else:\n", "            out['icir'] = float('nan')\n", "    else:\n", "        out['icir'] = float('nan')\n", "    return out\n"]}, {"cell_type": "markdown", "id": "7d8ecb95", "metadata": {"id": "7d8ecb95"}, "source": ["### Probing: Pipeline e MÃ©tricas\n"]}, {"cell_type": "code", "execution_count": null, "id": "7bd1edfd", "metadata": {"id": "7bd1edfd"}, "outputs": [], "source": ["# === Probing pipeline: fit on val, eval on test ===\n", "from sklearn.metrics import classification_report\n", "\n", "# Find column index for PROBE_RETURN_COL\n", "try:\n", "    close_idx = INDICATORS_TO_USE.index(PROBE_RETURN_COL)\n", "except ValueError:\n", "    raise ValueError(f\"PROBE_RETURN_COL={PROBE_RETURN_COL} nÃ£o estÃ¡ em INDICATORS_TO_USE: {INDICATORS_TO_USE}\")\n", "\n", "# Extrai embeddings (flatten) e roda sweep de horizontes para avaliaÃ§Ã£o rÃ¡pida\n", "X_val_full = extract_embeddings(val_loader, model, DEVICE, pooling=PROBE_POOLING)\n", "X_test_full = extract_embeddings(test_loader, model, DEVICE, pooling=PROBE_POOLING)\n", "HSWEEP = sorted(set([int(PROBE_HORIZON), 4, 8, 16]))\n", "print('Horizon sweep ->', HSWEEP)\n", "for h in HSWEEP:\n", "    y_val, r_val, n_val = window_labels_for_ds(val_loader.dataset, val_raw, close_idx, h)\n", "    y_test, r_test, n_test = window_labels_for_ds(test_loader.dataset, test_raw, close_idx, h)\n", "    X_val = X_val_full[:n_val]\n", "    X_test = X_test_full[:n_test]\n", "    print(f\"Embeddings -> val: {X_val.shape}, test: {X_test.shape} | horizon={h}\")\n", "    print(f\"Labels -> val: {y_val.shape}, test: {y_test.shape} | pos_rate_test={y_test.mean():.3f}\")\n", "    probe = Pipeline([\n", "        ('scaler', StandardScaler()),\n", "        ('pca', PCA(n_components=PROBE_PCA_COMPONENTS, whiten=True, random_state=PROBE_RANDOM_STATE)),\n", "        ('clf', LogisticRegression(\n", "            C=PROBE_REG_C,\n", "            max_iter=PROBE_MAX_ITER,\n", "            class_weight='balanced',\n", "            solver='lbfgs',\n", "            random_state=PROBE_RANDOM_STATE\n", "        ))\n", "    ])\n", "    probe.fit(X_val, y_val, clf__sample_weight=np.abs(r_val))\n", "    proba_test = probe.predict_proba(X_test)[:,1]\n", "    metrics_all = eval_probe(y_test, proba_test, r_test)\n", "    print('--- Probe (TEST) - mÃ©tricas gerais ---')\n", "    for k, v in metrics_all.items():\n", "        print(f\"{k}: {v:.6f}\")\n", "    print()\n", "\n", "# ReconstrÃ³i com PROBE_HORIZON para anÃ¡lise por regime\n", "h = int(PROBE_HORIZON)\n", "y_val, r_val, n_val = window_labels_for_ds(val_loader.dataset, val_raw, close_idx, h)\n", "y_test, r_test, n_test = window_labels_for_ds(test_loader.dataset, test_raw, close_idx, h)\n", "X_val = X_val_full[:n_val]\n", "X_test = X_test_full[:n_test]\n", "probe = Pipeline([\n", "    ('scaler', StandardScaler()),\n", "    ('pca', PCA(n_components=PROBE_PCA_COMPONENTS, whiten=True, random_state=PROBE_RANDOM_STATE)),\n", "    ('clf', LogisticRegression(\n", "        C=PROBE_REG_C,\n", "        max_iter=PROBE_MAX_ITER,\n", "        class_weight='balanced',\n", "        solver='lbfgs',\n", "        random_state=PROBE_RANDOM_STATE\n", "    ))\n", "])\n", "probe.fit(X_val, y_val, clf__sample_weight=np.abs(r_val))\n", "proba_test = probe.predict_proba(X_test)[:,1]\n", "# MÃ©tricas por regime\n", "trend_reg_test, vol_reg_test = make_regimes(test_raw, close_idx, REG_TREND_WIN, REG_VOL_WIN, quantiles_source=train_raw)\n", "# Alinhar regimes ao final da janela\n", "ds = test_loader.dataset\n", "mask = ds_valid_range(ds, h, len(test_raw))\n", "end_positions = np.array([s + ds.seq_len - 1 for ok, s in zip(mask, ds.idxs) if ok], dtype=int)\n", "reg_aligned = trend_reg_test[end_positions]\n", "vol_aligned = vol_reg_test[end_positions]\n", "\n", "print('''\n", "--- Por regime de tendÃªncia (bear=-1, neutral=0, bull=1) ---''')\n", "for lbl in [-1,0,1]:\n", "    idx = np.where(reg_aligned == lbl)[0]\n", "    if len(idx) < 200:\n", "        continue\n", "    m = eval_probe(y_test[idx], proba_test[idx], r_test[idx])\n", "    print(f\"trend={lbl} | n={len(idx)} | auc={m['roc_auc']:.4f} | pr={m['pr_auc']:.4f} | ic={m['ic']:.4f} | icir={m['icir']:.4f}\")\n", "\n", "print('''\n", "--- Por regime de volatilidade (0=low,1=mid,2=high) ---''')\n", "for lbl in [0,1,2]:\n", "    idx = np.where(vol_aligned == lbl)[0]\n", "    if len(idx) < 200:\n", "        continue\n", "    m = eval_probe(y_test[idx], proba_test[idx], r_test[idx])\n", "    print(f\"vol={lbl} | n={len(idx)} | auc={m['roc_auc']:.4f} | pr={m['pr_auc']:.4f} | ic={m['ic']:.4f} | icir={m['icir']:.4f}\")\n"]}, {"cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null, "source": ["# === Probing: Hyperparameter sweep (PCA dims, C) + signal flip ===\n", "PCA_GRID = [64, 128, 256, 512]\n", "C_GRID = [0.1, 0.5, 1.0, 2.0, 5.0]\n", "DO_SWEEP = True\n", "\n", "def select_probe_hparams(X_val, y_val, r_val, dims_grid, c_grid, random_state=PROBE_RANDOM_STATE):\n", "    # Chronological 70/30 split inside val for hparams\n", "    n = len(y_val)\n", "    split = max(2, int(n*0.7))\n", "    i_tr = np.arange(split)\n", "    i_he = np.arange(split, n)\n", "    best = None\n", "    for d in dims_grid:\n", "        d_eff = int(min(max(d, 2), X_val.shape[1]-1 if X_val.shape[1] > 2 else 2, max(2, X_val.shape[0]-2)))\n", "        if d_eff < 2:\n", "            continue\n", "        for C in c_grid:\n", "            pipe = Pipeline([\n", "                ('scaler', StandardScaler()),\n", "                ('pca', PCA(n_components=int(d_eff), whiten=True, random_state=random_state)),\n", "                ('clf', LogisticRegression(\n", "                    C=C, max_iter=PROBE_MAX_ITER, class_weight='balanced', solver='lbfgs', random_state=random_state\n", "                ))\n", "            ])\n", "            pipe.fit(X_val[i_tr], y_val[i_tr], clf__sample_weight=np.abs(r_val[i_tr]))\n", "            proba_he = pipe.predict_proba(X_val[i_he])[:,1]\n", "            m = eval_probe(y_val[i_he], proba_he, r_val[i_he])\n", "            score = (m['pr_auc'], m['roc_auc'])\n", "            if (best is None) or (score > best[0]):\n", "                best = (score, d_eff, C, m, pipe)\n", "    flip = False\n", "    if best is not None:\n", "        pipe = best[4]\n", "        proba_he = pipe.predict_proba(X_val[i_he])[:,1]\n", "        m = eval_probe(y_val[i_he], proba_he, r_val[i_he])\n", "        proba_he_flip = 1.0 - proba_he\n", "        m_flip = eval_probe(y_val[i_he], proba_he_flip, r_val[i_he])\n", "        if (m_flip['pr_auc'] > m['pr_auc']) or ((m['roc_auc'] < 0.5) or (m['ic'] < 0)):\n", "            flip = True\n", "    return (best[1] if best else PCA_GRID[0]), (best[2] if best else C_GRID[0]), flip\n", "\n", "if DO_SWEEP:\n", "    print('=== Sweep over PCA dims and C with sign-flip selection (per horizon) ===')\n", "    HSWEEP = sorted(set([int(PROBE_HORIZON), 4, 8, 16]))\n", "    for h in HSWEEP:\n", "        y_val, r_val, n_val = window_labels_for_ds(val_loader.dataset, val_raw, close_idx, h)\n", "        y_test, r_test, n_test = window_labels_for_ds(test_loader.dataset, test_raw, close_idx, h)\n", "        X_val = X_val_full[:n_val]\n", "        X_test = X_test_full[:n_test]\n", "        dims_sel, C_sel, flip = select_probe_hparams(X_val, y_val, r_val, PCA_GRID, C_GRID)\n", "        print(f'H={h} | best pca={dims_sel}, C={C_sel}, flip={flip}')\n", "        pipe = Pipeline([\n", "            ('scaler', StandardScaler()),\n", "            ('pca', PCA(n_components=int(dims_sel), whiten=True, random_state=PROBE_RANDOM_STATE)),\n", "            ('clf', LogisticRegression(\n", "                C=C_sel, max_iter=PROBE_MAX_ITER, class_weight='balanced', solver='lbfgs', random_state=PROBE_RANDOM_STATE\n", "            ))\n", "        ])\n", "        pipe.fit(X_val, y_val, clf__sample_weight=np.abs(r_val))\n", "        proba_test = pipe.predict_proba(X_test)[:,1]\n", "        if flip:\n", "            proba_test = 1.0 - proba_test\n", "        m = eval_probe(y_test, proba_test, r_test)\n", "        print('--- Sweep Probe (TEST) ---')\n", "        for k in ['roc_auc','pr_auc','ic','icir']:\n", "            print(f'{k}: {m[k]:.6f}')\n", "        ds = test_loader.dataset\n", "        mask = ds_valid_range(ds, h, len(test_raw))\n", "        end_positions = np.array([s + ds.seq_len - 1 for ok, s in zip(mask, ds.idxs) if ok], dtype=int)\n", "        trend_test, vol_test = make_regimes(test_raw, close_idx, REG_TREND_WIN, REG_VOL_WIN, quantiles_source=train_raw)\n", "        reg_aligned = trend_test[end_positions]\n", "        vol_aligned = vol_test[end_positions]\n", "        unique, counts = np.unique(vol_aligned, return_counts=True)\n", "        print('Vol bucket counts:', dict(zip(unique.tolist(), counts.tolist())))\n", "        print('--- Sweep by trend regime ---')\n", "        for lbl in [-1,0,1]:\n", "            idx = np.where(reg_aligned == lbl)[0]\n", "            if len(idx) < 100: continue\n", "            sm = eval_probe(y_test[idx], proba_test[idx], r_test[idx])\n", "            print(f'trend={lbl} | n={len(idx)} | auc={sm[\"roc_auc\"]:.4f} | pr={sm[\"pr_auc\"]:.4f} | ic={sm[\"ic\"]:.4f} | icir={sm[\"icir\"]:.4f}')\n", "        print('--- Sweep by vol regime ---')\n", "        for lbl in [0,1,2]:\n", "            idx = np.where(vol_aligned == lbl)[0]\n", "            if len(idx) < 100: continue\n", "            sm = eval_probe(y_test[idx], proba_test[idx], r_test[idx])\n", "            print(f'vol={lbl} | n={len(idx)} | auc={sm[\"roc_auc\"]:.4f} | pr={sm[\"pr_auc\"]:.4f} | ic={sm[\"ic\"]:.4f} | icir={sm[\"icir\"]:.4f}')\n"]}, {"cell_type": "markdown", "id": "171dad07", "metadata": {"id": "171dad07"}, "source": ["### Embeddings: Export para PPO Agent\n"]}, {"cell_type": "code", "execution_count": null, "id": "8ec9b158", "metadata": {"id": "8ec9b158"}, "outputs": [], "source": ["# === Exportar embeddings para consumo por PPO Agent ===\n", "import os, json as _json\n", "from pathlib import Path as _Path\n", "import numpy as _np\n", "# DiretÃ³rio de saÃ­da (ajuste se necessÃ¡rio)\n", "EMBED_SAVE_DIR = \"/content/drive/MyDrive/timesnet_mnq/embeddings\"  #@param {type:\"string\"}\n", "EMBED_POOLING_EXPORT = 'avg'  #@param [\"avg\", \"flatten\"]\n", "os.makedirs(EMBED_SAVE_DIR, exist_ok=True)\n", "print(f\"Salvando em: {EMBED_SAVE_DIR}\")\n", "@torch.no_grad()\n", "def _extract_embeddings(loader, model, device, pooling: str = 'avg'):\n", "    model.eval()\n", "    vecs = []\n", "    ctx = torch.autocast(device_type='cuda', dtype=AMP_DTYPE) if ('AMP_DTYPE' in globals() and AMP_DTYPE is not None) else nullcontext()\n", "    for xb in loader:\n", "        xb = xb.to(device, non_blocking=True)\n", "        with ctx:\n", "            emb = model.extractor(xb)  # [B,E,H,W]\n", "        if pooling == 'avg':\n", "            v = emb.mean(dim=(2,3))  # [B,E]\n", "        else:\n", "            v = emb.view(emb.size(0), -1)\n", "        vecs.append(v.detach().to('cpu').numpy())\n", "    X = _np.concatenate(vecs, axis=0)\n", "    return X.astype(_np.float32, copy=False)\n", "def _export_split(name: str, loader, raw_split: _np.ndarray, close_idx: int):\n", "    # Extrai embeddings em ordem de dataset\n", "    obs = _extract_embeddings(loader, model, DEVICE, pooling=EMBED_POOLING_EXPORT)\n", "    ds = loader.dataset\n", "    start_idx = _np.asarray(ds.idxs, dtype=_np.int64)\n", "    end_idx = start_idx + int(SEQ_LEN) - 1\n", "    # ConsistÃªncia\n", "    if obs.shape[0] != len(start_idx):\n", "        print(f\"WARN: obs len {obs.shape[0]} != windows {len(start_idx)}; ajustando para o mÃ­nimo comum.\")\n", "        n = min(obs.shape[0], len(start_idx))\n", "        obs = obs[:n]\n", "        start_idx = start_idx[:n]\n", "        end_idx = end_idx[:n]\n", "    # SÃ©rie de preÃ§os para recompensas na Env PPO\n", "    if close_idx is not None and raw_split is not None and raw_split.ndim==2:\n", "        close = raw_split[:, close_idx].astype(_np.float64, copy=False)\n", "    else:\n", "        close = None\n", "    out_path = os.path.join(EMBED_SAVE_DIR, f\"{name}_emb_{EMBED_POOLING_EXPORT}.npz\")\n", "    save = {\n", "        'obs': obs,              # float32 [T,D]\n", "        'X': obs,                # alias\n", "        'start_idx': start_idx,  # int64 [T]\n", "        'end_idx': end_idx,      # int64 [T]\n", "    }\n", "    if close is not None:\n", "        save['close'] = close    # float64 [N]\n", "    _np.savez_compressed(out_path, **save)\n", "    print(f\"{name}: salvo obs{obs.shape} -> {out_path}\")\n", "    return out_path\n", "# Determina o Ã­ndice da coluna 'close' para exportarmos a sÃ©rie de preÃ§os\n", "try:\n", "    _close_idx = INDICATORS_TO_USE.index('close')\n", "except Exception:\n", "    _close_idx = None\n", "splits = {\n", "    'train': (train_loader, train_raw),\n", "    'val':   (val_loader,   val_raw),\n", "    'test':  (test_loader,  test_raw),\n", "}\n", "paths = {}\n", "for _name, (_loader, _raw) in splits.items():\n", "    paths[_name] = _export_split(_name, _loader, _raw, _close_idx)\n", "# Metadados para consumidores downstream (Env PPO)\n", "from datetime import datetime as _dt\n", "first_split = next(iter(paths.values())) if paths else None\n", "if first_split is not None:\n", "    with _np.load(first_split, allow_pickle=False) as _tmp:\n", "        _export_dim = int(_tmp['obs'].shape[1])\n", "else:\n", "    _export_dim = int(EMBED_DIM) if EMBED_POOLING_EXPORT == 'avg' else int(EMBED_DIM) * int(EMBED_H) * int(EMBED_W)\n", "meta = {\n", "    'schema_version': 1,\n", "    'created_at': _dt.utcnow().isoformat()+'Z',\n", "    'seq_len': int(SEQ_LEN),\n", "    'window_stride': int(WINDOW_STRIDE),\n", "    'pooling': EMBED_POOLING_EXPORT,\n", "    'embed_dim': int(_export_dim),   # dimensÃ£o do vetor 'obs'\n", "    'embed_hw': [int(EMBED_H), int(EMBED_W)],\n", "    'features': INDICATORS_TO_USE,\n", "    'normalization': NORMALIZATION_TYPE,\n", "    'timesnet_checkpoint_path': CHECKPOINT_PATH,\n", "    'splits': paths,\n", "    'index_semantics': \"end_idx indexa o vetor 'close' do mesmo split\"\n", "}\n", "meta_path = os.path.join(EMBED_SAVE_DIR, 'embeddings_meta.json')\n", "with open(meta_path, 'w') as f:\n", "    _json.dump(meta, f, indent=2)\n", "print(f\"Meta salvo em: {meta_path}\")\n"]}, {"cell_type": "markdown", "id": "5ce3f893", "metadata": {"id": "5ce3f893"}, "source": ["### VisualizaÃ§Ã£o DinÃ¢mica\n"]}, {"cell_type": "code", "execution_count": null, "id": "71fed431", "metadata": {"id": "71fed431"}, "outputs": [], "source": ["\n", "# === VisualizaÃ§Ã£o dinÃ¢mica do histÃ³rico de treino ===\n", "import os, json\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from IPython.display import display, clear_output\n", "import ipywidgets as w\n", "\n", "HIST_PATH = HISTORY_PATH\n", "assert os.path.exists(HIST_PATH), f\"HistÃ³rico nÃ£o encontrado em {HIST_PATH}. Treine primeiro.\"\n", "\n", "# Carrega JSON Lines\n", "rows = []\n", "with open(HIST_PATH, 'r') as f:\n", "    for line in f:\n", "        line = line.strip()\n", "        if not line:\n", "            continue\n", "        try:\n", "            rows.append(json.loads(line))\n", "        except Exception:\n", "            pass\n", "\n", "df = pd.DataFrame(rows)\n", "if df.empty:\n", "    raise SystemExit('HistÃ³rico vazio.')\n", "\n", "# Widgets\n", "smoothing = w.IntSlider(description='Smoothing', min=1, max=10, value=1)\n", "show_lr = w.Checkbox(value=True, description='Mostrar LR')\n", "show_throughput = w.Checkbox(value=False, description='Mostrar Throughput')\n", "show_gpu = w.Checkbox(value=False, description='Mostrar GPU Peak')\n", "epoch_range = w.IntRangeSlider(description='Ãpocas', min=int(df.epoch.min()), max=int(df.epoch.max()), value=[int(df.epoch.min()), int(df.epoch.max())], step=1)\n", "refresh = w.Button(description='Recarregar', button_style='')\n", "\n", "out = w.Output()\n", "\n", "# Plot function\n", "\n", "def _plot(*args):\n", "    with out:\n", "        clear_output(wait=True)\n", "        lo, hi = epoch_range.value\n", "        d = df[(df.epoch>=lo)&(df.epoch<=hi)].copy()\n", "        if smoothing.value>1:\n", "            d['train_s'] = d['train_loss'].rolling(smoothing.value, min_periods=1).mean()\n", "            d['val_s'] = d['val_loss'].rolling(smoothing.value, min_periods=1).mean()\n", "        else:\n", "            d['train_s'] = d['train_loss']\n", "            d['val_s'] = d['val_loss']\n", "        best_ep = int(df.loc[df.val_loss.idxmin(),'epoch'])\n", "        fig, ax1 = plt.subplots(1,1, figsize=(10,5))\n", "        ax1.plot(d['epoch'], d['train_s'], label='train (smoothed)')\n", "        ax1.plot(d['epoch'], d['val_s'], label='val (smoothed)')\n", "        ax1.axvline(best_ep, color='g', linestyle='--', alpha=0.5, label=f'best@{best_ep}')\n", "        ax1.set_xlabel('epoch')\n", "        ax1.set_ylabel('loss')\n", "        ax1.grid(True, alpha=0.2)\n", "        lines, labels = ax1.get_legend_handles_labels()\n", "        # Optional axes\n", "        if show_lr.value:\n", "            ax2 = ax1.twinx()\n", "            ax2.plot(d['epoch'], d['lr'], color='tab:purple', alpha=0.4, label='lr')\n", "            ax2.set_ylabel('lr')\n", "            l2, lab2 = ax2.get_legend_handles_labels()\n", "            lines += l2; labels += lab2\n", "        if show_throughput.value:\n", "            ax3 = ax1.twinx()\n", "            ax3.spines.right.set_position((\"axes\", 1.1))\n", "            ax3.plot(d['epoch'], d['throughput'], color='tab:orange', alpha=0.4, label='samp/s')\n", "            l3, lab3 = ax3.get_legend_handles_labels()\n", "            lines += l3; labels += lab3\n", "        if show_gpu.value and 'gpu_peak_gb' in d.columns:\n", "            ax4 = ax1.twinx()\n", "            ax4.spines.right.set_position((\"axes\", 1.2))\n", "            ax4.plot(d['epoch'], d['gpu_peak_gb'], color='tab:red', alpha=0.4, label='gpu peak GB')\n", "            l4, lab4 = ax4.get_legend_handles_labels()\n", "            lines += l4; labels += lab4\n", "        ax1.legend(lines, labels, loc='best')\n", "        plt.show()\n", "\n", "# Refresh handler\n", "\n", "def _reload(_btn):\n", "    global df\n", "    rows = []\n", "    with open(HIST_PATH, 'r') as f:\n", "        for line in f:\n", "            line=line.strip()\n", "            if not line: continue\n", "            try: rows.append(json.loads(line))\n", "            except Exception: pass\n", "    df = pd.DataFrame(rows)\n", "    epoch_range.max = int(max(epoch_range.max, df.epoch.max()))\n", "    epoch_range.value = [int(df.epoch.min()), int(df.epoch.max())]\n", "    _plot()\n", "\n", "refresh.on_click(_reload)\n", "for wdg in [smoothing, show_lr, show_throughput, show_gpu, epoch_range]:\n", "    wdg.observe(_plot, names='value')\n", "\n", "controls = w.HBox([smoothing, show_lr, show_throughput, show_gpu])\n", "display(controls, epoch_range, refresh, out)\n", "_plot()\n"]}], "metadata": {"accelerator": "GPU", "colab": {"gpuType": "T4", "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}